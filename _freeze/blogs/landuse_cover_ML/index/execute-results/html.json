{
  "hash": "cf09ab007059c6f39cbbb323065eb8fc",
  "result": {
    "markdown": "---\ntitle: \"Applying Supervised Machine Learning to Landuse Cover in Santa Barbara, CA\"\ndescription: \"using algorithms to understand how we use our land\"\nauthor:\n  - name: Raymond Hunter\n    url: https://ramhunte.github.io/\ndate: 12-019-2023\n# bibliography: references.bib\n#citation:\n  # url: \nimage: \"images/SB.png\"\ncategories: [Quarto, R, Spatial] # self-defined categories\nformat: \n  html:\n    code-fold: true\n    code-copy: true\n    code-summary: \"code\"\n    code-line-numbers: true\n    code-tools: true\n    code-block-border-left: true\ntoc: true\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n---\n\n\n# Overview\n\nHumans have been altering the natural world for centuries through agriculture, farming, development, recreation, etc. The impacts of landuse change have become critical to understand as evidence shows that it contributes significantly to climate change and is responsible for ecological degredation globally.[^1] Monitoring the distribution and change in landuse types can help us understand the impacts of climate change, natural disasters, deforestation, urbanization, and much more. \n\nDale, V. H. (1997). The relationship between land‚Äêuse change and climate change. Ecological applications, 7(3), 753-769.\n\n\n\nDetermining land cover types over large areas is a major application of remote sensing because we are able to distinguish different materials based on their spectral reflectance. In other words, remote sensing has opened up new doors to study land use change by looking at different proportions of light that it reflects up to satellites. By utiliizng remotely sensed imagery, we can classify landcover into classes or groups that allow us to understand the distribution and change in landcover types over large areas. There are many approaches for performing landcover classification -- *supervised* approaches use training data labeled by the user, whereas *unsupervised* approaches use algorithms to create groups which are identified by the user afterward.\n\n# Objective\n\nIn this analysis, I used a form of supervised classification, a *decision tree classifier*, to predict landuse cover in Santa Barbara, California. [Decision trees](https://medium.com/@ml.at.berkeley/machine-learning-crash-course-part-5-decision-trees-and-ensemble-models-dcc5a36af8cd) classify pixels using a series of conditions based on values in spectral bands. These conditions (or decisions) are developed based on training data. Here, I created a land cover classification for southern Santa Barbara County based on multi-spectral imagery and data on the location of 4 land cover types:\n\n-   green vegetation\n-   dry grass or soil\n-   urban\n-   water\n\n## Summary\n\nMy approach to this analysis can be summed up in 5 steps:\n\n-   Step 1: load and process Landsat scene data\n-   Step 2: crop and mask Landsat data to study area (Santa Barbara)\n-   Step 3: extract spectral data at training sites (subset of parcels within Santa Barbara)\n-   Step 4: train and apply decision tree classifier\n-   Step 5: plot results\n\n## Data\n\n**Landsat 5 Thematic Mapper**\n\nData was obtained from [Landsat 5](https://www.usgs.gov/landsat-missions/landsat-5), including 1 scene from September 25, 2007. The specific spectral bands being used are: 1, 2, 3, 4, 5, 7 from the collection 2 surface reflectance product.\n\n**Study area and training data** A polygon representing southern Santa Barbara county was used as the overall study site. I also used polygons of regions within Santa Barbara representing training sites, including character string with land cover type. The decision tree derived from the training data is then applied to the entire Santa Barbara county polygon to create predictions of landuse cover for the whole region.\n\n# Workflow\n\n## Process data\n\n#### Load packages and set working directory\n\nBecause this project required working with both vector and raster data, I used both `sf` and `terra` packages in my workflow. To train our classification decision tree and plot the results, I used the `rpart` and `rpart.plot` packages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, results = FALSE, fig.align = \"center\")\n\nlibrary(sf)\nlibrary(terra)\nlibrary(here)\nlibrary(dplyr)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(tmap)\n```\n:::\n\n\n\n\n#### Load Landsat data\n\nFirst, I created a raster stack based on the 6 bands I worked with. Each file name ends with the band number (e.g. `B1.tif`). Band 6 corresponds to thermal data, which we will not be working with for this analysis, so it was not included in the data. To create a raster stack, I made a list of the files that I wanted to work with and read them all in at once using the `rast` function. The names of the layers were then updated to match the spectral bands and plot a true color image to see what we're working with.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# list files for each band, including the full file path\nfilelist <- list.files(file.path(data, \"landsat-data/\"), full.names = TRUE)\n\n# read in and store as a raster stack\n\nlandsat <- rast(filelist)\nlandsat\n\n# update layer names to match band\n\nnames(landsat) <- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nlandsat\n\n# plot true color image\n\nplotRGB(landsat, r = 3, g = 2, blue = 1, stretch = \"lin\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n#### Load study area\n\nI wanted to constrain the analysis to the southern portion of the county where there was training data, so I read in a file that defines the region of interest (ROI). I made sure that both CRS from the shape file and the Landsat data matched.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# read in shapefile for southern portion of SB county\nSB_county_south <- st_read(file.path(data, \"SB_county_south.shp\"))\n\n# project to match the Landsat data\nSB_county_south <- st_transform(SB_county_south, crs = st_crs(landsat))\n\n# plot(SB_county_south)\n```\n:::\n\n\n#### Crop and mask Landsat data to study area\n\nNext, I cropped and masked the Landsat data to the study area. This reduced the amount of data that needed to be worked with and therefore saves computational time and energy. Furthermore, I removed objects that I no longer was going to be working with using the `rm()` function to save space (optional).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# crop Landsat scene to the extent of the SB county shapefile\nlandsat_cropped <- crop(landsat, SB_county_south)\n\n# mask the raster to southern portion of SB county\nlandsat_masked <- mask(landsat_cropped, SB_county_south)\n\n# remove unnecessary object from environment\nrm(landsat, landsat_cropped, SB_county_south)\n\nplotRGB(landsat_masked, r = 3, g = 2, blue = 1, stretch = \"lin\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n#### Convert Landsat values to reflectance\n\nNext, I converted the values in the raster stack to correspond to reflectance values. To do so, I first removed erroneous values and applied any [scaling factors](https://www.usgs.gov/faqs/how-do-i-use-scale-factor-landsat-level-2-science-products#:~:text=Landsat%20Collection%202%20surface%20temperature,the%20scale%20factor%20is%20applied.) to convert to reflectance. In this case, I worked with [Landsat Collection 2](https://www.usgs.gov/landsat-missions/landsat-collection-2). The valid range of pixel values for this collection is 7,273-43,636, with a multiplicative scale factor of 0.0000275 and an additive scale factor of -0.2. So I reclassified any erroneous values as `NA` and updated the values for each pixel based on the scaling factors. Now the pixel values should range from 0-100%.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(landsat_masked)\n# reclassify erroneous values as NA\nrcl <- matrix(c(-Inf, 7273, NA,\n         43636, Inf, NA), \n        ncol = 3, byrow = TRUE)\n\n\nlandsat <- classify(landsat_masked, rcl = rcl)\n\n# adjust values based on scaling factor\n\nlandsat <- (landsat * 0.0000275 - 0.2)*100\nsummary(landsat)\n\n# plot true color image to check results\nplotRGB(landsat, r = 3, g = 2, b = 1, stretch = \"lin\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\n# check values are 0 - 100\nsummary(landsat)\n```\n:::\n\n\n## Classify image\n\n#### Extract reflectance values for training data\n\nHere, I loaded in the shapefile identifying different locations within the study area as containing one of the 4 land cover types. I then extracted the spectral values at each site to create a data frame that relates land cover types to their spectral reflectance.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# read in and transform training data\ntraining_data <- st_read(file.path(data, \"trainingdata.shp\")) %>% \n  st_transform(., crs = st_crs(landsat))\n#plot\nplot(training_data)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\n# extract reflectance values at training sites\ntraining_data_values <- extract(landsat, training_data, df = TRUE)\n\n# convert training data to data frame\ntraining_data_attributes <- training_data %>% \n  st_drop_geometry()\n\n# join training data attributes and extracted reflectance values\n\nSB_training_data <- left_join(training_data_values, training_data_attributes,\n          by = c(\"ID\" = \"id\")) %>% \n  mutate(type = as.factor(type))\n```\n:::\n\n\n#### Train decision tree classifier\n\nTo train the decision tree, I first needed to establish a model formula (i.e. what the response and predictor variables are). The `rpart` function implements the [CART algorithm](https://medium.com/geekculture/decision-trees-with-cart-algorithm-7e179acee8ff). The `rpart` function needs to know the model formula and training data you would like to use. Because I was performing a classification, I set `method = \"class\"`. I also set `na.action = na.omit` to remove any pixels with `NA`s from the analysis.\\\n\nTo understand how the decision tree will classify pixels, I first plotted the results. The decision tree is comprised of a hierarchy of binary decisions. Each decision rule has 2 outcomes based on a conditional statement pertaining to values in each spectral band.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# establish model formula\n\nSB_formula <- type ~ red + green + blue + NIR + SWIR1 + SWIR2\n\n# train decision tree\nSB_decision_tree <- rpart(formula = SB_formula,\n      data = SB_training_data, \n      method = \"class\", \n      na.action = na.omit)\n\n# plot decision tree\nprp(SB_decision_tree)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n#### Apply decision tree\n\nAfter making the decision tree, I applied it to the entire image. The `terra` package includes a `predict()` function that allows you to apply a model to the data. In order for this to work properly, the names of the layers needed to match the column names of the predictors I used to train our decision tree. The `predict()` function then returns a raster layer with integer values. These integer values correspond to the *factor levels* in the training data. To figure out what category each integer corresponded to, I then inspected the levels of the training data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# classify image based on decision tree\nSB_classification <- predict(landsat, SB_decision_tree, type = \"class\", na.rm = TRUE)\n\n# inspect level to understand the order of classes in prediction\nSB_classification\n```\n:::\n\n\n#### Plot results\n\nFinally, I plotted the results to check out the land cover map.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# plot results\ntm_shape(SB_classification) +\n  tm_raster()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Conclusion\n\nThis analysis highlights the power of using supervised classification to gain insight of land use over on a large scale. While this technique has some challenges, it is a powerful tool that can be used in the environmental field to understand not just land use cover but a wide variety of environmental topics. It is important that these practices are also ground truthed to ensure the predicted model results are accurate.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}