[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "",
    "section": "",
    "text": "Education\n\n\nUniversity of California, Santa Barbara\n\n Masters in Environmental Science and Management | Sept 2022 - June 2024\n\nUniversity of California, Santa Cruz\n\n B.Sc. Ecology and Evolutionary Biology | Highest Honors 4.0 GPA| Sept 2017 - Dec 2020\n B.A. Environmental Studies | Highest Honors 4.0 GPA| Sept 2017 - Dec 2020 \n\nSkills\n\n\nData Science and Management\n\nR/RStudio, Python, Git/GitHub, SQL, Bash, Excel, Google Drive\n\nRemote Sensing & Geospatial Analysis\n\nR, QGIS, Python and Google Earth Engine\n\nMachine Learning\n\nR, Python, supervised/unsupervised, validation, training/testing, optimization, feature engineering, model evaluation\n\nData Visualization\n\nShiny app/dashboard, ggplot, Leaflet, Quarto, RMarkown, CSS, HTML, Javascript, Tableau\n\n\nExperience\n\n\nData Analyst – NOAA Fisheries (Contract)\n\nRemote (4/25-pres.)\n\nDeveloped reproducible data pipelines in R, incorporating statistical and machine learning methods to analyze sustainable fisheries data and assess the effectiveness of federally managed fisheries.\nBuilt interactive R Shiny dashboards, apps, and public web pages to communicate analytical findings and provide accessible fisheries data to stakeholders.\nIntegrated SQL databases with R to create automated, robust workflows for data extraction, modeling, and reporting.\n\n\nData Analyst – Comunidad y Biodiversidad (Contract)\n\nRemote (4/25-7/25.)\n\nDesigned and deployed a server-based evaluation tool using R Shiny, enabling users to assess the effectiveness of marine protected areas across Mexico.\n\n\nData Manager – Masters Group Project\n\nUCSB (3/23-6/24) \n\nDesigned and spearheaded extensive geospatial analyses with 180k+ hectares of remotely sensed spatial and tabular data sets to model riparian habitat restoration requirements and generate cost estimates\nLeveraged zonal statistics, spatial subsets/joins, and raster/vector transformations with overlayed indigenous demographics to identify keys ecologically unique areas for recommended restoration\n\nDeveloped a user friendly and interactive Shiny dashboard (HTML and CSS) to visually present results and facilitate clear communication of key findings to NOAA restoration management and other stakeholders\n\n\nTeaching Assistant – UCSB\n\nUCSB (9/22-6/24)\n\nApplied Ecology | Environmental Chemistry | Environmental Ethics | Infectious Disease Ecology\n\n\nBiosecurity Intern – The Nature Conservancy\n\nSanta Barbara, CA (6/23-9-23)\n\nLed a comprehensive study including statistical analysis/modeling, GIS, report writing, fieldwork, and publication to address biosecurity weaknesses in the Channel Islands while mentoring an undergraduate\nCoded logistic regression models with environmental covariates measured using GIS to predict behavioral responses; performed model evaluations and feature engineering to optimize model performance\nPresented findings at the California Islands Symposium (publishing a first-authored scientific paper)\n\n\nData Analyst – Yoga Soup\n\nSanta Barbara, CA (9/23-pres.)\n\nProgrammed and executed a data management plan to distribute and archive terabytes of company data\nCoded automated reproducible pipelines in R to perform identification and relocation of sensitive data\n\n\nBiologist – Mountain View Biological Consulting\n\nMammoth Lakes, CA (2/21–6/21)\n\nMapped dozens of field sites in GIS, located access points and project perimeters, and communicated project logistics to senior biologists, project managers, and contractors\nDrafted environmental compliance reports for contractors, consultants, and land owners summarizing project description and biological activity within the region of interest (ROI)\n\n\nLab Technician – Sierra Nevada Aquatic Research laboratory\n\nMammoth Lakes, CA (1/21–6/22)\n\nQuantified 20+ years of long term acid mining drainage impacts on alpine stream biodiversity to inform US Forest Service on remediation success\nIdentified and recorded 100,000’s of stream organisms in excel to manage over 20 years of project data\n\n\nResearch Technician – Palkovacs Lab UCSC\n\nSanta Cruz, CA (9/2019-12-20)\n\nSecured $10,000’s in research grants to study the link between steelhead trout genotypes and prey selectivity on benthic macroinvertebrate (BMI) communities\nDesigned a study measuring the ecological consequences of wildfires across 3 different watersheds in coastal California to inform land management of potential trophic cascades\n\n\nEnvironmental Consultant Assistant – Laguna Geosciences\n\nLaguna Beach, CA (seasonal 1/18-6/20)\n\nOrganized and documented project data sets for senior management to ensure clients’ needs are met\n\n\nREU Researcher – University of Puerto Rico\n\nRio Piedras, PR (6/19-9/19)\n\nDesigned a research project measuring disturbance impacts on diversity of 25+ freshwater meiofauna taxa in proximity to highly developed areas in low income communities\nCollected hundreds of field samples, analyzed samples under the microscope, and reported findings to a committee of freshwater ecologists and Puerto Rican citizens in a research paper\n\n\nField Technician – Lyon Lab\n\nUCSC (9/19-12-20)\n\nDocumented hundreds of sparrow social interactions to provide data on the link between phenotypic hierarchical traits and social dominance within migrating flocks\n\n\nIntern - Small Mammal Undergraduate Research in the Forest\n\nUCSC (9/18-12/18)\n\nTrapped, tagged, and recorded hundreds of coastal California rodents and contributed results to long term metadata on mammal populations\nWorked intensive field days setting up transects, laying traps, and collecting field data in Monterey Bay\n\n\n\nCertificates\n\n\nGoogle Data Analytics (9/2023)\n\n\nAwards\n\n\nBren Academic Excellence Recruitment Fellowship 2022 & 2023\nNRS Field Science Fellowship 2020\nFuture Leaders in Coastal Science Award 2019\nKathryn D. Sullivan Impact Award 2019\nNorris Center Student Natural History Award 2019\nRichard A. Cooley Award 2019\nGolden Key International Honors Society 2018\nThe National Society of Collegiate Scholars 2017\nHonors Program UCSC\nUCSC Deans Honor Roll (2017-2020)\n\n\n\nView/Download Resume PDF"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "My Blog",
    "section": "",
    "text": "Data Pipelines with {Targets}\n\n\n\n\n\n\nReproducible Pipelines\n\n\n\nAn R package for creating reproducible workflows\n\n\n\n\n\nJul 10, 2025\n\n\nRaymond Hunter\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Machine Learning to Predict Water Potability\n\n\n\n\n\n\nML\n\n\nClassification\n\n\n\nWhat makes our water undrinkable?\n\n\n\n\n\nApr 3, 2024\n\n\nRaymond Hunter\n\n\n\n\n\n\n\n\n\n\n\n\nThe World’s Largest Fishing Fleet\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nDataViz\n\n\n\nChina’s Illegal and Legal Fishing Vessels Using AIS (2012-2020)\n\n\n\n\n\nMar 6, 2024\n\n\nRaymond Hunter\n\n\n\n\n\n\n\n\n\n\n\n\nApplying Supervised Machine Learning to Landuse Cover in Santa Barbara, CA\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nSpatial\n\n\n\nbuilding algorithms to understand how we use our land\n\n\n\n\n\nDec 19, 2023\n\n\nRaymond Hunter\n\n\n\n\n\n\n\n\n\n\n\n\nPotential Marine Aquaculture Habitat Along the US West Coast\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nSpatial\n\n\n\nmodeling habitat through spatial analysis\n\n\n\n\n\nDec 16, 2023\n\n\nRaymond Hunter\n\n\n\n\n\n\n\n\n\n\n\n\nA Spatial Analysis of Houston Blackouts and the Communities it Affected\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nSpatial\n\n\n\nan investigation of environmental injustice\n\n\n\n\n\nDec 15, 2023\n\n\nRaymond Hunter\n\n\n\n\n\n\n\n\n\n\n\n\nDrivers of Zooplankton Community Dynamics in Sierra Nevada Lakes\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\necology\n\n\n\nA look into modeling microcosms\n\n\n\n\n\nDec 8, 2023\n\n\nRaymond Hunter\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/marine_aquaculture/index.html",
    "href": "blogs/marine_aquaculture/index.html",
    "title": "Potential Marine Aquaculture Habitat Along the US West Coast",
    "section": "",
    "text": "Github Project Repository"
  },
  {
    "objectID": "blogs/marine_aquaculture/index.html#background",
    "href": "blogs/marine_aquaculture/index.html#background",
    "title": "Potential Marine Aquaculture Habitat Along the US West Coast",
    "section": "Background",
    "text": "Background\nLand based meat production poses serious environmental challenges through its contribution to greenhouse gas (GHG) emissions as well as harsh land use impacts. As society progresses further into uncharted waters with climate change, it is critical that we explore alternative options to mitigate GHG emissions. Marine aquaculture is an important practice that has a high potential to transform the way the global food supply operates as a more sustainable protein option than land-based meat production.1 One important study by Gentry et al. mapped the potential for marine aquaculture globally based on multiple constraints, including ship traffic, dissolved oxygen, and bottom depth .2 This highlighted the true potential of the industry, although it is still far from reaching its capacity."
  },
  {
    "objectID": "blogs/marine_aquaculture/index.html#objectives",
    "href": "blogs/marine_aquaculture/index.html#objectives",
    "title": "Potential Marine Aquaculture Habitat Along the US West Coast",
    "section": "Objectives",
    "text": "Objectives\nThe purpose of this work flow is to utilize spatial analysis tools to understand how we can identify commercial potential of natural resources. In this analysis, I am looking at how to identify potential marine species habitat suitable for aquaculture along the West Coast of the United States in each of its Exclusive Economic Zones (EEZ). I specifically look at habitat requirements across temperature and depth requirements for oysters (a widely commercially available food source) and then create a function from this analysis that can be applied to any west coast species with a given temperature and depth range and potential for aquaculture. This analysis utilizes a variety of spatial analysis tools to for both vector and raster data. These include:\n\nspatial subsetting and joins\nraster reclassifiaction\nrasterization and vectorization of spatial data\ncropping and masking layers\nzonal statistics\ntransforming CRS\nutilizing periodic checks"
  },
  {
    "objectID": "blogs/marine_aquaculture/index.html#approach",
    "href": "blogs/marine_aquaculture/index.html#approach",
    "title": "Potential Marine Aquaculture Habitat Along the US West Coast",
    "section": "Approach",
    "text": "Approach\nBased on previous research, we know that oysters need the following conditions in order to reach optimal growth rates: - sea surface temperature: 11-30°C - depth: 0-70 meters below sea level\nI used raster and vector data and spatial analysis tools to identify where these ranges overlapped to isolate suitable habitat. Following these steps, I created maps visualizing priority EEZ’s depending on their total and percent suitable aquaculture habitat. I applied this methodology into a function that reproduces the same workflow to find suitable habita for any species with a temperature and depth range that could be potentially grown along the west coast.\n\nData\n\nSea Surface Temperature\nAverage annual sea surface temperature (SST) from the years 2008 to 2012 was used to characterize the average sea surface temperature within the region. The data was originally generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\n\n\nBathymetry\nTo characterize the depth of the ocean I used the General Bathymetric Chart of the Oceans (GEBCO).3\n\n\nExclusive Economic Zones\nMaritime boundaries were designated using Exclusive Economic Zones off of the west coast of US from Marineregions.org."
  },
  {
    "objectID": "blogs/marine_aquaculture/index.html#analysis",
    "href": "blogs/marine_aquaculture/index.html#analysis",
    "title": "Potential Marine Aquaculture Habitat Along the US West Coast",
    "section": "Analysis",
    "text": "Analysis\n\nPrepare data\nTo start, I loaded in and organized all necessary data and made sure it had the correct coordinate reference system. This involved:\n\nreading in the shapefile for the West Coast EEZ (wc_regions_clean.shp)\n\nreading in SST raster data\n\naverage_annual_sst_2008.tif\n\naverage_annual_sst_2009.tif\n\naverage_annual_sst_2010.tif\n\naverage_annual_sst_2011.tif\n\naverage_annual_sst_2012.tif\n\n\ncombining SST rasters into a raster stack\nreading in bathymetry raster (depth.tif)\n\nchecking that data are in the same coordinate reference system\n\n\n\n\ncode\n#loading in and preparing the data in raster format \n\n# reading in west coast eez shape file \neez &lt;- st_read(file.path(data, \"wc_regions_clean.shp\"))\n\n#read in SST raster data as a stack\nstack &lt;- rast(c(file.path(data, \"average_annual_sst_2008.tif\"), \n                file.path(data, \"average_annual_sst_2009.tif\"), \n                file.path(data, \"average_annual_sst_2010.tif\"), \n                file.path(data, \"average_annual_sst_2011.tif\"), \n                file.path(data, \"average_annual_sst_2012.tif\")))\n\n#changing CRS\nstack &lt;- project(stack, \"epsg:4326\")\n\n#read in bathymetry data \nbath &lt;- rast(file.path(data, \"depth.tif\"))\n\n# plot(stack)\n# plot(bath)\n\n\n\n\nProcess data\nNext, I processed the SST and depth data so that they could be combined. In this case the SST and depth data had slightly different resolutions, extents, and positions. Because I didn’t want to change the underlying depth data, I resampled to match the SST data using the nearest neighbor approach.\nThe steps I took can be summarized as follows:\n\nStep 1: find the mean SST from 2008-2012\n\nStep 2: convert SST data from Kelvin to Celsius\n\nStep 3: crop depth raster to match the extent of the SST raster (match CRS first)\nStep 4: resample the NPP data to match the resolution of the SST datausing the nearest neighbor approach\n\nStep 5: check that the depth and SST match in resolution, extent, and coordinate reference system\n\n\nI then created a check to ensure that the extent, resolution, and CRS of bathymetry and temperature raster layers matched.\n\n\ncode\n# finding mean temperature raster data in celsius and aligning resolutions and extents of rasters\n\n#findig mean of sst rasters\nmean_sst &lt;- mean(stack)\n# plot(mean_sst)\n\n#finding temp in celsius\ncels_sst &lt;- mean_sst - 273.15\n\n#cropping depth raster to sst\ncrop_bath &lt;- crop(bath, cels_sst)\n# plot(crop_bath)\n\n#resampling bath layer to match resolution of celsius\nresamp_bath &lt;- resample(crop_bath, cels_sst, method = \"near\")\n\n# plot(reclass_bath)\n\n                  ####### creating a check #######\ncat(\"Extent, resolution, and CRS of bathymetry and temprature raster layers match:\")\n\n\nExtent, resolution, and CRS of bathymetry and temprature raster layers match:\n\n\ncode\nif(ext(resamp_bath) == ext(cels_sst) & \n   st_crs(resamp_bath)$epsg == st_crs(cels_sst)$epsg &\n   # resolution has more than one responses, so we need to make sure that both are true \n   all(res(resamp_bath) == res(cels_sst))) {\n  cat(\"TRUE\")\n} else{\n  cat(\"FALSE\")\n}\n\n\nTRUE\n\n\n\n\nFinding suitable locations\nIn order to find suitable locations for marine aquaculture, I first needed to find locations that were suitable in terms of both SST and depth. This involved reclassifying the SST and depth data into locations that are suitable for Oysters by assigning a 1 or a NA value to suitable and unsuitable habitats. I then multiplied the two rasters to look for locations that satisfy both SST and depth conditions (raster cells where 1 values overlapped) using the lapp() function. Lastly, I created a check to ensure that the masked ideal layer only showed overlapping ideal depth and temperature conditions.\n\n\ncode\n#reclassifying temperature and bathymetry raster data, and then overlaying the layers to find ideal habitat \n\n\n# reclassify temp raster to NA or 1 if in 11-30C temp range \nrcl_temp &lt;- classify(cels_sst, rcl = \n                      matrix(c(-Inf, 11, NA, \n                      11, 30, 1, \n                      30, Inf, NA), ncol = 3, byrow = TRUE))\n# plot(rcl_temp)\n\n# reclassify bathymetry data (0-70 meters below sea level)\nrcl_bath &lt;- classify(resamp_bath, rcl = \n                      matrix(c(-Inf, -70, NA, \n                      -70, 0, 1, \n                      0, Inf, NA), ncol = 3, byrow = TRUE))\n# plot(rcl_bath)\n\n#creating a raster stack\nideal_stack &lt;- c(rcl_temp, rcl_bath)\n\n#renaming rasters in stack\nnames(ideal_stack) &lt;- c(\"temperature\", \"depth\")\n\n#creating a multiplication function \nmultiply &lt;- function(x,y){\n  return(x*y)\n}\n\n# multiplying layers to get just cells = 1 (ideal depth and temp )\nideal_layer &lt;- lapp(ideal_stack[[c(1,2)]], fun = multiply)\n\n# plot(ideal_layer)\n\n              ################### Check ###############\n#checking to make sure ideal layer is only showing areas where the temp and depth are in ideal range\ncat(\"Masked ideal layer only shows overlapping ideal depth and temperature conditons:\")\n\n\nMasked ideal layer only shows overlapping ideal depth and temperature conditons:\n\n\ncode\nif(all(\n  values(ideal_layer == 1) == values(ideal_stack[[1]] == 1),\n  values(ideal_layer == 1) == values(ideal_stack[[2]] == 1), na.rm = TRUE)) {\n  cat(\"TRUE\")\n} else{\n  cat(\"FALSE\")\n}\n\n\nTRUE\n\n\n\n\nDetermine the most suitable EEZ\nI first needed to determine the total suitable area within each EEZ in order to rank zones by priority. To do so, I found the total area of suitable locations within each EEZ. The steps in which I approached this can be summarized as such:\n\nStep 1: read in, rasterize, and crop the the eez data to the extent and resolution of the ideal habitat layer\nStep 2: calculate the size of each raster cell in km^2\nStep 3: sum the tala area of suitable habitat cells using zonal() function\nStep 4: join this data with the eez data and create new columns with the total eez area, the toal suitable habitat in each eez, and the percent suitable habitat by eez\n\n\n\ncode\n#here we are calculating the total area of suitable habitat and the percent area of suitable habitat for each EEZ\n\n#read in EEZ data \neez &lt;- st_read(file.path(data, \"wc_regions_clean.shp\"))\n\n#rasterizing polygon data with ideal locations by region and masking it to ideal locations\neez_mask &lt;- rasterize(eez, ideal_layer, field = \"rgn\") %&gt;% \n  terra::crop(ideal_layer, mask = TRUE)\n# plot(eez_mask)\n\n#find area of raster cells \ncell_area &lt;- cellSize(eez_mask, unit = \"km\")\n# plot(cell_area)\n\n#calculate the total area of km2 by region\nideal_area &lt;- zonal(cell_area, eez_mask, fun = \"sum\", na.rm = TRUE)\n\n#joining ideal area data w/ eez vector data \njoined_eez &lt;- left_join(eez, \n            ideal_area, \n            by = \"rgn\") %&gt;% \n  mutate(ideal_area_km2 = area, \n         perc_ideal = (ideal_area_km2/area_km2)*100, \n         .before = geometry)\n\n\n\n\nVisualize results\nFinally, I created two maps showing 1) the total suitable oyster aquaculture habitat area by region, and 2) the percent suitable oyster aquaculture habitat area by region:\n\n\ncode\n# pulling in states/base map layer for mapping EEZ's\nstates &lt;- us_states\n\n#choosing just the west coast regions\nwest_coast &lt;- st_sf(us_states) %&gt;% \n  filter(REGION == \"West\") %&gt;% \n  st_transform(crs = st_crs(joined_eez))\n\n\n#plotting the percent suitable area of ideal habitat for oysters for each EEZ\nggplot() +\n  geom_sf(data = west_coast, color = \"black\", fill = \"#E4CA74\") +\n  geom_sf(data = joined_eez, aes(fill = perc_ideal)) +\n  scale_fill_viridis_c() +\n  labs(title = \"% Suitable Oyster Aquaculture Habitat of West \\nCoast Exclusive Economic Zones in the US\", y = \"Longitude\", x = \"Latitude\", fill = expression(\"% Suitable Area\")) +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"lightblue3\"), \n    plot.title = element_text(size = 12, hjust = 0.5)) +\n    # panel.grid.major = element_blank(),\n   # panel.grid.minor = element_blank()) +\n     coord_sf(xlim = c(-132.5, -114.5)) +\n   annotation_scale( location = \"bl\") +\n  # theme( legend.position= \"bottom\") +\n         # plot.margin=unit(c(1,6,.5,1),\"cm\")) +\n  annotation_north_arrow(location = \"bl\",\n                         pad_x = unit(0.05, \"in\"),\n                         pad_y = unit(0.3, \"in\"),\n                         style = ggspatial::north_arrow_minimal()) +\n  annotate(\"text\", x = -121, y = 32.5, label = \"Southern CA\", size = 2, color = \"black\") +\n   annotate(\"text\", x = -124, y = 36, label = \"Central CA\", size = 2, color = \"black\") +\n  annotate(\"text\", x = -126.5, y = 40, label = \"Northern CA\", size = 2, color = \"lightgrey\") +\n    annotate(\"text\", x = -126.5, y = 44, label = \"Oregon\", size = 2, color = \"lightgrey\") +\n      annotate(\"text\", x = -126.5, y = 46.7, label = \"Washington\", size = 2, color = \"black\") \n\n\n\n\n\nFigure 1. % Suitable Oyster Aquaculture Habitat of West Coast Exclusive Economic Zones in the US. Shows the percent of ideal oyster aquaculture habitat by exclusive economic zones along the west coast. Washington shows the highest % suitable habitat, followed by Central and Southern California.\n\n\n\n\n\n\ncode\n#plotting the total suitable area of ideal habitat for oysters in each EEZ\nggplot() +\n  geom_sf(data = west_coast, color = \"black\", fill = \"#E4CA74\") +\n  geom_sf(data = joined_eez, aes(fill = ideal_area_km2)) +\n  scale_fill_viridis_c() +\n  labs(title = \"Suitable Oyster Aquaculture Habitat Area by West \\nCoast Exclusive Economic Zones in the US\", y = \"Longitude\", x = \"Latitude\", fill = expression(\"Total Suitable Area (km\"^2*\")\")) +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"lightblue3\"), \n    plot.title = element_text(size = 12, hjust = 0.5)) +\n    # panel.grid.major = element_blank(),\n   # panel.grid.minor = element_blank()) +\n     coord_sf(xlim = c(-132.5, -114.5)) +\n   annotation_scale( location = \"bl\") +\n  # theme( legend.position= \"bottom\") +\n         # plot.margin=unit(c(1,6,.5,1),\"cm\")) +\n  annotation_north_arrow(location = \"bl\",\n                         pad_x = unit(0.05, \"in\"),\n                         pad_y = unit(0.3, \"in\"),\n                         style = ggspatial::north_arrow_minimal()) +\n  annotate(\"text\", x = -121, y = 32.5, label = \"Southern CA\", size = 2, color = \"black\") +\n   annotate(\"text\", x = -124, y = 36, label = \"Central CA\", size = 2, color = \"black\") +\n  annotate(\"text\", x = -126.5, y = 40, label = \"Northern CA\", size = 2, color = \"lightgrey\") +\n    annotate(\"text\", x = -126.5, y = 44, label = \"Oregon\", size = 2, color = \"lightgrey\") +\n      annotate(\"text\", x = -126.5, y = 46.7, label = \"Washington\", size = 2, color = \"lightgrey\") \n\n\n\n\n\nFigure 2. Suitable Oyster Aquaculture Habitat Area by West Coast Exclusive Economic Zones in the US. Shows the total area (km2) of ideal oyster farming habitat by exclusive economic zones along the West Coast. Central California shows the highest total suitable area, followed by Souther California and Washington.\n\n\n\n\n\n\nExpanding to other species\nAfter having identified the potential suitable habitat for oyster aquaculture, I wanted to expand this workflow to easily make maps for other species requirements. I created a function that can accept any species name, temperature, and depth ranges, and it will create two replicated maps for the given species.\n\n\ncode\n#creating a function that shows the ideal species habitat using species name,  temperature, and depth as inputs \nideal_crib_for_a_species &lt;- function(species = \"NAME\", depth_min, depth_max, temp_min, temp_max) {\n# depth reclassification\n  rcl_depth &lt;- classify(resamp_bath, rcl = matrix(c(-Inf, depth_min, NA, \n                                  depth_min, depth_max, 1, \n                                  depth_max, Inf, NA), ncol = 3, byrow = TRUE))\n#temp reclassification\n  rcl_temp &lt;- classify(cels_sst, rcl = matrix(c(-Inf, temp_min, NA, \n                                temp_min, temp_max, 1, \n                                temp_max, Inf, NA), ncol = 3, byrow = TRUE))\n  #ideal layers stack\n  ideal_stack &lt;- c(rcl_temp, rcl_bath)\n  #ideal layer\n  ideal_layer &lt;- lapp(ideal_stack[[c(1,2)]], fun = multiply)\n  #rasterize eez polygons\n  eez_mask &lt;- rasterize(eez, ideal_layer, field = \"rgn\") %&gt;% \n    terra::crop(ideal_layer, mask = TRUE)\n  #find area of raster cells \n  cell_area &lt;- cellSize(eez_mask, unit = \"km\")\n  #calculate the total area of km2 by region\n  ideal_area &lt;- zonal(cell_area, eez_mask, fun = \"sum\", na.rm = TRUE)\n  #joining ideal area data w/ eez vector data \n  joined_eez &lt;- left_join(eez, ideal_area, by = \"rgn\") %&gt;% \n    mutate(ideal_area_km2 = area, perc_ideal = (ideal_area_km2/area_km2)*100, .before = geometry)\n\n\n############################# mapping #####################################\n  \n  #percent area\n m1 &lt;- ggplot() +\n    geom_sf(data = west_coast, color = \"black\", fill = \"#E4CA74\") +\n    geom_sf(data = joined_eez, aes(fill = perc_ideal)) +\n    scale_fill_viridis_c() +\n    labs(title = paste(\"% Suitable\", species, \"Aquaculture Habitat of West \\nCoast Exclusive Economic Zones in the US\"), y = \"Longitude\", x = \"Latitude\", fill = expression(\"% Suitable Area\")) +\n    theme_minimal() +\n    theme(\n      panel.background = element_rect(fill = \"lightblue3\"), \n      plot.title = element_text(size = 12, hjust = 0.5)) +\n    coord_sf(xlim = c(-132.5, -114.5)) +\n    annotation_scale( location = \"bl\") +\n    annotation_north_arrow(location = \"bl\",\n                           pad_x = unit(0.05, \"in\"),\n                           pad_y = unit(0.3, \"in\"),\n                           style = ggspatial::north_arrow_minimal()) +\n    annotate(\"text\", x = -121, y = 32.5, label = \"Southern CA\", size = 2, color = \"lightgrey\") +\n    annotate(\"text\", x = -124, y = 36, label = \"Central CA\", size = 2, color = \"lightgrey\") +\n    annotate(\"text\", x = -126.5, y = 40, label = \"Northern CA\", size = 2, color = \"lightgrey\") +\n    annotate(\"text\", x = -126.5, y = 44, label = \"Oregon\", size = 2, color = \"lightgrey\") +\n    annotate(\"text\", x = -126.5, y = 46.7, label = \"Washington\", size = 2, color = \"black\") \n###################################################################################\n\n  #plotting total area\n  m2 &lt;- ggplot() +\n  geom_sf(data = west_coast, color = \"black\", fill = \"#E4CA74\") +\n  geom_sf(data = joined_eez, aes(fill = ideal_area_km2)) +\n  scale_fill_viridis_c() +\n  labs(title = paste(\"Suitable\", species, \"Aquaculture Habitat Area by West \\nCoast Exclusive Economic Zones in the US\"), y = \"Longitude\", x = \"Latitude\", fill = expression(\"Total Suitable Area (km\"^2*\")\")) +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"lightblue3\"), \n    plot.title = element_text(size = 12, hjust = 0.5)) +\n    # panel.grid.major = element_blank(),\n   # panel.grid.minor = element_blank()) +\n     coord_sf(xlim = c(-132.5, -114.5)) +\n   annotation_scale( location = \"bl\") +\n  # theme( legend.position= \"bottom\") +\n         # plot.margin=unit(c(1,6,.5,1),\"cm\")) +\n  annotation_north_arrow(location = \"bl\",\n                         pad_x = unit(0.05, \"in\"),\n                         pad_y = unit(0.3, \"in\"),\n                         style = ggspatial::north_arrow_minimal()) +\n  annotate(\"text\", x = -121, y = 32.5, label = \"Southern CA\", size = 2, color = \"lightgrey\") +\n   annotate(\"text\", x = -124, y = 36, label = \"Central CA\", size = 2, color = \"lightgrey\") +\n  annotate(\"text\", x = -126.5, y = 40, label = \"Northern CA\", size = 2, color = \"lightgrey\") +\n    annotate(\"text\", x = -126.5, y = 44, label = \"Oregon\", size = 2, color = \"lightgrey\") +\n      annotate(\"text\", x = -126.5, y = 46.7, label = \"Washington\", size = 2, color = \"black\") \n  \n  return(list(m1, m2))\n  \n  }\n\n\nI wanted to test out this function on kelp crabs (Pugettia producta) as they are widely dispersed and could potentially make a good candidate for aquaculture. Data on the Kelp Crab was obtained from SeaLifeBase which presents habitat information on thousands of species. Lets take a look:\n\n\ncode\nideal_crib_for_a_species(\"Kelp Crab\", depth_min = -98, depth_max =  0, temp_min = 7.8, temp_max =  12.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd there you have it! This is how I used spatial analysis tools in R to figure out where potential oyster (and any other species) aquaculture might be by EEZ along the west coast of the United States."
  },
  {
    "objectID": "blogs/marine_aquaculture/index.html#footnotes",
    "href": "blogs/marine_aquaculture/index.html#footnotes",
    "title": "Potential Marine Aquaculture Habitat Along the US West Coast",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHall, S. J., Delaporte, A., Phillips, M. J., Beveridge, M. & O’Keefe, M. Blue Frontiers: Managing the Environmental Costs of Aquaculture (The WorldFish Center, Penang, Malaysia, 2011).↩︎\nGentry, R. R., Froehlich, H. E., Grimm, D., Kareiva, P., Parke, M., Rust, M., Gaines, S. D., & Halpern, B. S. Mapping the global potential for marine aquaculture. Nature Ecology & Evolution, 1, 1317-1324 (2017).↩︎\nGEBCO Compilation Group (2022) GEBCO_2022 Grid (doi:10.5285/e0f0bb80-ab44-2739-e053-6c86abc0289c).↩︎"
  },
  {
    "objectID": "blogs/houston_blackout/houston_blackout.html",
    "href": "blogs/houston_blackout/houston_blackout.html",
    "title": "A Spatial Analysis of Houston Blackouts and the Communtiies it Affected",
    "section": "",
    "text": "Github project repository"
  },
  {
    "objectID": "blogs/houston_blackout/houston_blackout.html#background",
    "href": "blogs/houston_blackout/houston_blackout.html#background",
    "title": "A Spatial Analysis of Houston Blackouts and the Communtiies it Affected",
    "section": "Background",
    "text": "Background\nNatural disasters oftentimes occur unpredictably and rapidly, and the severity of them can vary widely. Specifically in the Southern United States, major storms (like Hurricane Katrina) can have devastating and long lasting consequences on communities.1 “In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”2. While these storms seem like once in a blue moon events, they are becoming much more of a normal occurrence. More importantly, storms do not impact all communities the same, and therefore it is critical that we incorporate racial justice components to our lines of environmental analysese to understand how we can mitigate marginalized groups from being disproportianately harmed from climate change."
  },
  {
    "objectID": "blogs/houston_blackout/houston_blackout.html#goal",
    "href": "blogs/houston_blackout/houston_blackout.html#goal",
    "title": "A Spatial Analysis of Houston Blackouts and the Communtiies it Affected",
    "section": "Goal",
    "text": "Goal\nThe goal of this study is to try and answer the question: Which communities in Houston experienced blackouts from the February 2021 storms, and did they vary by socioeconomic status? Other studies have shown that not only do lower socioeconomic status communities experience more intense consequences of natural disaster, but they also don’t always receive the same degree of aid following one.3\nBy identifying the total number of houses in Houston that lost power and overlaying socioeconomic data to these homes, we can begin to look at these underlying patterns of potential environmental injustice. These types of studies are becoming critically important as climate change continues to increase the frequency of severe storms including intense precipitation events throughout Texas4 as not all communities are affected the same. We need to look at how communities of varying socioeconomic status are impacted as a result."
  },
  {
    "objectID": "blogs/houston_blackout/houston_blackout.html#approach",
    "href": "blogs/houston_blackout/houston_blackout.html#approach",
    "title": "A Spatial Analysis of Houston Blackouts and the Communtiies it Affected",
    "section": "Approach",
    "text": "Approach\nI used remotely-sensed night lights data from before and after the storms to estimate power outages. Data was acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) from the Suomi satellite. I used the VNP46A1 tolook for variation in night lights before and after the storm as a way of identifying areas that lost electric power as a result of the storms. Furthermore, data containing the physical boundaires and area of buildings and roads is needed to spatially join to areas that lost power. I gathered this data from OpenStreetMap providing high resolution data on the structural layout of the city. Finally, we need to join this data with socioeconomic factor data pertaining to all the communities in Houston affected by the blackouts. I pulled this data from the US Census Bureau (see Readme file) that shows block group county level information on various socioeconomic characterisitcs.\nThese types of analysis require a variety of different tools to carry out from satellite images and census data to finalized maps. Some of the most important tools in carrying out this analysis include: * loading, transforming, and wrangling vector/raster data * raster operations to identify blackout areas from satellite imagery * vector operations to isolate affected buildings and remove interference (road lights) * spatial joins of regions experiencing blackouts and the total number of untis within them\n\nData\n\nNight lights\nUsing NASA’s Worldview to find data pre and post storm, I found satellite images recorded on 2021-02-07 and 2021-02-16 that provided two clear, contrasting images to visualize the extent of the power outage in Texas. VIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection that are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06, and so I downloaded two tiles per date to ensure the whole city was being included. These data are stored in the VNP46A1 folder:\n\nVNP46A1.A2021038.h08v05.001.2021039064328.h5.tif: tile h08v05, collected on 2021-02-07\n\nVNP46A1.A2021038.h08v06.001.2021039064329.h5.tif: tile h08v06, collected on 2021-02-07\n\nVNP46A1.A2021047.h08v05.001.2021048091106.h5.tif: tile h08v05, collected on 2021-02-16\n\nVNP46A1.A2021047.h08v06.001.2021048091105.h5.tif: tile h08v06, collected on 2021-02-16\n\n\n\nRoads\nOne issue with night light data is that light pollution can interfere with detecting patterns of interest. Highways make up much of the night lights seen from space (see Google’s Earth at Night). Therefore, it is important to minimize falsely identifying areas with reduced traffic as areas without power. OpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. I used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area.\n\ngis_osm_roads_free_1.gpkg\n\n\n\nHouses\nI also obtained building data from OpenStreetMap. Again, I downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\n\ngis_osm_buildings_a_free_1.gpkg\n\n\n\nSocioeconomic\nI used data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019 to reoresent socioeconomic data of different regions of Houston. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file. Each layer contains a subset of the fields documents in the ACS metadata. The geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use."
  },
  {
    "objectID": "blogs/houston_blackout/houston_blackout.html#analysis",
    "href": "blogs/houston_blackout/houston_blackout.html#analysis",
    "title": "A Spatial Analysis of Houston Blackouts and the Communtiies it Affected",
    "section": "Analysis",
    "text": "Analysis\n\nFind locations of blackouts\n\ncombine the data\nFirst, I read in the night lights tiles data and combining them into a single stars object for each date (2021-02-07 and 2021-02-16).\n\n\ncode\n#reading in raster data \nt1 = read_stars(file.path(data, \"VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif\"))\nt2 = read_stars(file.path(data,\"VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif\"))\nt3 = read_stars(file.path(data,\"VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif\"))\nt4 = read_stars(file.path(data,\"VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif\"))\n\n#creating a mosaic of houston raster data for both days \nlights_mos1 &lt;- st_mosaic(t1, t2)\nlights_mos2 &lt;- st_mosaic(t3, t4)\n\n# plot(lights_mos1)\n\n\n\n\ncreate a blackout mask\nI then found the change in night lights intensity (presumably) caused by the storm. I first reclassified a difference raster, assuming that any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout. NA values were to all locations that experienced a drop of less than 200 nW cm-2sr-1.\n\n\ncode\n#finding difference in light intensity by subtracting light difference \nlight_diff &lt;- lights_mos1 - lights_mos2\n\n# plot(light_diff)\n\n#reclassify raster to give NA values to locations that did not experience a blackout\nlight_diff[light_diff &lt; 200] = NA\n\n# plot(light_diff)\n\n\n\n\nvectorize the mask\nI vectorized the the blackout mask and fixed any invalid geometries using st_make_valid. I then inspected it to make sure it was an sf object and to look at its characteristics.\n\n\ncode\n#creating a black out mask as a sf object. This creates a mask of geometries that experienced a black out\nbo_mask &lt;- st_as_sf(light_diff)\n\n#making sure geometries are valid\nst_make_valid(bo_mask)\n\n#making sure new object is an sf and inspecting it\nclass(bo_mask)\n# plot(bo_mask)\nsummary(bo_mask)\n\n\n\n\ncrop the vectorized map to Houston\nI defined the Houston metropolitan area with the following coordinates: (-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29) and then turned them into a polygon using st_polygon. A CRS was then added to the polygon to make a simple features collection using st_sfc(). Lastly, I cropped (spatially subset) the blackout mask to our region of interest and re-project the cropped blackout dataset to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area).\n\n\ncode\n#creating a matrix of coordinates for houston\n#note: last coordinate need to be same as the first\nhstn_cords &lt;- matrix(c(-96.5, 29, -96.5, 30.5, -94.5, 30.5, -94.5, 29, -96.5, 29), ncol = 2, byrow = TRUE)\n\n#creating a polygon for houston \nhstn_poly &lt;- st_polygon(list(hstn_cords))\n\nclass(hstn_poly)\n\n#making a simple feature collection using same crs as bo_mask\nhstn_sf &lt;- st_sfc(hstn_poly, crs = 4326)\n\nclass(hstn_sf)\n\n#cropping black out mask to the houston sf object \nbo_mask_houst &lt;- bo_mask[hstn_sf, ] \n\n#transforming crs to EPSG:3083 \nbo_mask_houst &lt;- st_transform(bo_mask_houst, 3083)\n\n\n\n\nCheck #1: Ensurign that the mask function worked\n\n\ncode\n##### creating a check ######\n\ncat(\"CHECK #1 - see if raster data is masked within Houston:\", \"\\n\")\n\n\nCHECK #1 - see if raster data is masked within Houston: \n\n\ncode\nif(nrow(bo_mask_houst) == nrow(bo_mask)) {\n  cat(\"No: mask did not work.\", \"\\n\")\n} else {\n  cat(\" Yes, all black out raster data lie within the Houston polygon.\", \"\\n\")\n}\n\n\n Yes, all black out raster data lie within the Houston polygon. \n\n\n \n\n\nexclude highways from blackout mask\nThe roads geopackage provoded includes data on roads other than highways. However, we can avoid reading in data we don’t need by taking advantage of st_read’s ability to subset using a SQL query. This allows us to save comutational energy by just pulling in relevant data we want.\nFirst I defined a SQL query:\nquery &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\nI applied to st_read to load just highway data from geopackage. I then reprojected the data to EPSG:3083 and identified areas within 200m of all highways using st_buffer and dissolved them with st_union. Lastly, I find areas that experienced blackouts that are further than 200m from a highway through spatial subsetting using the st_disjoint operation.\n\n\ncode\n#assigning a query that pulls highway roads\nhighway_query &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n#reading in highway data \nhighways &lt;- st_read(file.path(data,\"gis_osm_roads_free_1.gpkg\"), query = highway_query, quiet = TRUE)\n\n#transforming highways crs to 3083\nhighways &lt;- st_transform(highways, 3083)\n\n#creating a 200m buffer around highways \nhigh_buff &lt;- st_buffer(highways, dist = 200)\n\n#dissolveing buffers \nhigh_buff_dis &lt;- st_union(high_buff)\n\n# plot(high_buff_dis)\n\n#finding areas more than 200m from a highway that experienced a blackout \nmask_no_hwy &lt;- bo_mask_houst[high_buff_dis, op = st_disjoint]\n\n\n\n\nCheck #2: see if the highway exclusion mask removed all polygons within the 200m of highways\n\n\ncode\n#### creating a check #######\ncat(\"CHECK #2 - see if the highway exclusion mask removed all polygons within the 200m of highways:\", \"\\n\")\n\n\nCHECK #2 - see if the highway exclusion mask removed all polygons within the 200m of highways: \n\n\ncode\nif(nrow(mask_no_hwy) + nrow(bo_mask_houst[high_buff_dis, ]) == nrow(bo_mask_houst)) {\n  cat(\"Yes, the highway mask removed all polygons within the 200m of highways.\", \"\\n\")\n  } else {\n    cat(\"Error! Observations within highway mask are included.\", \"\\n\")\n  }\n\n\nYes, the highway mask removed all polygons within the 200m of highways. \n\n\n \n\n\n\nFind homes impacted by blackouts\n\nload buildings data\nThe buildings dataset was then loaded in using st_read and the following SQL query:\nSELECT *  FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\n\nAgain, this reduces the amount of data pulled inoto R, saving us time, energy, and storage.\n\n\ncode\n#making a query for pulling specific building data\nbuildings_query &lt;- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\n#reading in building data \nbuildings &lt;- st_read(file.path(data, \"gis_osm_buildings_a_free_1.gpkg\"), query = buildings_query, quiet = TRUE)\n\n#transforming crs to 3083 \nbuildings &lt;- st_transform(buildings, 3083)\n\n# plot(buildings)\n\n\n\n\nfind homes in blackout areas\nI then filtered the mask to homes within blackout areas and counted the number of impacted homes.\n\n\ncode\n#subsettign the number of homes experiencing blackouts \n#I am including areas inside of houston that also extend outside of Houston border using st_intersects\nbo_homes &lt;- buildings[mask_no_hwy, ]\n\n#counting blackout homes\nbo_homes_cnt &lt;- comma(nrow(bo_homes))\n\ncat(\"Number of homes that experienced blackouts during the storm:\", bo_homes_cnt, \"\\n\")\n\n\nNumber of homes that experienced blackouts during the storm: 139,148 \n\n\n\n\n\n\nInvestigate socioeconomic factors\n\nload ACS data\nFirst, I read in the geodatabase layers using st_read() (geometries are stored in the ACS_2019_5YR_TRACT_48_TEXAS layer and income data is stored in the X19_INCOME layer). For this analysis, I used the median income field B19013e1 for income. Data was then reprojected to EPSG:3083.\n\n\ncode\n# reading ACS gdb layers\nst_layers(file.path(data, \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"))\n\n# reading in geometries layer\nacs_geoms &lt;- st_read(file.path(data, \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"), layer = \"ACS_2019_5YR_TRACT_48_TEXAS\") %&gt;% \n  st_transform(\"EPSG:3083\")\n\n# reading in income layer\n#selecting just GEOID and median income \nacs_income &lt;- st_read(file.path(data, \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"), layer = \"X19_INCOME\") %&gt;% \n  select(GEOID_Data = GEOID, med_income = B19013e1)\n\n\n\n\ndetermine which census tracts experienced blackouts\nNext, I joined the income data to the census tract geometries uaing a left_join function by “ID”. This was then spatially joined to the census tract data with buildings determined to be impacted by blackouts using the st_intersects operation. Lastly, census tracts that had blackouts were identified.\n\n\ncode\n#finding out how many and which census tracts experienced a blackout\n\n#joining income data to geometries \nacs_join &lt;- left_join(acs_geoms, acs_income)\n\n# spatial join census tract data with buildings \nspat_join &lt;- acs_join %&gt;% \n   st_filter(bo_homes, .predicate = st_intersects)\n\n# length(spat_join)\n# names(spat_join)\n\n#census tracts that experience a blackout\nbo_tract_list &lt;- unique(spat_join$NAMELSAD)\n\n#count of census tracts that experience a blackout\nbo_tract_count &lt;- length(bo_tract_list)\n\n\n\n\nCHECK #3: making sure only census tracts with homes that experienced a blackout are included:\n\n\ncode\n### creating a check #####\ncat(\"CHECK #3 - making sure only census tracts with homes that experienced a blackout are included:\", \"\\n\")\n\n\nCHECK #3 - making sure only census tracts with homes that experienced a blackout are included: \n\n\ncode\nif(nrow(acs_join) &gt; nrow(spat_join)) {\n  cat(\"Yes, only black out census tracts included.\", \"\\n\")\n} else {\n  cat(\"No, filter didn't work\", \"\\n\")\n}\n\n\nYes, only black out census tracts included. \n\n\n\n\ncompare incomes of impacted tracts to unimpacted tracts\nFinally, I created a map of median income by census tract and designated which tracts experienced blackouts. I then plot the distribution of income in impacted and nonimpacted tracts to visualize what the spread of the community income looked like.\n\n\ncode\n# In this section, I am mapping the median income of affected census tracts from the black out to visually show what the income of affected areas are in Houston\n\n#check census tract crs\nst_crs(acs_join)\n\n#transform Houston polygon crs to match census tract crs\nhstn_3083 &lt;- st_transform(hstn_sf, 3083)\n\n# masking census tracts to houston\ntract_mask &lt;- acs_join[hstn_3083, ]\n\n#making a new column saying whether or not tracts was a black out or not\ntract_mask &lt;- tract_mask %&gt;% \n  mutate(blackout = ifelse(NAMELSAD %in% bo_tract_list, \"TRUE\", \"FALSE\"))\n\n\n\n# mapping median income of census tracts that experienced blackouts\n\n# getting a polygon of texas \ntexas &lt;- us_states %&gt;% \n  filter(NAME == \"Texas\") %&gt;% \n  st_transform(3083)\n\n#making an inset map of texas with Houston filled in red  \ninset_map &lt;- ggplot() +\n  geom_sf(data = texas) + \n  geom_sf(data = hstn_3083, fill = \"red\") + \n  theme_bw() +\n   theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.grid.minor = element_blank())\n\n\n#creating a main map of census tracts that experienced blackouts by color \nmain_map &lt;-\n  ggplot() +\n  geom_sf(data = tract_mask, color = \"lightgrey\") +\n  geom_sf(data = spat_join, aes(fill = med_income)) +\n  scale_fill_viridis_c() +\n  labs(fill = \"Median Income ($)\") +\n  theme_minimal() +\n  annotation_scale(plot_unit = \"m\", location = \"bl\") +\n  theme( legend.position= c(1.25, .75),\n         plot.margin=unit(c(1,6,.5,1),\"cm\")) +\n  annotation_north_arrow(location = \"bl\",\n                         pad_x = unit(0.05, \"in\"),\n                         pad_y = unit(0.3, \"in\"),\n                         style = ggspatial::north_arrow_minimal())\n\n#combining main and inset map \n comb_map &lt;- main_map + \n    inset_element(inset_map, 1, -0.4, 1.4, .9) +\n  plot_annotation(tag_levels = \"A\")\n \n comb_map\n\n\n\n\n\nFigure 1. Median income of census tracts in Houston, TX that experienced blackouts in February 2021. Map A shows all census tracts in Houston area, were tracts that experienced a blackout color coded by the median income of houslholds within the tracts. Brighter colored tracts have higher median incomes where darker colored tracts have lower. Map B shows an inset map of where Houston is located in the state of Texas.\n\n\n\n\n\n\n\ncode\n# here I am creating a histogram and box plot to compare the median incomes of affected and unaffected tracts\n\n#first, calculate median of median incomes for both groups \nmeds &lt;- tract_mask %&gt;% \n  group_by(blackout) %&gt;% \n  summarize(med_med_income = median(med_income, na.rm = TRUE)) %&gt;% \n  st_drop_geometry()\n\n\n#creating a histogram of income distribution \nhist &lt;- ggplot(tract_mask, aes(x = med_income, fill = blackout)) +\n  geom_histogram(bins = 65) +\n  labs(x = \"Median income ($)\", y = \"Frequency\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  geom_segment(data = meds, aes(x = med_med_income, xend = med_med_income, y = 0, yend = 80, color = blackout), linetype = \"dashed\") +\n  # geom_vline(data = meds, aes(xintercept = med_med_income, color = blackout), linetype = \"dashed\") +\n  scale_fill_manual(name = \"\", values = c( \"lightblue\", \"orange\"), labels = c(\"No blackouts\", \"Blackouts\" )) +\n  scale_color_manual(name = \"Median\", values = c( \"blue\", \"orange3\"), labels = c(\"No blackouts\", \"Blackouts\" )) \n  # coord_cartesian(ylim = c(0, 75))\n  \n  \n#creating a boxplot of median income distribution \nbox_p &lt;- ggplot(tract_mask, aes(x = blackout, y = med_income, fill = blackout)) +\n  geom_boxplot(width = 0.5, show.legend = FALSE) +\n  # geom_jitter(width = 0.2, alpha = 0.5, color = \"grey\") +  \n  labs(y = \"Median Income ($)\", x = \"\") +\n  theme_minimal() +\n  scale_fill_manual( name = \"\", values = c( \"lightblue\", \"orange\"), labels = c(\"No blackouts\", \"Blackouts\")) +\n  scale_x_discrete(labels = c(\"FALSE\" = \"No blackouts\", \"TRUE\" = \"Blackouts\")) \n\n\n#adding up plots into one using patchwork\npatched &lt;- hist + box_p + \n  plot_annotation(tag_levels = \"A\") +\n  plot_layout(guides = \"collect\") &\n  theme(legend.position = \"bottom\")\n\npatched\n\n\n\n\n\nFigure 2. Median houshold income distribution of census tracts in blackout vs non-blackout areas in Houston, 2021. The first plot A shows a histogram of the distribution of median income in both groups of tracts. The frequency is concentrated just under $50,000 for both groups. Data is abnormally distributed with a long right tail, indiating most housholds are lower income with some higher income households drawing the tail right. Plot B shows a whisker plot of median distribution. Boxes represent the inter quartile range (IQR, 50% of the data), and the line inside each box represents the median. The whiskers represent 1.5 times the length of the IQR, with some outliers falling above the whiskers.\n\n\n\n\n\n\n\n\nConclusion\nOverall, these results show that census tracts had a similar median income distribution regardless of whether or not they experienced a black out in the February 2021 storm, but income was slightly higher for tracts that experienced a blackout than those that did not. The median of the census tract median income distribution was $57,002 for non-blackout tracts and $60,642 for blackout tracts. The majority of the income distribution is centered at about $35,000 - $45,000, with some outliers in the $200,000 + range. Some of the limitations of the study are that the census tracts don’t represent how many homes experienced a black out or how long the black out was experienced for. These factors could have underlying socioeconomic patterns that we can’t see here which could explain why certain tracts experienced blackouts for the duration that they did. This should be an area for fututre investigation to strengthen the results found in this study."
  },
  {
    "objectID": "blogs/houston_blackout/houston_blackout.html#footnotes",
    "href": "blogs/houston_blackout/houston_blackout.html#footnotes",
    "title": "A Spatial Analysis of Houston Blackouts and the Communtiies it Affected",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLaJoie, A. S., Sprang, G., & McKinney, W. P. (2010). Long‐term effects of Hurricane Katrina on the psychological well‐being of evacuees. Disasters, 34(4), 1031-1044.↩︎\nHess, D. B. (2005). Access to employment for adults in poverty in the Buffalo-Niagara region. Urban Studies, 42(7), 1177-1200.↩︎\nHess, D. B. (2005). Access to employment for adults in poverty in the Buffalo-Niagara region. Urban Studies, 42(7), 1177-1200.↩︎\nLi, Z., Li, X., Wang, Y., & Quiring, S. M. (2019). Impact of climate change on precipitation patterns in Houston, Texas, USA. Anthropocene, 25, 100193.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "",
    "section": "",
    "text": "Background\n\nI currently work as a Data Analyst at the NOAA Fisheries (NMFS). How did I get here? My journey in environmental data began at the University of California, Santa Cruz, where I earned a B.S. in Ecology and Evolutionary Biology and a B.A. in Environmental Studies. I held research technician positions studying aquatic insect and microbial communities across California, Puerto Rico, and Mexico.\nI later moved to Mammoth Lakes, CA, to work as a Freshwater Ecologist at the Sierra Nevada Aquatic Research Center (SNARL), where I researched the long term ecological impacts of acid mine drainage on alpine stream ecosystems. (And I may have had a bit too much fun romping around in the mountains when I wasn’t working)\nKnowing I wanted to dive deeper into data, I pursued a Master’s in Environmental Science and Management at the Bren School at UC Santa Barbara, with a focus in Data Science. During this time, I served as a Data Manager on a NOAA project modeling salmon restoration costs and worked as a Data Scientist Intern for The Nature Conservancy, studying potential invasive species on the Channel Islands.\nThese experiences built the foundation for the data-driven career I’m lucky to have today.\n\nData Projects\n\nCurrent\nI work as a Data Analyst with a team of economists at Northwest Fisheries Science Center (a branch of NOAA Fisheries). My research involves performing analyses to evaluate the effectiveness of commercial fishing vessels and processors that participate in a catch share program to promote sustainable fishing practices among a variety of fisheries. Specifically, I work in R and SQL crunching numbers, building pipelines, running machine learningmodels, and developing visual applications to turn data into insight. I then write up these findings and publish the tools so that stakeholders can engage with the data and understand how are fisheries are chaning over time.\nI work part time doing data related contracts as well, so feel free to reach out if you have work in mind you would like assistance with. Recently, I built a Shiny application for Comunidad y Biodiversidad, a Mexican non-profit that supports small, local fisheries. The app serves as an evaluation tool that enables users to assess the effectiveness of marine protected area across Mexico. Check it out here.\nPrevious\nAt UCSB, I served as the data manager for my masters capstone working with NOAA. I designed extensive spatial models in R and ArcGIS utilizing heterogeneous vector and raster data to estimate salmon habitat restoration costs in WA state. We modeled watershed characteristic data to estimate habitat restoration costs. I oversaw the project GitHub repository, analytcial pipeline, and developed an interactive Shiny app to visualize and communicate our findings which you can checkout here.\nI also spent a summer interning with The Nature Conservancy as a Biosecurity Data Scientist to study how potential invasive rodents respond to endemic predators on the Channel Islands. I designed and executed a robust study from scratch while mentoring an undergraduate in the field. Through methodical data collection, statistical modeling, and data visualization, we were able to inform TNC management on the potential shortcomings of existing biosecurity protocols. The study resulted in empirical evidence on how to improve conservation efforts within TNC. I presented our research at the annual California Islands Symposium as well as at the Point Conception Institute Symposium in 2024.\n\n\n\n\n\n\n\n\n\nField work in Puerto Rico\n\n\n\n\n\n\n\nHolding a desert iguana in Baja California\n\n\n\n\n\n\n\n\n\nSome beautiful microbes from Puerto Rico\n\n\n\n\n\n\n\nField work in Big Sur, CA\n\n\n\n\n\n\nIn my free time\n\nWhile much of my life passion lies in working with data, I enjoy getting outside equally as much. Some of my favorite activities include: rock climbing, surfing, paragliding, spearfishing, running, and practicing yoga.\n\n\n\n\n\n\n\n\n\nSome California Surf!\n\n\n\n\n\n\n\nFrench Polynesia exploring\n\n\n\n\n\n\n\n\n\nMountaineering trip up Matterhorn Peak in the Sierra Nevada mountains\n\n\n\n\n\n\n\nPreparing for a multiday ascent up El Capitan, Yosemite\n\n\n\n\n\n\n\n\n\nBig wall climbing on El Capitan, Yosemite\n\n\n\n\n\n\n\nEl Capitan, Yosemite\n\n\n\n\n\n\n\n\n\nSnack during a backcountry ski tour\n\n\n\n\n\n\n\nClimbing Lone Pine Peak, California\n\n\n\n\n\n\n\n\n\nBeach day in Kauai\n\n\n\n\n\n\n\nParagliding in the eastern Sierra Nevada mountains"
  },
  {
    "objectID": "blogs/china_fishing/index.html",
    "href": "blogs/china_fishing/index.html",
    "title": "The World’s Largest Fishing Fleet",
    "section": "",
    "text": "Github Project Repository"
  },
  {
    "objectID": "blogs/china_fishing/index.html#introduction",
    "href": "blogs/china_fishing/index.html#introduction",
    "title": "The World’s Largest Fishing Fleet",
    "section": "Introduction",
    "text": "Introduction\nThis blog post is intended to show how to create an infographic from start to finish while considering 10 different design elements along the way: graphic forms, text, themes, colors, typography, general design, data contextualization, primary message, accessibility, and diversity, equity, and inclusion (DEI). I used publically available data provided by Global Fishing Watch (GFW) to investigate and compare different aspects of China’s illegal and legal fishing fleets. While showing figures and plots individually has its relevance, stitching together a story through multiple visualizations and text by creating an infographic can be a powerful tool. It allows the reader to walk through an oftentimes artistic thought process or an idea in multiple steps rather than interperet a stand alone figure. However, it is improtant to keep in minds the draw backs that can come with infographics such as confusing the reader through too much complext visualization. Here, I will walk you through how I created an artistic, simple, yet compelling story on China’s fishing vessels through a tree map, donut chart, and barchart."
  },
  {
    "objectID": "blogs/china_fishing/index.html#about-the-data",
    "href": "blogs/china_fishing/index.html#about-the-data",
    "title": "The World’s Largest Fishing Fleet",
    "section": "About the Data",
    "text": "About the Data\nThis data set I used is publicly available on the Global Fishing Watch (Data Link) website. The website contains a number of different data sets on global fishing vessels with the goal of trying highlighting illegal fishing vessel activity. The specific data set I used was called fishing-vessels-v2.csv which is the most up-to-date public data set on predicted legal/illegal fishing vessel status. It was generated by GFW using machine learning to predict which vessels were actively fishing regardless of whether or not they identified as a legal fishing vessel. The data identifies global fishing vessels that used Automatic Identification System (AIS) from 2012-2020 by country. It contains a number of different variables describing the fishing vessel class, engine power, length, weight, fishing time by year, country, and whether or not it was a self reported AIS fishing vessel. This data allows us to find which vessel classes by country are using AIS around the globe and report whether they are operating legally or illegally. I primarily focused on using the variables mmsi (the maritime mobile service identity or vessel identification number), flag_gfw (the country the vessel represents), vessel_class_gfw (the predicted vessel class), and self_reported_fishing_vessel (whether the vessel broadcasts itself as a fishing vessel using AIS) as my variables of interest."
  },
  {
    "objectID": "blogs/china_fishing/index.html#questions",
    "href": "blogs/china_fishing/index.html#questions",
    "title": "The World’s Largest Fishing Fleet",
    "section": "Questions",
    "text": "Questions\nI really wanted to look at China’s illegal and legal fishing vessels at a broad scale to compare across countries and identify dominant fishing vessel classes within the fleets. I asked the following questions to do so:\n1) How large are China’s illegal and legal fishing fleets using automatic identification system (AIS) compared to the next 9 largest countries’?\n2) What percentage of China’s fleet using AIS legally reported its fishing status between 2012-2020?\n3) What were the total number of illegal and legal fishing vessels by class using AIS between 2012-2020?"
  },
  {
    "objectID": "blogs/china_fishing/index.html#set-up",
    "href": "blogs/china_fishing/index.html#set-up",
    "title": "The World’s Largest Fishing Fleet",
    "section": "Set up",
    "text": "Set up\nHere I loaded some packages, imported some fonts and icons, and picked some colors for the theme of the infographic:\n\n\ncode\n# loading necessary packages\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(showtext) # for grabbing fonts\nlibrary(png)\nlibrary(grid)\nlibrary(ggpubr)\nlibrary(patchwork)\nlibrary(treemapify)\nlibrary(ggtext)\nlibrary(glue)\nlibrary(here)\n\n# custom icons and fonts fonts \nfont_add(family = \"Font Awesome 6 Brands\",\n         regular = here(rootdir, \"otfs/Font-Awesome-6-Brands-Regular-400.otf\"))\nfont_add_google(name = \"Sen\", \n                family = \"sen\")\nfont_add_google(name = \"Playfair Display\", \n                family = \"play\")\nshowtext_auto() # for importing fonts\n\n# github icon\ngithub_icon &lt;- \"&#xf09b\"\ngithub_username &lt;- \"ramhunte\"\ngh_caption &lt;- glue::glue(\n  \"&lt;span style='font-family:\\\"Font Awesome 6 Brands\\\";'&gt;{github_icon};&lt;/span&gt;\n  &lt;span style='color: white'&gt;{github_username}&lt;/span&gt;\"\n  )\n\n# custom color palette\npal &lt;- c(light_text = \"#f4c430\",\n         dark_text = \"#b22222\",\n         bg = \"#516776\")"
  },
  {
    "objectID": "blogs/china_fishing/index.html#data-wrangling",
    "href": "blogs/china_fishing/index.html#data-wrangling",
    "title": "The World’s Largest Fishing Fleet",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nBefore I can create any plots, I need to wrangle and clean the data so that it can be used for the three different plots:\n\n\ncode\n# Cleaning and warngling data \n\n# master data sheet\nboats &lt;- read.csv(here(rootdir, \"data/fishing-vessels-v2.csv\"))\n\n# clean sheet\nboats_clean &lt;- boats %&gt;%\n  #cleaning names\n  clean_names() %&gt;%\n  # selecting specific columns\n  select(mmsi, flag_gfw, vessel_class_gfw, tonnage_gt_gfw, self_reported_fishing_vessel,  fishing_hours_2012:fishing_hours_2020) %&gt;%\n  # filtering out na values\n  filter(!is.na(flag_gfw)) %&gt;%\n  # changing class of variables\n  mutate(mmsi = as.character(mmsi),\n         flag_gfw = as.factor(flag_gfw)) %&gt;%\n  #finding proportion of boats that are self reported by country\n  group_by(flag_gfw) %&gt;%\n  mutate(prop_self_rep = mean(self_reported_fishing_vessel, na.rm = TRUE),\n         n_vessels = n()) %&gt;%\n  #moving to column #4\n  relocate(prop_self_rep, .after = 4) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(cols = starts_with(\"fishing\"),\n               names_to = \"year\",\n               values_to = \"fishing_time\") %&gt;%\n  mutate(year = str_remove(year, \"fishing_hours_\"))\n\n\n# ---------------- Barchart Data --------------------\n\nbarchart_df &lt;- boats_clean %&gt;%\n  filter(flag_gfw == \"CHN\",\n         !is.na(self_reported_fishing_vessel)) %&gt;%\n  # renaming vessels to other\n  mutate(vessel_class_gfw = ifelse(vessel_class_gfw %in% c(\"trawlers\", \"fishing\", \"set_gillnets\", \"set_longlines\", \"fixed_gear\"), vessel_class_gfw, \"other\")) %&gt;%\n  # finding the total number of vessesl by class and legality\n  group_by(mmsi, vessel_class_gfw) %&gt;%\n  summarise(self_reported_fishing_vessel = self_reported_fishing_vessel) %&gt;%\n  distinct() %&gt;%\n  group_by(vessel_class_gfw, self_reported_fishing_vessel) %&gt;%\n  summarise(count = n())\n\n# -------------- Donut Chart Data-------------------\n\n#wrangling data for a donut chart\ndonut_df &lt;- boats_clean %&gt;%\n  filter(flag_gfw == \"CHN\",\n         !is.na(self_reported_fishing_vessel)) %&gt;%\n  group_by(mmsi, vessel_class_gfw) %&gt;%\n  summarise(self_reported_fishing_vessel = self_reported_fishing_vessel) %&gt;%\n  # filtering just distinct number of individual vessels\n  distinct() %&gt;%\n  group_by(self_reported_fishing_vessel) %&gt;%\n  summarize(count = n()) %&gt;%\n  mutate(self_reported_fishing_vessel = ifelse(self_reported_fishing_vessel, \"Legal\", \"Illegal\"),\n         percentage = round(count/sum(count)*100, digits = 1),\n         ymax = cumsum(percentage),\n         ymin = lag(ymax, default = 0),\n         labelPosition = (ymax + ymin)/2,\n         label = paste0(percentage, \"%\"))\n\n# finding the total count of all chinese fishing vessels\ntotal_count &lt;- format(sum(donut_df$count),\n                      big.mark = \",\")\n\n# ----------- Tree map data -----------------\n\n# wrangling top 10 countries by vessel count to compare\ntop_10 &lt;- boats_clean %&gt;%\n  distinct(mmsi, flag_gfw, self_reported_fishing_vessel) %&gt;%\n  group_by(flag_gfw) %&gt;%\n  summarise(count = n()) %&gt;%\n  slice_max(order_by = count, n = 10)\n\n# wrangling a data set for the treemap figure\ntree_df &lt;- boats_clean %&gt;%\n  # filter to just top 10 countries\n  filter(flag_gfw %in% top_10$flag_gfw,\n         !is.na(self_reported_fishing_vessel)) %&gt;%\n  # only pull out distinct vessels\n  distinct(mmsi, flag_gfw, self_reported_fishing_vessel) %&gt;%\n  group_by(flag_gfw, self_reported_fishing_vessel) %&gt;%\n  summarise(count = n()) %&gt;%\n  # rename to full country\n  mutate(country = case_when(\n    flag_gfw == \"CHN\" ~ \"China\",\n    flag_gfw == \"ESP\" ~ \"Spain\",\n    flag_gfw == \"GBR\" ~ \"UK\",\n    flag_gfw == \"ISL\" ~ paste0(\"Ice-\",\"\\n\", \"land\"),\n    flag_gfw == \"ITA\" ~ \"Italy\",\n    flag_gfw == \"JPN\" ~ \"Japan\",\n    flag_gfw == \"KOR\" ~ \"Korea\",\n    flag_gfw == \"NOR\" ~ \"Norway\",\n    flag_gfw == \"TWN\" ~ \"Taiwan\",\n    flag_gfw == \"USA\" ~ \"USA\"))"
  },
  {
    "objectID": "blogs/china_fishing/index.html#tree-map",
    "href": "blogs/china_fishing/index.html#tree-map",
    "title": "The World’s Largest Fishing Fleet",
    "section": "Tree Map",
    "text": "Tree Map\nNote: As in all plots, red represents legal and yellow represents illegal. I also did not include titles, subtitles, captions, or guides in any of the three stand along plots that I created as they are all going to be small pieces of a larger infographic in which these details will be communicated. However, it is critical to include these components to your visualization if they are stand alone and do not have the additional aid from the inforgraphic.\nThe first figure I want to create is a tree map that compares the total sizes of the top 10 countries’ illegal and legal fishing vessels using AIS between 2012 and 2020. This will answer my main question:\nHow large are China’s illegal and legal fishing fleets using automatic identification system (AIS) compared to the next 9 largest countries?\n\n\ncode\n# making a tree map\ntree_fig &lt;-  ggplot(tree_df, aes(area = count, fill = self_reported_fishing_vessel,\n                                 label = self_reported_fishing_vessel, \n                                 subgroup = country)) +\n  # labs(title = \"Comparing the World's 10 Largest Fishing Fleets\") +\n  scale_fill_manual(values = c(\"#f4c430\", \"#b22222\")) +\n  geom_treemap() +\n  geom_treemap_subgroup_border(colour = \"white\", size = 2) +\n  geom_treemap_subgroup_text(place = \"centre\", grow = TRUE,\n                             alpha = 0.7, \n                             lineheight = 0.3,\n                             colour = \"white\",\n                             size = 2) +\n  theme(\n    panel.background = element_rect(fill = \"transparent\", color = NA),\n    plot.background = element_rect(fill = \"transparent\", color = NA),\n    legend.position = \"none\")"
  },
  {
    "objectID": "blogs/china_fishing/index.html#donut-chart",
    "href": "blogs/china_fishing/index.html#donut-chart",
    "title": "The World’s Largest Fishing Fleet",
    "section": "Donut Chart",
    "text": "Donut Chart\nMy next plot will be a donut chart that answers our second question:\nWhat percentage of China’s fleet using AIS legally reported its fishing status between 2012-2020?\n\n\ncode\n# Making a donut chart\ndonut &lt;- ggplot(data = donut_df, aes(ymax=ymax, ymin=ymin, xmax=4, \n                                     xmin=3, fill = self_reported_fishing_vessel)) +\n  geom_rect() +\n  # labeling percentages\n  geom_label( x=3.5, aes(y=labelPosition, label=label), size=20,\n              color = \"white\",\n              label.size = NA,\n              family = \"sen\",\n              fill = NA,\n              lineheight = unit(.3, \"lines\")) +\n  # adding annotation for total number of vessels\n  annotate(geom = 'text', x = 1.7, y = 1, label = paste(total_count, \"\\n\", \"Vessels\"),\n           size = 30, color = \"white\", lineheight = unit(.3, \"lines\"), family = \"sen\") +\n  scale_fill_manual(values = c(\"#f4c430\", \"#b22222\")) +\n  coord_polar(theta=\"y\") +\n  xlim(c(1.7, 4)) +\n  theme_void() +\n  theme(legend.position = \"none\",\n        panel.background = element_rect(fill = NA, color = NA),\n        plot.background = element_rect(fill = NA, color = NA))\n\n# making a donut plot with colored background for displaying individually\ndonut_filled &lt;- donut +\n  theme(panel.background = element_rect(fill = pal[\"bg\"], color = NA),\n    plot.background = element_rect(fill = pal[\"bg\"], color = NA))"
  },
  {
    "objectID": "blogs/china_fishing/index.html#barchart",
    "href": "blogs/china_fishing/index.html#barchart",
    "title": "The World’s Largest Fishing Fleet",
    "section": "Barchart",
    "text": "Barchart\nMy last plot will be a barchart answering the final question:\nWhat were the total number of illegal and legal fishing vessels by class using AIS between 2012-2020?\nI added value lables on here because I removed the x axis for simplicity in design. It important to make sure that as you modify or remove elements that you are making sure that information is not entirely lost as it will confuse the reader.\n\n\ncode\n# making a barchart\nbarchart &lt;- ggplot(data = barchart_df, aes(x = fct_reorder(vessel_class_gfw, count), \n                                           y = count, fill = self_reported_fishing_vessel)) +\n  # adding count text to each bar\n  geom_text(aes(label = scales::comma(count), hjust = -0.1), color = \"white\", size = 20,\n            position = position_dodge(width = .9),\n            family = \"sen\") +\n  theme_void() +\n  geom_col(position = \"dodge\") +\n  labs(fill = \"Self Registered\", \n       x = \"\", \n       y = \"\") +\n  scale_fill_manual(values = c(\"#f4c430\", \"#b22222\")) +\n  scale_y_continuous(limits = c(0, 30000)) +\n  scale_x_discrete(labels = c(\"trawlers\" = \"Trawlers\", \n                              \"fishing\" = \"Unknown\", \n                              \"set_longlines\" = \"Longlines\",\n                              \"set_gillnets\" = \"Gillnets\", \n                              \"other\" = \"Mixed Other\", \n                              \"fixed_gear\" = \"Fixed Gear\")) +\n  coord_flip() +\n  theme(\n    # note: need to make these fills as transparent for final figure when stitching together\n    # they are colored right now so we can show the bacgkround color in the individual plot\n    panel.background = element_rect(fill = NA, color = NA),\n    plot.background = element_rect(fill = NA, color = NA),\n    # axis \n    axis.line.x = element_blank(),\n    axis.line.y = element_blank(),\n    axis.text.y = element_text(color = \"white\", family = \"sen\",size = 48, \n                               margin = margin(-30, -20, -50, 0)),\n    axis.text.x = element_blank(),\n    # legend\n    legend.position = \"none\"\n  )\n\n\n# making a barchart with colored background for displaying individually\nbarchart_filled &lt;- barchart +\n  theme(panel.background = element_rect(fill = pal[\"bg\"], color = NA),\n    plot.background = element_rect(fill = pal[\"bg\"], color = NA))"
  },
  {
    "objectID": "blogs/china_fishing/index.html#text-annotation",
    "href": "blogs/china_fishing/index.html#text-annotation",
    "title": "The World’s Largest Fishing Fleet",
    "section": "Text Annotation",
    "text": "Text Annotation\nNow we are going to create some text annotation to this plot to explain exactly what AIS is and provide some context to guide the reader through our visuals. This step is very important as my plots do not have any titles or subtitles, so explaining missing links that explain the data and figures will be crucial for the reader to follow along:\n\n\ncode\n# making annotation texts for the infographic \nquote &lt;- paste0(\"China boasts the world's largest fishing fleet, with over 73K vessels that utilized the Automatic Identification System (AIS) between 2012 and 2020. AIS reveals a vessel's location and fishing activities. AIS usage is mandatory for many larger vessels, but not all comply in broadcasting their fishing status. Approximately 27.4% of these vessels failed to broadcast their fishing status, indicating potential illegal fishing activities according to data from Global Fishing Watch. These vessels predominantly included: \")\n\nboat_quote &lt;- paste0(\"Trawlers: pull large nets across the sea floor \", \"\\n\",\n                      \"Longlines: pull long fishing lines through the water with multiple baited\", \"\\n\", \"                  hooks attatched to each single line\", \"\\n\", \n                      \"Gillnets: pull large nets through the middle water column\", \"\\n\", \n                      \"Fixed Gear: place fixed objects such as nets and traps tethered to buoys\")\n\n# making quotes a ggplot object\nquote_gg &lt;- ggplot() +\n  geom_rect(aes(xmin = 0, xmax = 1, ymin = 0.1, ymax = Inf), fill = \"lightblue\", alpha = 0.7) +  # Add blue background\n  annotate(\"text\", x = 0.03, y = .73, label = str_wrap(quote, width = 80),\n           family = \"sen\", colour = \"black\", size = 16, \n           lineheight = 0.32, hjust = 0, vjust = 0.4) +\n  annotate(\"text\", x = 0.03, y = .28, label = boat_quote,\n           family = \"sen\", colour = \"black\", size = 18,\n           lineheight = 0.32, hjust = 0, vjust = 0.4, fontface = \"bold\") +\n  \n  xlim(0, 1) +\n  ylim(0, 1) +\n  theme_void()"
  },
  {
    "objectID": "blogs/china_fishing/index.html#base-infographic",
    "href": "blogs/china_fishing/index.html#base-infographic",
    "title": "The World’s Largest Fishing Fleet",
    "section": "Base Infographic",
    "text": "Base Infographic\nWe are now getting towards the end. I want to make a base template in ggplot2 that will serve as the foundation of my infographic with the title, subtitle, and caption. I do so by creating an empty ggplot() object and adding labels to it along with with my github information in the caption. I also set the color background here and adjust the margins:\n\n\ncode\n# Base template ggplot for infographic \ng_base &lt;- ggplot() +\n  # geom_segment(aes(x = .5, y = .5, xend = .5, yend = 0.6), color = \"black\") +\n  labs( \n    title = \"The World's Largest Fishing Fleet\",\n    subtitle = \"China's &lt;span\nstyle='color:#f4c430;font-size:78pt;'&gt;**Illegal**&lt;/span&gt; and &lt;span style='color:#b22222;font-size:78pt;'&gt;**Legal**&lt;/span&gt; Fishing Vessels Using AIS (2012-2020)\",\n caption = gh_caption) +\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    text = element_text(family = \"play\", size = 60, \n                        lineheight = 0.3, colour = \"white\"),\n    plot.background = element_rect(fill = pal[\"bg\"], colour = pal[\"bg\"]),\n    plot.title = element_text(size = 128,face = \"bold\", \n                              hjust = 0.5, margin = margin(b = 10)),\n    plot.subtitle = element_markdown(family = \"sen\", hjust = 0.5, \n                                     margin = margin(b = 20)),\n    plot.caption = element_markdown(family = \"sen\", hjust = 0.5, \n                                     margin = margin(b = 10)),\n    plot.margin = margin(b = 20, t = 50, r = 50, l = 50),\n    axis.text.x = element_blank() \n  )"
  },
  {
    "objectID": "blogs/china_fishing/index.html#adding-creativity",
    "href": "blogs/china_fishing/index.html#adding-creativity",
    "title": "The World’s Largest Fishing Fleet",
    "section": "Adding Creativity",
    "text": "Adding Creativity\nLastly, we want to spice up our figure and add some fishy themed elements to make it pop out and not just look like a bunch of statistics. I just grabbed and edited some basic marine-themed PNG’s from google which can go a long way. It is important to make sure that these visuals are not overpowering your infrographic or too distracting from the figures:\n\n\ncode\n# read in PNG images for spicing up the grpahics\n\n# hooks\nhookstem_donut &lt;- readPNG(here(rootdir, \"images/hookstem_donut.png\"))\nhook_donut &lt;- readPNG(here(rootdir, \"images/hook_donut.png\"))\nhookstem_bar &lt;- readPNG(here(rootdir, \"images/hookstem_bar.png\"))\nhook_bar &lt;- readPNG(here(rootdir, \"images/hook_bar.png\"))\n# fish banner\nfish_school &lt;- readPNG(here(rootdir, \"images/fish_school.png\"))\n\n\n# donut plot hooks\nhookstem_donut_gg &lt;- ggplot() +\n  annotation_custom(rasterGrob(hookstem_donut), xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf) +\n  theme_void()\n\nhook_donut_gg &lt;- ggplot() +\n  annotation_custom(rasterGrob(hook_donut), xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf) +\n  theme_void()\n\n# barchart hooks\nhookstem_bar_gg &lt;- ggplot() +\n  annotation_custom(rasterGrob(hookstem_bar), xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf) +\n  theme_void()\n\nhook_bar_gg &lt;- ggplot() +\n  annotation_custom(rasterGrob(hook_bar), xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf) +\n  theme_void()\n\n# fishing hook line\nline1 &lt;- ggplot() + geom_segment(aes(x = .5, y = 0, xend = .5, yend = 1), color = \"grey2\", linewidth=1.3) +\n  theme_void()\n\n# fish banner\nfish_school_gg &lt;- ggplot() +   annotation_custom(rasterGrob(fish_school), xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf) +\n  theme_void()"
  },
  {
    "objectID": "blogs/china_fishing/index.html#stitching-together",
    "href": "blogs/china_fishing/index.html#stitching-together",
    "title": "The World’s Largest Fishing Fleet",
    "section": "Stitching Together",
    "text": "Stitching Together\nWe are now at the final step now that all the figures, text, and infographic base have been created. I am using the patchwork package to add layers of inset elements/plots (using inset_element() so that our figures and creative elements are nicely stacked. Note: I had to adjust the fill to be NA for the plots in order to get them to not block one another when stacking my inset elements.\nWe are then left with the final result:\n\n\ncode\n# adding together all elements of infographic\nfinal_fig &lt;- g_base +\n  # fish banner\n  inset_element(fish_school_gg, left = -0.05, right = 1.05, top = 0.4, bottom = 0) +\n  # fishing line \n  inset_element(line1, left = .155, right = .17, top = .8, bottom = 0.425) +\n  # fishing line\n  inset_element(line1, left = .73, right = .74, top = .8, bottom = 0.34) +\n  # tree map\n  inset_element(tree_fig, left = 0, right = 1, top = 1, bottom = 0.7) +\n  # hookstem donut \n  inset_element(hookstem_donut_gg, left = 0.658, right = .748, top = 0.405, bottom = .191) +\n  # barchart \n  inset_element(barchart, left = -0.075, right = 1, top = 0.35, bottom = 0) +\n  # donut chart\n  inset_element(donut, left = 0.45, right = 1, top = 0.3, bottom = 0) +\n  # donut bar hooks\n  inset_element(hookstem_bar_gg, left = 0.15, right = .25, top = 0.447, bottom = .295) +\n  inset_element(hook_bar_gg, left = 0.14, right = .24, top = 0.43, bottom = .27) +\n  # bar donut hooks\n  inset_element(hook_donut_gg, left = 0.66, right = .76, top = 0.401, bottom = .151) +\n  # quotes\n  inset_element(quote_gg, left = -.04, right = 1.04, top = .7, bottom = .35) +\n  # annotations \n  plot_annotation(\n    theme = theme(\n      plot.background = element_rect(fill = pal[\"bg\"], colour = pal[\"bg\"])))\n\n\n\n\n\n\n\nInfographic showing the the relative size of China’s fishing fleet compared to the nex 9 largest countries’ fishing fleets, total number of fishing vessles by class, and percent of illegal and legal vessels. 27.4% of the vessels were reported to be illegal with trawlers, longlines, and gillnets making up the largest portion of vessels. Data represents legal (red) and illegal (yellow) fishing vessels using AIS between 2012-2020."
  },
  {
    "objectID": "blogs/china_fishing/index.html#design-elements",
    "href": "blogs/china_fishing/index.html#design-elements",
    "title": "The World’s Largest Fishing Fleet",
    "section": "Design Elements:",
    "text": "Design Elements:\nAnd that is how you make an infographic in ggplot2! There are many ways in which you can take this, but this is one effective and simple way that worked for me. It is important that you consider design elements as you start planning your infographic. Some of the primary design elements that I considered when creating this infographic were:\n1) Graphic Forms: I ended up using a tree map, donut chart, and barchart to answer the three questions listed. While these plots are not necessarily considered fancy, they really allowed me to focus on visualizing the data I wanted to portray effectively, simply, and artistically. I wanted to focus on big picture trends/information which these plots allowed me to do. Sometimes keeping it more simple and focusing on aesthetics allowes for a better portrayal of your take home messages.\n2) Text: I kept my title and subtitle at one line each to ensure the text was not overwhelming for the reader and remained concise. I kept the central annotation in the middle of the infographic all in one place instead of cutting it up into smaller pieces around the graphic so the reader would not have to move their eyes as much to get the bulk of the information. Cutting down on axes text using theme_void() for each plot and removing bars/labels when possible really simplified the infographic and llowed for easier interpretion of the figures.\n3) Themes: I set a blue/grey background to show a deep sea feel and to also allow the gold and yellow to pop out a bit more. I wanted to create a maritime feel to the infographic, so I added some fishhooks to hold onto the plots and also some fish swimming around at the bottom. I also utilized a light blue text box in the text annotation to highlight information within the ocean theme.\n4) Colors:I used red and gold throughout the inforgaphic as they are the colors of the Chinese flag. Red represents “legal” and yellow is “illegal” (as annotated in the subtitle acting as a key). While these two colors are not my favorite, it added a layer of uniqueness to the theme that made the colors a bit more meaningful.\n5) Typography: I used two different fonts here: Playfair Display for the title and Sen for the text. These two fonts run well together, are clear, and fit the theme of the infographic I was creating. Playfair Display was used for the title and Sen was used for mos other text text. It is simple but does not look boring.\n6) General Design: The general design here was to start with the big picture tree map comparing the relative size of China’s fleet to the next 9 countries’ fleets. It then went into the text annotation which explains the purpose/background of the infographic. After this is set, the reader then goes into the nitty gritty where they explore the percentage and abundances of vessels by class and legality. It is suppose to flow from big picture to specifics with text in the middle to guide the reader. I used modest spacing between figures, but the text is a bit tight as a lot needed to be explained regarding the AIS system which couldn’t be graphically shown.\n7) Data Contextualization: I really focused on the data contextualization through the central annotation. because I removed axes and titles, it was key to have the primary contextualization in words to guide the reader. There are some things like the AIS system that could not be explained in the data, so I put it into context verbally. I also explained some uncertainty around what illegal vs legal vessels are, and lastly that these total vessels are not ALL of the vessels in the fleet but rather just the ones using AIS. Explaining this in the text annotation allowed me to bridge these gaps not seen in the figures.\n8) Centering Primary message: I did so by placing the tee map first which really gets at the big question of how big is China’s illegal and legal fishing fleet? I tried to make this the big and central aspect of the infographic as I feel like the tree map carries a lot of weight even though it has a simple design. The sheer size of China’s fleet comapred to the top next 9 countries realy stands out and serves as a powerful statement right off the bat.\n9) Accessibility: In general, red, gold, and grey/blue are not problematic color schemes for color blindness to what I found. Furthermore, the background contrast is strong against the red and gold, so the color scheme is onto a big concern for this. Because I relied heavily on red in this, I did not use any green to ensure that red/green colorblindness (the most common form) was not an issue here.\n10) Diversity Equity Inclusion (DEI): While my infographic does not directly address DEI in the data itself, I tried to be mindful in the way that I portrayed China in this to avoid creating negative emotion towards the country. It was challenging to show this data accurately without painting China as the an environmental terrorist. Phrasing the questions and context more factually and less opinionated helps to avoid negative feelings."
  },
  {
    "objectID": "blogs/landuse_cover_ML/index.html",
    "href": "blogs/landuse_cover_ML/index.html",
    "title": "Applying Supervised Machine Learning to Landuse Cover in Santa Barbara, CA",
    "section": "",
    "text": "Humans have been altering the natural world for centuries through agriculture, farming, development, recreation, etc. The impacts of landuse change have become critical to understand as evidence shows that it contributes significantly to climate change and is responsible for ecological degredation globally.[^1] Monitoring the distribution and change in landuse types can help us understand the impacts of climate change, natural disasters, deforestation, urbanization, and much more.\nDale, V. H. (1997). The relationship between land‐use change and climate change. Ecological applications, 7(3), 753-769.\nDetermining land cover types over large areas is a major application of remote sensing because we are able to distinguish different materials based on their spectral reflectance. In other words, remote sensing has opened up new doors to study land use change by looking at different proportions of light that it reflects up to satellites. By utiliizng remotely sensed imagery, we can classify landcover into classes or groups that allow us to understand the distribution and change in landcover types over large areas. There are many approaches for performing landcover classification – supervised approaches use training data labeled by the user, whereas unsupervised approaches use algorithms to create groups which are identified by the user afterward."
  },
  {
    "objectID": "blogs/landuse_cover_ML/index.html#summary",
    "href": "blogs/landuse_cover_ML/index.html#summary",
    "title": "Applying Supervised Machine Learning to Landuse Cover in Santa Barbara, CA",
    "section": "Summary",
    "text": "Summary\nMy approach to this analysis can be summed up in 5 steps:\n\nStep 1: load and process Landsat scene data\nStep 2: crop and mask Landsat data to study area (Santa Barbara)\nStep 3: extract spectral data at training sites (subset of parcels within Santa Barbara)\nStep 4: train and apply decision tree classifier\nStep 5: plot results"
  },
  {
    "objectID": "blogs/landuse_cover_ML/index.html#data",
    "href": "blogs/landuse_cover_ML/index.html#data",
    "title": "Applying Supervised Machine Learning to Landuse Cover in Santa Barbara, CA",
    "section": "Data",
    "text": "Data\nLandsat 5 Thematic Mapper\nData was obtained from Landsat 5, including 1 scene from September 25, 2007. The specific spectral bands being used are: 1, 2, 3, 4, 5, 7 from the collection 2 surface reflectance product.\nStudy area and training data A polygon representing southern Santa Barbara county was used as the overall study site. I also used polygons of regions within Santa Barbara representing training sites, including character string with land cover type. The decision tree derived from the training data is then applied to the entire Santa Barbara county polygon to create predictions of landuse cover for the whole region."
  },
  {
    "objectID": "blogs/landuse_cover_ML/index.html#process-data",
    "href": "blogs/landuse_cover_ML/index.html#process-data",
    "title": "Applying Supervised Machine Learning to Landuse Cover in Santa Barbara, CA",
    "section": "Process data",
    "text": "Process data\n\nLoad packages and set working directory\nBecause this project required working with both vector and raster data, I used both sf and terra packages in my workflow. To train our classification decision tree and plot the results, I used the rpart and rpart.plot packages.\n\n\ncode\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, results = FALSE, fig.align = \"center\")\n\nlibrary(sf)\nlibrary(terra)\nlibrary(here)\nlibrary(dplyr)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(tmap)\n\n\n\n\nLoad Landsat data\nFirst, I created a raster stack based on the 6 bands I worked with. Each file name ends with the band number (e.g. B1.tif). Band 6 corresponds to thermal data, which we will not be working with for this analysis, so it was not included in the data. To create a raster stack, I made a list of the files that I wanted to work with and read them all in at once using the rast function. The names of the layers were then updated to match the spectral bands and plot a true color image to see what we’re working with.\n\n\ncode\n# list files for each band, including the full file path\nfilelist &lt;- list.files(file.path(data, \"landsat-data/\"), full.names = TRUE)\n\n# read in and store as a raster stack\n\nlandsat &lt;- rast(filelist)\nlandsat\n\n# update layer names to match band\n\nnames(landsat) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nlandsat\n\n# plot true color image\n\nplotRGB(landsat, r = 3, g = 2, blue = 1, stretch = \"lin\")\n\n\n\n\n\n\n\n\n\n\n\nLoad study area\nI wanted to constrain the analysis to the southern portion of the county where there was training data, so I read in a file that defines the region of interest (ROI). I made sure that both CRS from the shape file and the Landsat data matched.\n\n\ncode\n# read in shapefile for southern portion of SB county\nSB_county_south &lt;- st_read(file.path(data, \"SB_county_south.shp\"))\n\n# project to match the Landsat data\nSB_county_south &lt;- st_transform(SB_county_south, crs = st_crs(landsat))\n\n# plot(SB_county_south)\n\n\n\n\nCrop and mask Landsat data to study area\nNext, I cropped and masked the Landsat data to the study area. This reduced the amount of data that needed to be worked with and therefore saves computational time and energy. Furthermore, I removed objects that I no longer was going to be working with using the rm() function to save space (optional).\n\n\ncode\n# crop Landsat scene to the extent of the SB county shapefile\nlandsat_cropped &lt;- crop(landsat, SB_county_south)\n\n# mask the raster to southern portion of SB county\nlandsat_masked &lt;- mask(landsat_cropped, SB_county_south)\n\n# remove unnecessary object from environment\nrm(landsat, landsat_cropped, SB_county_south)\n\nplotRGB(landsat_masked, r = 3, g = 2, blue = 1, stretch = \"lin\")\n\n\n\n\n\n\n\n\n\n\n\nConvert Landsat values to reflectance\nNext, I converted the values in the raster stack to correspond to reflectance values. To do so, I first removed erroneous values and applied any scaling factors to convert to reflectance. In this case, I worked with Landsat Collection 2. The valid range of pixel values for this collection is 7,273-43,636, with a multiplicative scale factor of 0.0000275 and an additive scale factor of -0.2. So I reclassified any erroneous values as NA and updated the values for each pixel based on the scaling factors. Now the pixel values should range from 0-100%.\n\n\ncode\nsummary(landsat_masked)\n# reclassify erroneous values as NA\nrcl &lt;- matrix(c(-Inf, 7273, NA,\n         43636, Inf, NA), \n        ncol = 3, byrow = TRUE)\n\n\nlandsat &lt;- classify(landsat_masked, rcl = rcl)\n\n# adjust values based on scaling factor\n\nlandsat &lt;- (landsat * 0.0000275 - 0.2)*100\nsummary(landsat)\n\n# plot true color image to check results\nplotRGB(landsat, r = 3, g = 2, b = 1, stretch = \"lin\")\n\n\n\n\n\n\n\n\n\ncode\n# check values are 0 - 100\nsummary(landsat)"
  },
  {
    "objectID": "blogs/landuse_cover_ML/index.html#classify-image",
    "href": "blogs/landuse_cover_ML/index.html#classify-image",
    "title": "Applying Supervised Machine Learning to Landuse Cover in Santa Barbara, CA",
    "section": "Classify image",
    "text": "Classify image\n\nExtract reflectance values for training data\nHere, I loaded in the shapefile identifying different locations within the study area as containing one of the 4 land cover types. I then extracted the spectral values at each site to create a data frame that relates land cover types to their spectral reflectance.\n\n\ncode\n# read in and transform training data\ntraining_data &lt;- st_read(file.path(data, \"trainingdata.shp\")) %&gt;% \n  st_transform(., crs = st_crs(landsat))\n#plot\nplot(training_data)\n\n\n\n\n\n\n\n\n\ncode\n# extract reflectance values at training sites\ntraining_data_values &lt;- extract(landsat, training_data, df = TRUE)\n\n# convert training data to data frame\ntraining_data_attributes &lt;- training_data %&gt;% \n  st_drop_geometry()\n\n# join training data attributes and extracted reflectance values\n\nSB_training_data &lt;- left_join(training_data_values, training_data_attributes,\n          by = c(\"ID\" = \"id\")) %&gt;% \n  mutate(type = as.factor(type))\n\n\n\n\nTrain decision tree classifier\nTo train the decision tree, I first needed to establish a model formula (i.e. what the response and predictor variables are). The rpart function implements the CART algorithm. The rpart function needs to know the model formula and training data you would like to use. Because I was performing a classification, I set method = \"class\". I also set na.action = na.omit to remove any pixels with NAs from the analysis.\n\nTo understand how the decision tree will classify pixels, I first plotted the results. The decision tree is comprised of a hierarchy of binary decisions. Each decision rule has 2 outcomes based on a conditional statement pertaining to values in each spectral band.\n\n\ncode\n# establish model formula\n\nSB_formula &lt;- type ~ red + green + blue + NIR + SWIR1 + SWIR2\n\n# train decision tree\nSB_decision_tree &lt;- rpart(formula = SB_formula,\n      data = SB_training_data, \n      method = \"class\", \n      na.action = na.omit)\n\n# plot decision tree\nprp(SB_decision_tree)\n\n\n\n\n\n\n\n\n\n\n\nApply decision tree\nAfter making the decision tree, I applied it to the entire image. The terra package includes a predict() function that allows you to apply a model to the data. In order for this to work properly, the names of the layers needed to match the column names of the predictors I used to train our decision tree. The predict() function then returns a raster layer with integer values. These integer values correspond to the factor levels in the training data. To figure out what category each integer corresponded to, I then inspected the levels of the training data.\n\n\ncode\n# classify image based on decision tree\nSB_classification &lt;- predict(landsat, SB_decision_tree, type = \"class\", na.rm = TRUE)\n\n# inspect level to understand the order of classes in prediction\nSB_classification\n\n\n\n\nPlot results\nFinally, I plotted the results to check out the land cover map.\n\n\ncode\n# plot results\ntm_shape(SB_classification) +\n  tm_raster()"
  },
  {
    "objectID": "blogs/landuse_cover_ML/index.html#conclusion",
    "href": "blogs/landuse_cover_ML/index.html#conclusion",
    "title": "Applying Supervised Machine Learning to Landuse Cover in Santa Barbara, CA",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis highlights the power of using supervised classification to gain insight of land use over on a large scale. While this technique has some challenges, it is a powerful tool that can be used in the environmental field to understand not just land use cover but a wide variety of environmental topics. It is important that these practices are also ground truthed to ensure the predicted model results are accurate."
  },
  {
    "objectID": "blogs/sierra_lakes/index.html",
    "href": "blogs/sierra_lakes/index.html",
    "title": "Drivers of Zooplankton Community Dynamics in Sierra Nevada Lakes",
    "section": "",
    "text": "Github project repository"
  },
  {
    "objectID": "blogs/sierra_lakes/index.html#introduction",
    "href": "blogs/sierra_lakes/index.html#introduction",
    "title": "Drivers of Zooplankton Community Dynamics in Sierra Nevada Lakes",
    "section": "Introduction",
    "text": "Introduction\nThe Sierra Nevada Mountains of California contain some of the nations most pristine wilderness, including thousands of lakes scattered throughout the range. Historically, the majority of these lakes were not inhabited with fish due to their fragmented and isolated nature, making it very difficult for fish to access lakes at high elevation. The Department of Fish and Game began to stock these lakes in the 20th century to promote recreational fishing in the range (R. Knapp 1996). While this provided many benefits to society, fish stocking posed many ecological threats to native species (R. Knapp 1996). These introduced trout (Rainbow, Brooke, Golden, Brown, Cutthroat, and hybrids) rely on zooplankton, a broad classifications of small organisms suspended in the water column that feed on phytoplankton and microbes, as a main food source in their fry stage of life (MacLennan, Dings-Avery, and Vinebrooke 2015).\nThis study aims to understand the abiotic (non-living) and biotic (living) factors that influence zooplankton community dynamics in the High Sierra. The questions being asked are:\n1) How does the size and elevation of lakes affect the diversity and richness of zooplankton communities?\n2) Does the presence of fish influence the diversity and richness of zooplankton in these lakes?\nI used multiple linear regression models to understand if and how diversity and richness were being affected by these variables. Because previous literature suggests that lakes support smaller foodwebs with decreasing size (Post, Pace, and Hairston 2000) and become more oligotrophic (less nutrient input) with increasing elevation (Sadro, Nelson, and Melack 2012), I hypothesized that the diversity and richness of zooplankton communities would lessen as lake size decreases and elevation increases. Furthermore, the presence of fish will present selective predation on certain zooplankton species, altering species diversity and richness as well.\n\n\ncode\n# map of the sierra nevada mountains\nmap &lt;- ggmap(basemap) +\n    geom_sf(data = snb, color = \"black\",  fill = \"lightblue4\", alpha = .4, inherit.aes = FALSE) +\n   labs(title = \"The Sierra Nevada Mountains\", x = \"\", y = \"\") +\n  theme_classic() +\n     annotation_scale(location = \"bl\",\n                      pad_y = unit(0.4, \"in\")) +\n  annotation_north_arrow(location = \"bl\",\n                         pad_x = unit(0.3, \"in\"),\n                         pad_y = unit(0.8, \"in\"),\n                         height = unit(.4, \"in\"),\n                           width = unit(.3, \"in\"),\n                         size = 5)\nmap\n\n\n\n\n\nFigure 1. Map of the Sierra Nevada Mountain Range in California. Data: https://gis.data.cnra.ca.gov/datasets/727b3cc24f8549759fe946a298dc3a20/explore?location=38.508918%2C-118.370394%2C6.43"
  },
  {
    "objectID": "blogs/sierra_lakes/index.html#methods",
    "href": "blogs/sierra_lakes/index.html#methods",
    "title": "Drivers of Zooplankton Community Dynamics in Sierra Nevada Lakes",
    "section": "Methods",
    "text": "Methods\nOver 1,100 lakes were surveyed from 1995-2002 as a part of the Sierra Lakes Inventory Project (SLIP), with the goal of describing the biodiversity of lentic water bodies in lakes, ponds, marshes, and meadows in the Southern Sierra (R. A. Knapp et al., n.d.). All study observation and samples were collected between months of July and September. Area and elevation of the lakes were calculated using GPS and GIS for lakes where area was unknown. Fish presence was measured using both visual and net surveys at the edge of lakes to identify species presence. Zooplankton samples were collected in water samples that were then packed out and identified in the lab at the Sierra Nevada Aquatic Research Laboratory (SNARL) in Mammoth Lakes, California. Due to the varying stages of maturity and small sizes of zooplankton, taxon were sometimes identified to genus, family, or life stage if not able to be identified to species level. Zooplankton samples were originally collected in 3 different subsamples, so diversity was averaged for all 3 lake subsamples to calculate each lake’s Shannon-Wiener diversity index. Species richness was calculated by identifying unique taxon across all 3 samples. Zooplankton data was not collected in 1998, 1999, or 2002."
  },
  {
    "objectID": "blogs/sierra_lakes/index.html#results",
    "href": "blogs/sierra_lakes/index.html#results",
    "title": "Drivers of Zooplankton Community Dynamics in Sierra Nevada Lakes",
    "section": "Results",
    "text": "Results\n\nExploratory Data Analysis\nBefore doing any modeling, I wanted to visualize and explore the data a bit to understand how it was distributed. A histogram of the log lake area returned a relatively normally distributed curve with a short right tail, suggesting that medium to larger lakes were more frequently observed than small ones (fig.2.A). Distributions appeared to be similar regardless of fish presence. A histogram of lake elevation showed less of a normal distribution with a long left tail, indicating that the majority of lakes observed were at higher elevations (fig.2.B). Regardless of fish presence, the shape of the curve still resembles somewhat of a normal unimodal shape despite the long tail.\nNote:Log area is used in this study as the area had a skewed distribution, and the log transformation suited the data better for linear modeling.\n\n\ncode\n# making histograms of area and elevation\n\nh1 &lt;- ggplot(data = total , aes(x = log(lake_area_nbr),  fill = actual_fish_presence)) +\n  geom_histogram(color = \"black\", size = .1) +\n  scale_fill_manual(name = \"Fish Present \", values = c(\"lightblue4\", \"cyan3\")) +\n  theme_classic() +\n  labs(x = expression(\"log Lake Area m\"^2))\n\nh2 &lt;- ggplot(data = total , aes(x = lake_elevation_nbr, fill = actual_fish_presence)) +\n  geom_histogram(color = \"black\", size = .1) +\n  scale_fill_manual(name = \"Fish Present \", values = c(\"lightblue4\", \"cyan3\")) +\n  theme_classic() +\n  labs(x = \"Lake Elevation (m)\", y = \"\")\n\nhist &lt;- h1 + h2 +\n  plot_layout(guides = \"collect\",\n              heights = 5) +\n  plot_annotation(tag_levels = \"A\") &\n  theme(legend.position = \"bottom\")\n\nhist \n\n\n\n\n\nFigure 2. A) Histogram showing the frequency of lakes observed by log area. B) Histogram showing the frequency of lakes observed by elevation.\n\n\n\n\ncode\n#correlation of independent variables\nind_cor &lt;- cor(log(total$lake_area_nbr), total$lake_elevation_nbr)\n\n\nOne concern in working with this data was collinearity between independent variables, which can lead to dramatic changes in the dependent variables (diversity and richness) if it occurred. I examined a simple scatterplot to visually inspect for a possible relationship between log lake area and elevation, both with and without fish present (fig.3). While fish seemed to be more frequent in larger lakes and observations were clustered slightly clustered at high elevation, the data points appeared to be scattered and noisy with little evidence of a significant relationship. The correlation between log lake area and elevation was minimal so I kept both independent variables in consideration through the study (r = 0.03.\n\n\ncode\n#making scatterplots with independent variables\n\nscatter &lt;- ggplot(total, aes(y = log(lake_area_nbr), x = lake_elevation_nbr, color = actual_fish_presence)) +\n  geom_point(alpha = .7) + \n  scale_color_manual(name = \"Fish Present \", values = c(\"lightblue4\", \"cyan3\")) +\n  labs(x = \"Lake Elevation (m)\", y = expression(\"log Lake Area m\"^2) ) +\n  theme_classic()\n\nscatter\n\n\n\n\n\nFigure 3. Scatterplot showing relationship of log lake area and lake elevation. Correlation between vairables was low, indicationg no collinearity between dependent variables.\n\n\n\n\n\n\ncode\n#correlation of independent and dependent variables\ncors = total %&gt;% \n  group_by(actual_fish_presence) %&gt;% \n  summarise(\n        cor_da = round(cor(mean_div, log(lake_area_nbr)), 2),\n         cor_de = round(cor(mean_div, lake_elevation_nbr), 2),\n         cor_ra = round(cor(richness, log(lake_area_nbr)), 2),\n         cor_re = round(cor(richness, lake_elevation_nbr), 2))\n\n\nExploratory data analysis suggested that zooplankton species diversity and richness increases with the log area of a lake and decrease with elevation (fig. 4). The correlation was positive and weaker for species diversity and log lake area when fish were absent compared to present (r = 0.11, r = 0.33) (fig.4.A). Similar results occured for species richness and log lake area (r = 0.05, r = 0.36) (fig.4.B). Diversity and richness in lakes were strongly and negatively correlated with with elevation, and the presence of fish did not change the correlation strength noticeably (r = -0.44, r = -0.45) (fig.4.C). Stronger negative relationships were found between species richness and elevation both with and without fish (r = -0.55, r = -0.53) (fig.4.D).\n\n\ncode\n# diversity log lake area\nda &lt;- ggplot(data = total, aes(x = log(lake_area_nbr), y = mean_div, color = actual_fish_presence)) +\n  labs(x = expression(\"log Lake Area m\"^2), y = \"Mean Diversity\") + \n  geom_jitter(alpha = 0.7) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = T, lwd = 0.8) +\n  theme_classic() + \n  scale_color_manual(name = \"Fish Present \", values = c(\"lightblue4\", \"cyan3\")) +\n  stat_cor(aes(label = ..r.label..),\n           r.accuracy = 0.01,\n           label.x.npc = .01,\n            label.y.npc = 1,\n           show.legend = FALSE,\n           size = 2.5)\n\n# diversity log lake elevation\nde &lt;- ggplot(data = total, aes(x = lake_elevation_nbr, y = mean_div, color = actual_fish_presence)) +\n  labs(x = \"Lake Elevation (m)\", y = \"Mean Diversity\") + \n  geom_jitter(alpha = 0.7) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = T, lwd = 0.8) +\n  theme_classic() + \n  scale_color_manual(name = \"Fish Present \", values = c(\"lightblue4\", \"cyan3\")) +\n  stat_cor(aes(label = ..r.label..),\n           r.accuracy = 0.01,\n           label.x.npc = .075,\n            label.y.npc = .25,\n           show.legend = FALSE,\n           size = 2.5)\n\n# richness log lake area\nra&lt;- ggplot(data = total, aes(x = log(lake_area_nbr), y = richness, color = actual_fish_presence)) +\n  labs(x = expression(\"log Lake Area m\"^2), y = \"Richness\") + \n  geom_jitter(alpha = 0.7) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = T, lwd = 0.8) +\n  theme_classic() + \n  scale_color_manual(name = \"Fish Present \", values = c(\"lightblue4\", \"cyan3\")) +\n  stat_cor(aes(label = ..r.label..),\n           r.accuracy = 0.01,\n           label.x.npc = .01,\n            label.y.npc = 1,\n           show.legend = FALSE,\n           size = 2.5)\n\n# diversity log lake elevation\nre &lt;- ggplot(data = total, aes(x = lake_elevation_nbr, y =richness, color = actual_fish_presence)) +\n  labs(x = \"Lake Elevation (m)\", y = \"Richness\") + \n  geom_jitter(alpha = 0.7) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = T, lwd = 0.8) +\n  theme_classic() + \n  scale_color_manual(name = \"Fish Present \", values = c(\"lightblue4\", \"cyan3\")) +\n  stat_cor(aes(label = ..r.label..),\n           r.accuracy = 0.01,\n           label.x.npc = .01,\n            label.y.npc = 1,\n           show.legend = FALSE,\n           size = 2.5)\n\n#combined correlations figure\ncorrelations &lt;- (da + ra)/(de + re) + \n  plot_layout(guides = \"collect\",\n              heights = 5) + \n  plot_annotation(tag_levels = \"A\") &\n  theme(legend.position = \"bottom\")\n# & ylab(NULL) & theme(plot.margin = margin(6.5, 5.5, 0, 5.5)) \n\ncorrelations\n\n\n\n\n\nFigure 4. A) Mean zooplankton species diversity as a function of lake log area; B) Zooplankton species richness as a function of the log area of lakes; C) Mean zooplankton species diversity as a function of lake elevation; D) Zooplankton species richness as a function of lake elevation; Diversity was calculated using the Shannon-Wiener Index.\n\n\n\n\n\n\nMultiple Linear Regression\nThese preliminary results were used to inform further analysis through the use of multiple linear regression models. I tested these initial observed patterns by modeling zooplankton species diversity and richness as a function of the following explanatory variables: log lake area, lake elevation, fish presence, interaction of log lake area x fish presence (eda hinted at a potential effect as seen in the difference of slopes in fig.2.A and fig.2.B), and the addition of a temporal component (year) to see if these community dynamic patterns changed aver time. The goal of these models were to test the strength of the predictor variables in explaining the variation in zooplankton diversity and richness and to also test if these variable were statistically significance (\\(\\alpha\\) = .05).\n\\[diversity_i=\\beta_{0}+\\beta_{1} \\cdot log(area)_i + \\beta_{2} \\cdot elevation_i + \\beta_{3} \\cdot fish_i + \\beta_{4} \\cdot log(area)_i \\cdot fish_i + \\beta_{5} \\cdot year_i + \\varepsilon_i\\]\n\\[richness_i=\\beta_{0}+\\beta_{1} \\cdot log(area)_i + \\beta_{2} \\cdot elevation_i + \\beta_{3} \\cdot fish_i + \\beta_{4} \\cdot log(area)_i \\cdot fish_i + \\beta_{5} \\cdot year_i + \\varepsilon_i\\]\nThe species diversity model results conclude that the log area (positively) and lake elevation (negatively) significantly affected zooplankton species diversity (p &lt; .05, p &lt; .001). While lakes with fish present reduced zooplankton diversity, no significance was found (P &gt; .05). Similarly, the interaction between log lake area and fish presence was insignificant but positively affected diversity (more so than lakes without fish) (p = .14). No individual year had a significant effect on species diversity.\nThe species richness model also showed that the log lake area (positively) and lake elevation (negatively) significantly affected zooplankton richness (p &lt; .05, p &lt; .001). Unlike the diversity model, species richness was significantly and negatively affected by the presence of fish in lakes (p &lt; 0.05). The interaction between log lake area and fish presence also was a significant factor in affecting species richness, indicating that the size of the lake affected zooplankton richness differently depending on whether fish were present or not (p &lt; .05). No individual year had a significant effect on species richness.\n\n\ncode\n#diversity model \nm_div &lt;- lm(mean_div ~ log(lake_area_nbr) + lake_elevation_nbr + actual_fish_presence + log(lake_area_nbr):actual_fish_presence + year, data = total)\n# m_div_sum &lt;- summary(m_div)\n\n\n#richness model\nm_rich &lt;- lm(richness ~ log(lake_area_nbr) + lake_elevation_nbr + actual_fish_presence + log(lake_area_nbr):actual_fish_presence +  year, data = total)\n# summary(m_rich)\n\n#creatign a table \ntab_model(m_div, m_rich,\n          pred.labels = c(\"Intercept\", \"log Area\", \"Elevation\",\n                          \"Fish Presence(Yes)\", \"1997\",\n                          \"2000\", \"2001\",\n                          \"log Area x Fish Presence(Yes)\"),\n          dv.labels = c(\"Divsersity\", \"Richness\"),\n          string.ci = \"Conf. Int (95%)\",\n          string.p = \"p-value\",\n          title = \"Table 1. Multiple Linear Regression Model Results.\",\n          digits = 4)\n\n\n\nTable 1. Multiple Linear Regression Model Results.\n\n\n \nDivsersity\nRichness\n\n\nPredictors\nEstimates\nConf. Int (95%)\np-value\nEstimates\nConf. Int (95%)\np-value\n\n\nIntercept\n1.8560\n1.4600 – 2.2521\n&lt;0.001\n12.6131\n10.7139 – 14.5123\n&lt;0.001\n\n\nlog Area\n0.0285\n0.0046 – 0.0523\n0.019\n0.1383\n0.0238 – 0.2527\n0.018\n\n\nElevation\n-0.0004\n-0.0005 – -0.0003\n&lt;0.001\n-0.0026\n-0.0031 – -0.0021\n&lt;0.001\n\n\nFish Presence(Yes)\n-0.3205\n-0.7335 – 0.0924\n0.128\n-2.2044\n-4.1847 – -0.2242\n0.029\n\n\n1997\n0.0830\n-0.0317 – 0.1976\n0.156\n-0.0721\n-0.6218 – 0.4777\n0.797\n\n\n2000\n-0.0304\n-0.1515 – 0.0906\n0.621\n-0.3699\n-0.9504 – 0.2106\n0.211\n\n\n2001\n0.0192\n-0.1260 – 0.1644\n0.795\n-0.1259\n-0.8223 – 0.5706\n0.723\n\n\nlog Area x Fish Presence(Yes)\n0.0305\n-0.0100 – 0.0710\n0.140\n0.2185\n0.0242 – 0.4129\n0.028\n\n\nObservations\n545\n545\n\n\nR2 / R2 adjusted\n0.240 / 0.230\n0.349 / 0.340\n\n\n\n\n\n\n\n\n\nResiduals\nQQ plots were made to test if the residuals were normally distributed (fig.5). As seen in the plots, the residuals for both models fit the normal distribution quite well, with the exception of a slight and minor deviation in the tails. This implies that the diversity model residuals have a slight negative skew (fig.5.A), and the richness model has slightly fatter than expected tails (fig.5.B). In conclusion, the QQ plots imply assumptions of normality are met.\n\n\ncode\n#diversity model residuals\nm_div_res &lt;- residuals(m_div)\n#richness model residuals\nm_rich_res &lt;- residuals(m_rich)\n\n#qq plot for diversity \ndiv_qq &lt;-  ggplot( m_div, aes(sample = .resid)) +\n  geom_qq(color = \"lightblue4\") +\n  geom_qq_line() +\n  theme_classic() +\n  labs( x = \"Theoretical Quantiles\", y = \"Sample Quantiles\")\n\n#qq plot for richness\nrich_qq &lt;-  ggplot( m_rich, aes(sample = .resid)) +\n  geom_qq(color = \"lightblue4\") +\n  geom_qq_line() +\n  theme_classic() +\n  labs( x = \"Theoretical Quantiles\", y = \"\")\n\n#joining plots\nqq &lt;- div_qq + rich_qq +\nplot_annotation(tag_levels = \"A\") \n\nqq\n\n\n\n\n\nFigure 5. A) QQ plot of zooplankton species diversity model residuals. Theororetical and sample residuals are generally suggestive of a normal distribution, but show slight deviation in the tails which suggest the distribution has a slight negative skew. B) QQ plot of zooplankton species richness model residuals. Theororetical and sample residuals are generally suggestive of a normal distribution, but show slight deviation in the tails which suggest the distribution has fat tails.\n\n\n\n\ncode\nggsave(\"figures/residuals.png\", qq, height = 5, width = 8, unit = 'in')"
  },
  {
    "objectID": "blogs/sierra_lakes/index.html#discussion",
    "href": "blogs/sierra_lakes/index.html#discussion",
    "title": "Drivers of Zooplankton Community Dynamics in Sierra Nevada Lakes",
    "section": "Discussion",
    "text": "Discussion\nZooplankton species diversity and richness are closely related, and they both responded similarly to abiotic conditions (log lake area and lake elevation). (Urmy and Warren 2019) also showed that zooplankton species responded noticeably across elevation even at a fine scale. However, species richness was noticeably lower in lakes with fish present compared to without when holding lake area and elevation constant, indicating that biotic factors (fish present) alter the ability of specific zooplankton species to persist (likely due to taxa specific predation).\nThe most interesting finding from these results is that the area of a lake affects the zooplankton species richness noticeably more when fish are present as seen in the interaction coefficient of the richness model. Richness is very low in small lakes and very high in large lakes when fish are present (steep positive slope), whereas richness is moderate to high in lakes without fish (moderate positive slope). Other studies have found similar results where the effects of fish predation on zooplankton are amplified by varying environmental conditions (MacLennan, Dings-Avery, and Vinebrooke 2015). This begs the question: Why are zooplankton more sensitive to changes in lake area when fish are present vs absent? This question can be answered by the ecological principle known as the Island Biogeography Theory which states that prey on smaller islands (or habitats) experience more intense predation pressure from predators than on larger islands (MacArthur and Wilson 2001). Due to their isolation and limited resources, zoo plankton in small lakes experience higher extinction rates from concetrated predation, less resources (nutrient input), and lower emigration . This applies quite literally into our study system here were introduced fish in small lakes (islands) suppress zooplankton species richness (prey) more intensely than in larger lakes (islands). Similar patterns have also been observed across taxa in lakes across North America (Browne 1981).\nThis study did not include a spatial element as coordinates were not provided in the data as a measure to keep sensitive and rare fish lakes discreet. Future research should look at other taxa in oligotrophic alpine lake food webs (algae, microbes, benthic macroinvertebrates) to understand potential trophic cascades and community wide effects of fish introduction."
  },
  {
    "objectID": "blogs/water_class/index.html",
    "href": "blogs/water_class/index.html",
    "title": "Using Machine Learning to Predict Water Potability",
    "section": "",
    "text": "Water quality is a major threat to health concern that affects billions of people around the globe. There a number of factors in water that can determine whether it is safe to dink or not including both abiotic (non-living) and biotic (living) factors. By understanding the drivers of what makes water potable or safe to drink, we can predict whether whether or not the water is safe by just looking at a select number of attributes of the water. So our question beign asked here is:\n“Can we predict the potability of water based off of it’s chemical characteristics alone?”\nThe goal of this analysis is to identify the classification model that predicts water potability the best based off of chemical attributes from water samples. In this analysis, I train various supervised machine learning classification algorithms on a sample of over 3,000 water quality observations. This data is publicly available on Kaggle. The specific predictor variables included in the models are:\n\nph (the acidity of the water)\nhardness (the concentration of minerals)\nsolids (concentration of solid material)\nchloramines (concentration of chlorine and ammonia compounds)\nsulfate (oxidized sulfur concentration)\nconductivity (ability of the water to pass an electrical current)\norganic_carbon (concentration of organic forms of carbon)\ntrihalomethanes (byproducts of treating water with organic compounds )\nturbidity (cloudiness of water)\n\n\n\n\nNon-potable and potable water"
  },
  {
    "objectID": "blogs/water_class/index.html#cleaning-data",
    "href": "blogs/water_class/index.html#cleaning-data",
    "title": "Using Machine Learning to Predict Water Potability",
    "section": "Cleaning Data",
    "text": "Cleaning Data\n\n\ncode\n# reading in the data\nwater &lt;- read.csv(\"water_potability.csv\") %&gt;% \n  # cleaning up the column names\n  clean_names() %&gt;% \n  # making potability as a factor \n  mutate(potability = factor(potability))"
  },
  {
    "objectID": "blogs/water_class/index.html#missing-values",
    "href": "blogs/water_class/index.html#missing-values",
    "title": "Using Machine Learning to Predict Water Potability",
    "section": "Missing Values",
    "text": "Missing Values\nWe want to also check to see if our data has any missing values first. Missing values can be problematic especially in large quantities. We can do so using the naniar package to plot the missing data across columns using the viz_miss() function.\n\n\ncode\n# looking for missing data \nnaniar::vis_miss(water)\n\n\n\n\n\nFigure 1. Visualizing missing values throughout the data.\n\n\n\n\nWe can see that there are a fair amount of values missing from the ph, sulfate, and trihalomethanes columns. One way to tackle this is to get rid of that data all together, but we would be losing a lot of valuable data from other predictors in doing so. We can address this in our modeling by imputing the data, or filling it with estimated values from different methods. We will come back to this later."
  },
  {
    "objectID": "blogs/water_class/index.html#exploratory-data-analysis",
    "href": "blogs/water_class/index.html#exploratory-data-analysis",
    "title": "Using Machine Learning to Predict Water Potability",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nThe first step you want to take in making models is exploratory data analysis (EDA). This is the phase where you dive into your data, look at its nooks and crannies, and try to identify potential patterns that could give insight into the steps you will want to take when modeling. Looking at the abundance. distribution, and correlation of the data are all importance steps to take.\n\nBar Chart\nThe first thing we want to do is check to compare the total counts of our potability outcomes (0 = not potable and 1 = potable). We can do so by simply plotting a barchart:\n\n\ncode\nggplot(data = water, aes(x = potability, fill = potability)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"0\" = \"#1C3738\", \"1\" = \"#8BAAAD\"),\n                    guide = \"none\") +\n  theme_minimal() +\n  labs(y = \"\", x = \"Potability\", \n       title = \"Counts of Water Potability Outcomes in Data\") +\n  scale_x_discrete(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0),\n                     # modifying y axis scale \n                     labels = scales::label_number(\n                       scale_cut = scales::cut_short_scale())) +\n  theme(\n    panel.grid.minor.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.major.x = element_blank(),\n    plot.title.position = \"plot\",\n    plot.title = element_text(margin = margin(t = 0, r = 0, \n                                              b = 2, l = 0, unit = \"lines\")),\n  )\n\n\n\n\n\nFigure 2. A Bar chart showing the total number of observations by potability outcomes (0 = not potable, 1 = potable) in the data set.\n\n\n\n\nWe notice that there are significantly more observations with a value of 0 compared to 1. This tells us that we have a class imbalance in the data and may need to do some upsampling (which we will discuss later). Overall, this is not a major challenge, but it tells us that the models may do a better job at predicting non-potable water compared to potable water as there is less data for the latter."
  },
  {
    "objectID": "blogs/water_class/index.html#histograms",
    "href": "blogs/water_class/index.html#histograms",
    "title": "Using Machine Learning to Predict Water Potability",
    "section": "Histograms",
    "text": "Histograms\nOne great way to investigate our data in the exploratory data analysis is plotting the distribution of our predictors on histograms. Histograms show the distribution and frequency of observations in a data set. The allow us to check and see if it is normally distributed, roughly where the center fo the data is, how wide the spread is, and if there are outliers skewing the data.\n\n\ncode\n# making a histogram function\nhist_fun &lt;- function(x, x_lab) {\n  ggplot(data = water , aes(x = x,  fill = potability)) +\n  geom_histogram(color = \"black\", size = .1) +\n  scale_fill_manual(name = \"Potability \", values = pal) +\n   scale_x_continuous(expand = c(0, 0),\n                     # modifying y axis scale \n                     labels = scales::label_number(\n                       scale_cut = scales::cut_short_scale())) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_minimal() +\n  labs(x = x_lab, y = \"\")\n}\n\n# making individual histograms \nsolids_hist &lt;- hist_fun(x = water$solids, x_lab = \"Solids\" )\nph_hist &lt;- hist_fun(x = water$ph, x_lab = \"pH\" )\nhard_hist &lt;- hist_fun(x = water$hardness, x_lab = \"Hardness\" )\noc_hist &lt;- hist_fun(x = water$organic_carbon, x_lab = \"Organic Carbon\" )\n\n# combining\nhist &lt;- (solids_hist + ph_hist)/(hard_hist + oc_hist) +\n  plot_layout(guides = \"collect\",\n              heights = 5) &\n  theme(legend.position = \"bottom\")\n\n\nhist\n\n\n\n\n\nFigure 3. Histograms showing total solids, pH, hardness, and organic carbon by potability in water samples.\n\n\n\n\nThe histograms show that solids, ph, hardness, and organic_carbon all appear to be normally distributed. solids has a slight right skew, indicating that there may be some more high solid concentrations than lower in our sample as a result of some outliers. Overall, these distributions appear to be similarly distributed regardless of potability. This indicates that"
  },
  {
    "objectID": "blogs/water_class/index.html#boxplots",
    "href": "blogs/water_class/index.html#boxplots",
    "title": "Using Machine Learning to Predict Water Potability",
    "section": "Boxplots",
    "text": "Boxplots\nAnother way we can check the distribution of our data is by making boxplots. They provide a bit more insight into the spread of the data by providing the median, quartile ranges, and outliers of the data. It provides some more summary statistics that we can use to directly compare our data.\n\n\ncode\n# boxplot function\nbox_fun &lt;- function(y, y_lab) {\n  ggplot(data = water , aes(y = y, x = potability,  fill = potability)) +\n  geom_boxplot(color = \"black\", size = .1) +\n  scale_fill_manual(name = \"Potability \", values = pal) +\n   scale_y_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_minimal() +\n  labs(x = \"\", y = y_lab) +\n  theme(\n  panel.grid.minor.y = element_blank(),\n    panel.grid.major.x = element_blank(),\n  axis.text.x = element_blank())\n}\n\nchlor_box &lt;- box_fun(water$chloramines, y_lab = \"Chloramines\")\nsulf_box &lt;- box_fun(water$sulfate, y_lab = \"Sulfate\")\ncond_box &lt;- box_fun(water$conductivity, y_lab = \"Conductivity\")\ntri_box &lt;- box_fun(water$trihalomethanes, y_lab = \"Trihalomethanes\")\n\n\n\n# combining\nbox &lt;- (chlor_box + sulf_box)/(cond_box + tri_box) +\n  plot_layout(guides = \"collect\",\n              heights = 5) &\n  theme(legend.position = \"bottom\") \n\nbox\n\n\n\n\n\nFigure 4. Boxplots showing the distribution of cholarmine, sulfate, conductivity, and trihalomethane in water samples.\n\n\n\n\nAs you can see, the chloramines, sulfate, conductivity, and trihalomethanes variables all appear to have similar distributions regardless of the water is potable. However, this does not entirely mean that they will not be important predicotr variables in our models. We can see that quartile ranges of potable and non-potable data line up evenly for each variable with a bit wider of a spread for chloramines and sulfate that are classified as potable."
  },
  {
    "objectID": "blogs/water_class/index.html#correlation-plot",
    "href": "blogs/water_class/index.html#correlation-plot",
    "title": "Using Machine Learning to Predict Water Potability",
    "section": "Correlation Plot",
    "text": "Correlation Plot\nWe then need to check for correlation/collinearity between predictor variables as this could cause issues with our models. Collinearity arises when two or more predictor variables are highly correlated with one another. This can lead to difficulty in interpreting individual coefficients and inflated magnitudes as well as decrease predictability accuracy. Therefore, we want to make sure our data does not have any highly correlated predictors.\n\n\ncode\nwater %&gt;% \n  select_if(is.numeric) %&gt;% \n  cor(use = \"complete.obs\") %&gt;% \n  # corrplot(type = \"lower\", method = \"shade\", order = \"AOE\", diag = TRUE)\ncorrplot( addrect = 2, tl.col=\"black\") \n\n\n\n\n\nFigure.5. Correlation plot showing the correlation of all continuous water potability predictor variables. There is little to know correlation between variables.\n\n\n\n\nOur correlation plot shows that there is minimal to little correlation between predictor variables in this data set. Sulfates have a slight negative relationship with hardness and solids and ph has a slight positive relationship with hardness and negative relationship with solids. These are overall very small and not strong enough to consider collinearity an issue."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Raymond Hunter",
    "section": "",
    "text": "Out of the infinite amount of content on the web, you’ve stumbled upon a little snapshot of my life on a computer screen. Whoohoo! My name is Ray, and I created this personal website using Quarto to share a bit about who I am and what I do—including the skills, experiences, and hobbies that have shaped me into the nature-loving, data enthusiast I am today.\nAn Environmental Data Scientist by training, I currently work at NOAA Fisheries researching the economy and health of West Coast fisheries. I hold a Masters of Environmental Science and Management (MESM) degree from the Bren School of Environmental Science and Management at UC Santa Barbara, as well as a B.S.in Ecology and Evolutionary Biology, and a B.A. in Environmental Studies from UC Santa Cruz. I enjoy various outdoor activities when not squinting my eyes at error messages. I also take on part-time, contract data-related gigs—so feel free to reach out if I can be of service!\n\n“Not all those who wonder are lost” - Gandalf the Grey"
  },
  {
    "objectID": "index.html#hello-world",
    "href": "index.html#hello-world",
    "title": "Raymond Hunter",
    "section": "",
    "text": "Out of the infinite amount of content on the web, you’ve stumbled upon a little snapshot of my life on a computer screen. Whoohoo! My name is Ray, and I created this personal website using Quarto to share a bit about who I am and what I do—including the skills, experiences, and hobbies that have shaped me into the nature-loving, data enthusiast I am today.\nAn Environmental Data Scientist by training, I currently work at NOAA Fisheries researching the economy and health of West Coast fisheries. I hold a Masters of Environmental Science and Management (MESM) degree from the Bren School of Environmental Science and Management at UC Santa Barbara, as well as a B.S.in Ecology and Evolutionary Biology, and a B.A. in Environmental Studies from UC Santa Cruz. I enjoy various outdoor activities when not squinting my eyes at error messages. I also take on part-time, contract data-related gigs—so feel free to reach out if I can be of service!\n\n“Not all those who wonder are lost” - Gandalf the Grey"
  },
  {
    "objectID": "blogs/targets_wflow/index.html",
    "href": "blogs/targets_wflow/index.html",
    "title": "Data Pipelines with {Targets}",
    "section": "",
    "text": "If you’re reading this post, chances are you’re familiar with the process of collecting, cleaning, wrangling, and analyzing data. You’re probably also familiar with that dreaded moment—discovering an error (or receiving critical feedback from a supervisor) right at the finish line, forcing you to go aaalllllll the way back and re-run your time- and energy-intensive workflow.\nWorse yet, your pipeline has grown so cluttered and poorly documented from rushed, repeated fixes and edits that you’re no longer sure which parts of your code are still relevant. The result? A frustrating, tangled mess of rework.\nFortunately, there’s an R package designed to break this loop and make your work as an analyst, developer, or scientist far more efficient and enjoyable. It’s called {targets} (Landau 2021a)."
  },
  {
    "objectID": "blogs/targets_wflow/index.html#what-is-targets",
    "href": "blogs/targets_wflow/index.html#what-is-targets",
    "title": "Data Pipelines with {Targets}",
    "section": "What is {targets}?",
    "text": "What is {targets}?\n\n{targets} is an R package that helps organize, structure, and track the components of your analytical pipeline, making it more reproducible, scalable, and robust. It monitors the code and outputs of each step (or “target”) in your workflow to ensure they stay up to date when something changes—and, more importantly, to avoid rerunning steps that haven’t changed. This speeds up your analysis by only re-running code that has been modified (and its downstream dependencies). {targets} also flags steps that have changed but haven’t yet been executed, so you know exactly where to focus your attention in a pipeline. It encourages efficient, clean, and function-oriented code. While it may take a bit more effort to set up at first, the payoff in saved time and reduced frustration is well worth it.\nI sort of see it as The Eye of Sauron from JRR Tolkien’s The Lord of the Rings. Sauron (or the {targets} package in this case) is ceaselessly scouring Middle Earth (your analytical pipeline) with an everlasting, burning desire to find the ring (changes in your code) so that he can resume full power (update your code)."
  },
  {
    "objectID": "blogs/targets_wflow/index.html#a-simple-analysis",
    "href": "blogs/targets_wflow/index.html#a-simple-analysis",
    "title": "Data Pipelines with {Targets}",
    "section": "A simple analysis",
    "text": "A simple analysis\nLets run through a simple analysis using {palmerpenguins} data without targets so we can later apply it and see the magic.\nMaybe we are interested in seeing if penguin flipper length can be predicted by other physical characteristics of the animal. Here is a short workflow on how one might approach this analysis.\n\n\nSet Up\nWe start by installing and loading the necessary packages and reading in the data.\n\n# install packages if not already\n# install.packages(c(\"palmerpenguins\", \"janitor\", \"tidyverse\", \"targets\"))\n\n# load packages\nlibrary(\"palmerpenguins\") # penguin data \nlibrary(\"janitor\") # cleaning column names\nlibrary(\"tidyverse\") # collection of data processing packages\nlibrary(\"targets\")\n\n# read data\npenguins &lt;- palmerpenguins::penguins\n\n\n\n\nWrangling data\nIts critical to clean up and inspect the data prior to analysis. Fortunately for this simple example, palmerpenguins data is relatively clean already. Lets just touch up and filter some of those column names.\n\npenguins_mod &lt;- penguins |&gt; # raw data\n  janitor::clean_names() |&gt; # make columns lower case and remove spaces\n  mutate(name = stringr::word(species, 1)) |&gt;   # add a species name column\n  dplyr::select(name, island, flipper_length_mm, body_mass_g, bill_length_mm, sex)\n\n\n\n\nExploratory data analysis\nBefore conducting any modeling, you should thoroughly check the quality of the data and assumptions being made. There is no single way on how to perform exploratory data analysis as it is subjective to the data in hand and the question being asked.\nWe will inspect a possible linear relationship between body mass and flipper length by species.\n\n# visualizing relationship of body mass and flipper length of penguins\nggplot(data = penguins_mod,\n       aes(x = body_mass_g, y = flipper_length_mm, color = name)) + \n  geom_point() + # add data points\n  geom_smooth(method = \"lm\", se = FALSE) +  # Add trend lines for each species\n  labs(\n    y = \"Flipper Length (mm)\",\n    x = \"Body Mass (g)\",\n    title = \"Penguin Body Mass by Flipper Length\",\n    color = \"Species\"        \n  ) +\n  scale_color_manual( # changing colors \n    values = c(\n      \"Adelie\" = \"blue4\", \n      \"Chinstrap\" = \"tan\",     \n      \"Gentoo\" = \"darkgreen\"   \n    )) +\n  theme_minimal() # minimal appearance\n\n\n\n\nFigure 1. A graph showign linear relationship between Body Mass (g) and Flipper Length (mm) across penguin species.\n\n\n\n\n\n\n\nModeling\nThe plot suggested a potential linear relationship between flipper length and body mass, varying by species. There are many possible models and explanatory variables we could explore to better understand this relationship. For now, let’s start with a simple ordinary least squares (OLS) linear regression model to examine how body mass, species, and sex influence the flipper length of penguins.\n\n# fitting a linear regression model \nm1 &lt;- lm(flipper_length_mm ~ body_mass_g + name + sex, \n         data = penguins_mod)\n\n\n\n\nResults\nWe should create a summary table of model coefficients to help contextualize the magnitude of effects and their statistical significance. We’ll also inspect the R-squared value as an estimate of overall model fit.\n\n# model coefficients \nsummary(m1)$coefficients |&gt; \n  as.data.frame() |&gt; \n  knitr::kable( \n    digits = 2)\n\n\nTable 1. OLS Regression Summary: Predicting Penguin Flipper Length\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n164.59\n3.18\n51.70\n0\n\n\nbody_mass_g\n0.01\n0.00\n7.04\n0\n\n\nnameChinstrap\n5.54\n0.79\n7.06\n0\n\n\nnameGentoo\n18.02\n1.44\n12.49\n0\n\n\nsexmale\n2.48\n0.85\n2.90\n0\n\n\n\n\n\n\n# model r squared \nsummary(m1)$r.squared[1] %&gt;% \n  round(2)\n\n[1] 0.86\n\n\nAnd there you have it! A simple analysis of penguin data in R. Not too complicated, right? So, why do we need to use {targets}?\nImagine your supervisor asks you to investigate three different hypotheses using much larger datasets and a suite of machine learning models to identify the best predictors. Your exploratory data analysis becomes far more exhaustive, your models take minutes, hours, or even days to run, and your scripts grow increasingly difficult to manage. On top of that, you’re expected to write a report that will be scrutinized and sent back with edits—meaning you’ll have to re-run the entire process all over again."
  },
  {
    "objectID": "blogs/targets_wflow/index.html#using-targets",
    "href": "blogs/targets_wflow/index.html#using-targets",
    "title": "Data Pipelines with {Targets}",
    "section": "Using {targets}",
    "text": "Using {targets}\nLets run through our pipeline again but applying targets this time.\n\n1. Function-orient your workflow\nA key programming best practice is to break your workflow into individual, customized functions. Functions make your code reusable, easier to modify, and more scalable—much more efficient than copying and pasting code repeatedly.\nTo get started, create a folder named R in your root directory (the main folder where your project’s code lives). Inside that folder, create a script called functions.R where you will store all your custom functions.\n\ndir.create(\"R\") # creating an R folder in your working directory\n\nfile.create(\"R/functions.R\") # adding a functions script \n\nonce inside this functions.R script, we will translate our original pipeline into individual functions:\n\n# pull penguin data\npull_data &lt;- function() {\n  return(palmerpenguins::penguins)\n  }\n\n\n# cleaning and wrangling data\nclean_data &lt;- function(data) {\n  data |&gt; # raw data\n    janitor::clean_names() |&gt; # lower case columns\n    mutate(name = stringr::word(species, 1)) |&gt;   # species name column\n    dplyr::select(name, island, flipper_length_mm, body_mass_g, sex) # selecting variables\n}\n\n\n# plot data\nplot_data &lt;- function(data) {\n  ggplot(data = data, \n         aes(x = body_mass_g, y = flipper_length_mm, color = name)) + \n  geom_point() + # add data points\n  geom_smooth(method = \"lm\", se = FALSE) +  # Add trend lines for each species\n  labs(\n    y = \"Flipper Length (mm)\",\n    x = \"Body Mass (g)\",\n    title = \"Penguin Body Mass by Flipper Length\",\n    color = \"Species\"        \n  ) +\n  scale_color_manual( # changing colors \n    values = c(\n      \"Adelie\" = \"blue4\", \n      \"Chinstrap\" = \"tan\",     \n      \"Gentoo\" = \"darkgreen\"   \n    )) +\n  theme_minimal() # minimal appearance\n}\n\n\n# fit model\nfit_model &lt;- function(data) {\n  lm(flipper_length_mm ~ body_mass_g + name + sex, data)\n}\n\n\n# model coefficients \nmodel_coefs &lt;- function(model) {\n  summary(model)$coefficients |&gt; \n    as.data.frame() |&gt; \n    knitr::kable( \n      digits = 2)\n}\n\n\n# model r squared \nmodel_r2 &lt;- function(model) {\n  summary(model)$r.squared\n}\n\n\n\n\n2. Create the {targets} skeleton\nNow that our workflow is neatly packaged into individual functions it is time to create the backbone of our pipeline, a _targets.R script.\nYou first need to run:\n\ntargets::use_targets()\n\nThis will create a _targets.R file in your working directory. The file comes pre-populated with commented instructions and example code, many of which are irrelevant or optional for our example. Important: Delete the example target code inside script before proceeding.\nYou should then fill the file with the following structure:\n\n# _targets.R file\n# Load packages required to define the pipeline:\nlibrary(targets)\n\n# Run the R scripts in the R/ folder with your custom functions:\ntar_source()\n\n# Set target options:\ntar_option_set(packages = c(\"palmerpenguins\", \"janitor\", \"tidyverse\")) # load packages\n\n# Here is where we define our targets using our custom functions\nlist(\n  tar_target(data, pull_data()), \n  tar_target(data_c, clean_data(data)),\n  tar_target(plot, plot_data(data_c)),\n  tar_target(model, fit_model(data_c)),\n  tar_target(coefs, model_coefs(model)),\n  tar_target(r2, model_r2(model))\n)\n\nThis part can feel a little confusing, so let’s take it step by step.\nFirst, load the package with library(targets). Then, use tar_source() to load all the functions from your functions.R script. Next, use tar_option_set() to load any packages your functions depend on. Finally, define your pipeline using list() with individual tar_target() calls.\nEach tar_target() has two main parts:\n\n\nThe name we are assigning to the target object\n\n\nThe code used to generate it, often a function that may use upstream targets as inputs\n\n\nStill confused? That’s okay—let’s break down an example.\n\ntar_target(data, data_pull())\n\nThis tells {targets} to create a target called data using the data_pull() function we wrote earlier (which loads the penguins dataset).\nLets look at the next one:\n\ntar_target(data_c, clean_data(data))\n\nThis creates a new target called data_c using the clean_data() function—with data as its input. In other words, we used the data target inside a function to create a new target (data_c). Cool, right?\nAnd that’s the basic pattern we’ll follow for the rest of the pipeline.\n\n\n\n3. Running {targets}\nLets look at our targets pipeline\n\ntargets::tar_visnetwork()\n\n\n\n\n\n{targets} doesn’t just track dependencies—it also lets you visualize them and see which parts of your pipeline are related or out of date. In the visualization, functions appear as triangles, targets appear as circles, and arrows show relationships between them. If a target is blue, it means it’s outdated—either because its own code changed or because an upstream dependency changed. This will also trigger updates in downstream targets.\nWe haven’t run our targets workflow yet, so it makes sense that everything is outdated. To run and update the pipeline, run:\n\ntar_make() # updating and running our targets objects\n\nAnd to visualize the update:\n\ntar_visnetwork()\n\n\n\n\n\nAt this point, all of your target objects are up to date and reflected in the visual network. To test this, try running tar_make() again. You should see the message: “skipped pipeline”, indicating that nothing has changed and no targets need to be rebuilt.\nEach target is stored as an internal object managed by the package and won’t appear in your global environment like regular R objects. To access a specific target, you can load it manually using: tar_load()\n\ntar_load(data) # load the target data\ntar_load(plot) # load the target plot\ntar_load(coefs) # load the target model coefficients table\n\n\n\n4. Updating {targets}\nSo you ran your entire workflow using {targets}, wrote up some preliminary results, showed your boss, and she says: “Hey, i thought I asked you to put bill length as one of the model predictors in there. Can you go back and add that in there?” This would not be a big deal with such a small analysis such as this one. But, as you know, your analysis can become very large and difficult to track, re-run, and validate if not structured well. So lets test out what this change looks like here.\nWe go back into our functions.R script and add in bill length as a predictor.\nNOTE: Always make sure to save your functions file after a change before running targets\n\n# fit model with bill length\nfit_model &lt;- function(data) {\n  lm(flipper_length_mm ~ body_mass_g + name + sex + bill_length_mm, data)\n}\n\nHow has the pipeline changed?\n\ntar_visnetwork() # visualize outdated targets objects\n\n\n\n\n\nWe can see that the fit_model() function became outdated after we changed its code and saved the file. Consequently, all downstream targets are now outdated too. You can see that model, coefs, and r2 all are dependencies of our function. All other functions and targets that are not related to fit_model() remain unchanged. To fix this, we need to update our pipeline.\n\ntar_make() # re-running our pipeline's outdated objects\n\nSee how the only other objects that changed are the downstream dependencies of fit_model()? This can save an immense amount of headache! Let’s go ahead and look at the visual again and try re-running everything.\n\ntar_visnetwork()\n\n\n\n\ntar_make()\n\n✔ skipped pipeline [81ms, 6 skipped]\n\n\nThe network is all up to date and all 6 targets in the pipeline were skipped."
  },
  {
    "objectID": "blogs/targets_wflow/index.html#final-thoughts",
    "href": "blogs/targets_wflow/index.html#final-thoughts",
    "title": "Data Pipelines with {Targets}",
    "section": "Final thoughts",
    "text": "Final thoughts\nAnd there you have it! An introduction to {targets} by (Landau 2021a). I encourage you to dive into The {targets} R package user manual for a deeper look at the package and other cool features I did not mention here (Landau 2021b). The package can be used in many different ways outside of statistical analyses. It can be expanded to many different aspects of R workflows regardless of your field. While it can be intimidating at first to hop on board with targets, I promise you that your future self will thank you when you have to revise your workflow."
  },
  {
    "objectID": "blogs/targets_wflow/index.html#reporting",
    "href": "blogs/targets_wflow/index.html#reporting",
    "title": "Data Pipelines with {Targets}",
    "section": "Reporting",
    "text": "Reporting\nYou are all finished with your workflow beginning to end in the {targets} pipeline, now what? If I am writing a report, I usually create a Quarto markdown file (qmd) and use tar_load() at the beginning of with package loading to load in all of the target outputs I need. This keeps your lengthy data processing and analysis code separate from the final written product. Now whenever you render your report, you aren’t re-running your entire analysis every time. You are simply just loading in the final and updated targets objects (data/graphs/tables/etc.) that you need! You just need to make sure that your report file lives in the same directory as your _targets.R script."
  }
]