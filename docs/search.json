[
  {
    "objectID": "blogs/sierra_lakes/index.html",
    "href": "blogs/sierra_lakes/index.html",
    "title": "Drivers of Zooplankton Community Dynamics in Sierra Nevada Lakes",
    "section": "",
    "text": "Github project repository"
  },
  {
    "objectID": "blogs/sierra_lakes/index.html#introduction",
    "href": "blogs/sierra_lakes/index.html#introduction",
    "title": "Drivers of Zooplankton Community Dynamics in Sierra Nevada Lakes",
    "section": "Introduction",
    "text": "Introduction\nThe Sierra Nevada Mountains of California contain some of the nations most pristine wilderness, including thousands of lakes scattered throughout the range. Historically, the majority of these lakes were not inhabited with fish due to their fragmented and isolated nature, making it very difficult for fish to access lakes at high elevation. The Department of Fish and Game began to stock these lakes in the 20th century to promote recreational fishing in the range (R. Knapp 1996). While this provided many benefits to society, fish stocking posed many ecological threats to native species (R. Knapp 1996). These introduced trout (Rainbow, Brooke, Golden, Brown, Cutthroat, and hybrids) rely on zooplankton, a broad classifications of small organisms suspended in the water column that feed on phytoplankton and microbes, as a main food source in their fry stage of life (MacLennan, Dings-Avery, and Vinebrooke 2015).\nThis study aims to understand the abiotic (non-living) and biotic (living) factors that influence zooplankton community dynamics in the High Sierra. The questions being asked are:\n1) How does the size and elevation of lakes affect the diversity and richness of zooplankton communities?\n2) Does the presence of fish influence the diversity and richness of zooplankton in these lakes?\nI used multiple linear regression models to understand if and how diversity and richness were being affected by these variables. Because previous literature suggests that lakes support smaller foodwebs with decreasing size (Post, Pace, and Hairston 2000) and become more oligotrophic (less nutrient input) with increasing elevation (Sadro, Nelson, and Melack 2012), I hypothesized that the diversity and richness of zooplankton communities would lessen as lake size decreases and elevation increases. Furthermore, the presence of fish will present selective predation on certain zooplankton species, altering species diversity and richness as well.\n\n\ncode\n# map of the sierra nevada mountains\nmap &lt;- ggmap(basemap) +\n    geom_sf(data = snb, color = \"black\",  fill = \"lightblue4\", alpha = .4, inherit.aes = FALSE) +\n   labs(title = \"The Sierra Nevada Mountains\", x = \"\", y = \"\") +\n  theme_classic() +\n     annotation_scale(location = \"bl\",\n                      pad_y = unit(0.4, \"in\")) +\n  annotation_north_arrow(location = \"bl\",\n                         pad_x = unit(0.3, \"in\"),\n                         pad_y = unit(0.8, \"in\"),\n                         height = unit(.4, \"in\"),\n                           width = unit(.3, \"in\"),\n                         size = 5)\nmap\n\n\n\n\n\nFigure 1. Map of the Sierra Nevada Mountain Range in California. Data: https://gis.data.cnra.ca.gov/datasets/727b3cc24f8549759fe946a298dc3a20/explore?location=38.508918%2C-118.370394%2C6.43"
  },
  {
    "objectID": "blogs/sierra_lakes/index.html#methods",
    "href": "blogs/sierra_lakes/index.html#methods",
    "title": "Drivers of Zooplankton Community Dynamics in Sierra Nevada Lakes",
    "section": "Methods",
    "text": "Methods\nOver 1,100 lakes were surveyed from 1995-2002 as a part of the Sierra Lakes Inventory Project (SLIP), with the goal of describing the biodiversity of lentic water bodies in lakes, ponds, marshes, and meadows in the Southern Sierra (R. A. Knapp et al., n.d.). All study observation and samples were collected between months of July and September. Area and elevation of the lakes were calculated using GPS and GIS for lakes where area was unknown. Fish presence was measured using both visual and net surveys at the edge of lakes to identify species presence. Zooplankton samples were collected in water samples that were then packed out and identified in the lab at the Sierra Nevada Aquatic Research Laboratory (SNARL) in Mammoth Lakes, California. Due to the varying stages of maturity and small sizes of zooplankton, taxon were sometimes identified to genus, family, or life stage if not able to be identified to species level. Zooplankton samples were originally collected in 3 different subsamples, so diversity was averaged for all 3 lake subsamples to calculate each lake’s Shannon-Wiener diversity index. Species richness was calculated by identifying unique taxon across all 3 samples. Zooplankton data was not collected in 1998, 1999, or 2002."
  },
  {
    "objectID": "blogs/sierra_lakes/index.html#results",
    "href": "blogs/sierra_lakes/index.html#results",
    "title": "Drivers of Zooplankton Community Dynamics in Sierra Nevada Lakes",
    "section": "Results",
    "text": "Results\n\nExploratory Data Analysis\nBefore doing any modeling, I wanted to visualize and explore the data a bit to understand how it was distributed. A histogram of the log lake area returned a relatively normally distributed curve with a short right tail, suggesting that medium to larger lakes were more frequently observed than small ones (fig.2.A). Distributions appeared to be similar regardless of fish presence. A histogram of lake elevation showed less of a normal distribution with a long left tail, indicating that the majority of lakes observed were at higher elevations (fig.2.B). Regardless of fish presence, the shape of the curve still resembles somewhat of a normal unimodal shape despite the long tail.\nNote:Log area is used in this study as the area had a skewed distribution, and the log transformation suited the data better for linear modeling.\n\n\ncode\n# making histograms of area and elevation\n\nh1 &lt;- ggplot(data = total , aes(x = log(lake_area_nbr),  fill = actual_fish_presence)) +\n  geom_histogram(color = \"black\", size = .1) +\n  scale_fill_manual(name = \"Fish Present \", values = c(\"lightblue4\", \"cyan3\")) +\n  theme_classic() +\n  labs(x = expression(\"log Lake Area m\"^2))\n\nh2 &lt;- ggplot(data = total , aes(x = lake_elevation_nbr, fill = actual_fish_presence)) +\n  geom_histogram(color = \"black\", size = .1) +\n  scale_fill_manual(name = \"Fish Present \", values = c(\"lightblue4\", \"cyan3\")) +\n  theme_classic() +\n  labs(x = \"Lake Elevation (m)\", y = \"\")\n\nhist &lt;- h1 + h2 +\n  plot_layout(guides = \"collect\",\n              heights = 5) +\n  plot_annotation(tag_levels = \"A\") &\n  theme(legend.position = \"bottom\")\n\nhist \n\n\n\n\n\nFigure 2. A) Histogram showing the frequency of lakes observed by log area. B) Histogram showing the frequency of lakes observed by elevation.\n\n\n\n\ncode\n#correlation of independent variables\nind_cor &lt;- cor(log(total$lake_area_nbr), total$lake_elevation_nbr)\n\n\nOne concern in working with this data was collinearity between independent variables, which can lead to dramatic changes in the dependent variables (diversity and richness) if it occurred. I examined a simple scatterplot to visually inspect for a possible relationship between log lake area and elevation, both with and without fish present (fig.3). While fish seemed to be more frequent in larger lakes and observations were clustered slightly clustered at high elevation, the data points appeared to be scattered and noisy with little evidence of a significant relationship. The correlation between log lake area and elevation was minimal so I kept both independent variables in consideration through the study (r = 0.03.\n\n\ncode\n#making scatterplots with independent variables\n\nscatter &lt;- ggplot(total, aes(y = log(lake_area_nbr), x = lake_elevation_nbr, color = actual_fish_presence)) +\n  geom_point(alpha = .7) + \n  scale_color_manual(name = \"Fish Present \", values = c(\"lightblue4\", \"cyan3\")) +\n  labs(x = \"Lake Elevation (m)\", y = expression(\"log Lake Area m\"^2) ) +\n  theme_classic()\n\nscatter\n\n\n\n\n\nFigure 3. Scatterplot showing relationship of log lake area and lake elevation. Correlation between vairables was low, indicationg no collinearity between dependent variables.\n\n\n\n\n\n\ncode\n#correlation of independent and dependent variables\ncors = total %&gt;% \n  group_by(actual_fish_presence) %&gt;% \n  summarise(\n        cor_da = round(cor(mean_div, log(lake_area_nbr)), 2),\n         cor_de = round(cor(mean_div, lake_elevation_nbr), 2),\n         cor_ra = round(cor(richness, log(lake_area_nbr)), 2),\n         cor_re = round(cor(richness, lake_elevation_nbr), 2))\n\n\nExploratory data analysis suggested that zooplankton species diversity and richness increases with the log area of a lake and decrease with elevation (fig. 4). The correlation was positive and weaker for species diversity and log lake area when fish were absent compared to present (r = 0.11, r = 0.33) (fig.4.A). Similar results occured for species richness and log lake area (r = 0.05, r = 0.36) (fig.4.B). Diversity and richness in lakes were strongly and negatively correlated with with elevation, and the presence of fish did not change the correlation strength noticeably (r = -0.44, r = -0.45) (fig.4.C). Stronger negative relationships were found between species richness and elevation both with and without fish (r = -0.55, r = -0.53) (fig.4.D).\n\n\ncode\n# diversity log lake area\nda &lt;- ggplot(data = total, aes(x = log(lake_area_nbr), y = mean_div, color = actual_fish_presence)) +\n  labs(x = expression(\"log Lake Area m\"^2), y = \"Mean Diversity\") + \n  geom_jitter(alpha = 0.7) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = T, lwd = 0.8) +\n  theme_classic() + \n  scale_color_manual(name = \"Fish Present \", values = c(\"lightblue4\", \"cyan3\")) +\n  stat_cor(aes(label = ..r.label..),\n           r.accuracy = 0.01,\n           label.x.npc = .01,\n            label.y.npc = 1,\n           show.legend = FALSE,\n           size = 2.5)\n\n# diversity log lake elevation\nde &lt;- ggplot(data = total, aes(x = lake_elevation_nbr, y = mean_div, color = actual_fish_presence)) +\n  labs(x = \"Lake Elevation (m)\", y = \"Mean Diversity\") + \n  geom_jitter(alpha = 0.7) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = T, lwd = 0.8) +\n  theme_classic() + \n  scale_color_manual(name = \"Fish Present \", values = c(\"lightblue4\", \"cyan3\")) +\n  stat_cor(aes(label = ..r.label..),\n           r.accuracy = 0.01,\n           label.x.npc = .075,\n            label.y.npc = .25,\n           show.legend = FALSE,\n           size = 2.5)\n\n# richness log lake area\nra&lt;- ggplot(data = total, aes(x = log(lake_area_nbr), y = richness, color = actual_fish_presence)) +\n  labs(x = expression(\"log Lake Area m\"^2), y = \"Richness\") + \n  geom_jitter(alpha = 0.7) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = T, lwd = 0.8) +\n  theme_classic() + \n  scale_color_manual(name = \"Fish Present \", values = c(\"lightblue4\", \"cyan3\")) +\n  stat_cor(aes(label = ..r.label..),\n           r.accuracy = 0.01,\n           label.x.npc = .01,\n            label.y.npc = 1,\n           show.legend = FALSE,\n           size = 2.5)\n\n# diversity log lake elevation\nre &lt;- ggplot(data = total, aes(x = lake_elevation_nbr, y =richness, color = actual_fish_presence)) +\n  labs(x = \"Lake Elevation (m)\", y = \"Richness\") + \n  geom_jitter(alpha = 0.7) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = T, lwd = 0.8) +\n  theme_classic() + \n  scale_color_manual(name = \"Fish Present \", values = c(\"lightblue4\", \"cyan3\")) +\n  stat_cor(aes(label = ..r.label..),\n           r.accuracy = 0.01,\n           label.x.npc = .01,\n            label.y.npc = 1,\n           show.legend = FALSE,\n           size = 2.5)\n\n#combined correlations figure\ncorrelations &lt;- (da + ra)/(de + re) + \n  plot_layout(guides = \"collect\",\n              heights = 5) + \n  plot_annotation(tag_levels = \"A\") &\n  theme(legend.position = \"bottom\")\n# & ylab(NULL) & theme(plot.margin = margin(6.5, 5.5, 0, 5.5)) \n\ncorrelations\n\n\n\n\n\nFigure 4. A) Mean zooplankton species diversity as a function of lake log area; B) Zooplankton species richness as a function of the log area of lakes; C) Mean zooplankton species diversity as a function of lake elevation; D) Zooplankton species richness as a function of lake elevation; Diversity was calculated using the Shannon-Wiener Index.\n\n\n\n\n\n\nMultiple Linear Regression\nThese preliminary results were used to inform further analysis through the use of multiple linear regression models. I tested these initial observed patterns by modeling zooplankton species diversity and richness as a function of the following explanatory variables: log lake area, lake elevation, fish presence, interaction of log lake area x fish presence (eda hinted at a potential effect as seen in the difference of slopes in fig.2.A and fig.2.B), and the addition of a temporal component (year) to see if these community dynamic patterns changed aver time. The goal of these models were to test the strength of the predictor variables in explaining the variation in zooplankton diversity and richness and to also test if these variable were statistically significance (\\(\\alpha\\) = .05).\n\\[diversity_i=\\beta_{0}+\\beta_{1} \\cdot log(area)_i + \\beta_{2} \\cdot elevation_i + \\beta_{3} \\cdot fish_i + \\beta_{4} \\cdot log(area)_i \\cdot fish_i + \\beta_{5} \\cdot year_i + \\varepsilon_i\\]\n\\[richness_i=\\beta_{0}+\\beta_{1} \\cdot log(area)_i + \\beta_{2} \\cdot elevation_i + \\beta_{3} \\cdot fish_i + \\beta_{4} \\cdot log(area)_i \\cdot fish_i + \\beta_{5} \\cdot year_i + \\varepsilon_i\\]\nThe species diversity model results conclude that the log area (positively) and lake elevation (negatively) significantly affected zooplankton species diversity (p &lt; .05, p &lt; .001). While lakes with fish present reduced zooplankton diversity, no significance was found (P &gt; .05). Similarly, the interaction between log lake area and fish presence was insignificant but positively affected diversity (more so than lakes without fish) (p = .14). No individual year had a significant effect on species diversity.\nThe species richness model also showed that the log lake area (positively) and lake elevation (negatively) significantly affected zooplankton richness (p &lt; .05, p &lt; .001). Unlike the diversity model, species richness was significantly and negatively affected by the presence of fish in lakes (p &lt; 0.05). The interaction between log lake area and fish presence also was a significant factor in affecting species richness, indicating that the size of the lake affected zooplankton richness differently depending on whether fish were present or not (p &lt; .05). No individual year had a significant effect on species richness.\n\n\ncode\n#diversity model \nm_div &lt;- lm(mean_div ~ log(lake_area_nbr) + lake_elevation_nbr + actual_fish_presence + log(lake_area_nbr):actual_fish_presence + year, data = total)\n# m_div_sum &lt;- summary(m_div)\n\n\n#richness model\nm_rich &lt;- lm(richness ~ log(lake_area_nbr) + lake_elevation_nbr + actual_fish_presence + log(lake_area_nbr):actual_fish_presence +  year, data = total)\n# summary(m_rich)\n\n#creatign a table \ntab_model(m_div, m_rich,\n          pred.labels = c(\"Intercept\", \"log Area\", \"Elevation\",\n                          \"Fish Presence(Yes)\", \"1997\",\n                          \"2000\", \"2001\",\n                          \"log Area x Fish Presence(Yes)\"),\n          dv.labels = c(\"Divsersity\", \"Richness\"),\n          string.ci = \"Conf. Int (95%)\",\n          string.p = \"p-value\",\n          title = \"Table 1. Multiple Linear Regression Model Results.\",\n          digits = 4)\n\n\n\nTable 1. Multiple Linear Regression Model Results.\n\n\n \nDivsersity\nRichness\n\n\nPredictors\nEstimates\nConf. Int (95%)\np-value\nEstimates\nConf. Int (95%)\np-value\n\n\nIntercept\n1.8560\n1.4600 – 2.2521\n&lt;0.001\n12.6131\n10.7139 – 14.5123\n&lt;0.001\n\n\nlog Area\n0.0285\n0.0046 – 0.0523\n0.019\n0.1383\n0.0238 – 0.2527\n0.018\n\n\nElevation\n-0.0004\n-0.0005 – -0.0003\n&lt;0.001\n-0.0026\n-0.0031 – -0.0021\n&lt;0.001\n\n\nFish Presence(Yes)\n-0.3205\n-0.7335 – 0.0924\n0.128\n-2.2044\n-4.1847 – -0.2242\n0.029\n\n\n1997\n0.0830\n-0.0317 – 0.1976\n0.156\n-0.0721\n-0.6218 – 0.4777\n0.797\n\n\n2000\n-0.0304\n-0.1515 – 0.0906\n0.621\n-0.3699\n-0.9504 – 0.2106\n0.211\n\n\n2001\n0.0192\n-0.1260 – 0.1644\n0.795\n-0.1259\n-0.8223 – 0.5706\n0.723\n\n\nlog Area x Fish Presence(Yes)\n0.0305\n-0.0100 – 0.0710\n0.140\n0.2185\n0.0242 – 0.4129\n0.028\n\n\nObservations\n545\n545\n\n\nR2 / R2 adjusted\n0.240 / 0.230\n0.349 / 0.340\n\n\n\n\n\n\n\n\n\nResiduals\nQQ plots were made to test if the residuals were normally distributed (fig.5). As seen in the plots, the residuals for both models fit the normal distribution quite well, with the exception of a slight and minor deviation in the tails. This implies that the diversity model residuals have a slight negative skew (fig.5.A), and the richness model has slightly fatter than expected tails (fig.5.B). In conclusion, the QQ plots imply assumptions of normality are met.\n\n\ncode\n#diversity model residuals\nm_div_res &lt;- residuals(m_div)\n#richness model residuals\nm_rich_res &lt;- residuals(m_rich)\n\n#qq plot for diversity \ndiv_qq &lt;-  ggplot( m_div, aes(sample = .resid)) +\n  geom_qq(color = \"lightblue4\") +\n  geom_qq_line() +\n  theme_classic() +\n  labs( x = \"Theoretical Quantiles\", y = \"Sample Quantiles\")\n\n#qq plot for richness\nrich_qq &lt;-  ggplot( m_rich, aes(sample = .resid)) +\n  geom_qq(color = \"lightblue4\") +\n  geom_qq_line() +\n  theme_classic() +\n  labs( x = \"Theoretical Quantiles\", y = \"\")\n\n#joining plots\nqq &lt;- div_qq + rich_qq +\nplot_annotation(tag_levels = \"A\") \n\nqq\n\n\n\n\n\nFigure 5. A) QQ plot of zooplankton species diversity model residuals. Theororetical and sample residuals are generally suggestive of a normal distribution, but show slight deviation in the tails which suggest the distribution has a slight negative skew. B) QQ plot of zooplankton species richness model residuals. Theororetical and sample residuals are generally suggestive of a normal distribution, but show slight deviation in the tails which suggest the distribution has fat tails.\n\n\n\n\ncode\nggsave(\"figures/residuals.png\", qq, height = 5, width = 8, unit = 'in')"
  },
  {
    "objectID": "blogs/sierra_lakes/index.html#discussion",
    "href": "blogs/sierra_lakes/index.html#discussion",
    "title": "Drivers of Zooplankton Community Dynamics in Sierra Nevada Lakes",
    "section": "Discussion",
    "text": "Discussion\nZooplankton species diversity and richness are closely related, and they both responded similarly to abiotic conditions (log lake area and lake elevation). (Urmy and Warren 2019) also showed that zooplankton species responded noticeably across elevation even at a fine scale. However, species richness was noticeably lower in lakes with fish present compared to without when holding lake area and elevation constant, indicating that biotic factors (fish present) alter the ability of specific zooplankton species to persist (likely due to taxa specific predation).\nThe most interesting finding from these results is that the area of a lake affects the zooplankton species richness noticeably more when fish are present as seen in the interaction coefficient of the richness model. Richness is very low in small lakes and very high in large lakes when fish are present (steep positive slope), whereas richness is moderate to high in lakes without fish (moderate positive slope). Other studies have found similar results where the effects of fish predation on zooplankton are amplified by varying environmental conditions (MacLennan, Dings-Avery, and Vinebrooke 2015). This begs the question: Why are zooplankton more sensitive to changes in lake area when fish are present vs absent? This question can be answered by the ecological principle known as the Island Biogeography Theory which states that prey on smaller islands (or habitats) experience more intense predation pressure from predators than on larger islands (MacArthur and Wilson 2001). Due to their isolation and limited resources, zoo plankton in small lakes experience higher extinction rates from concetrated predation, less resources (nutrient input), and lower emigration . This applies quite literally into our study system here were introduced fish in small lakes (islands) suppress zooplankton species richness (prey) more intensely than in larger lakes (islands). Similar patterns have also been observed across taxa in lakes across North America (Browne 1981).\nThis study did not include a spatial element as coordinates were not provided in the data as a measure to keep sensitive and rare fish lakes discreet. Future research should look at other taxa in oligotrophic alpine lake food webs (algae, microbes, benthic macroinvertebrates) to understand potential trophic cascades and community wide effects of fish introduction."
  },
  {
    "objectID": "blogs/houston_blackout/houston_blackout.html",
    "href": "blogs/houston_blackout/houston_blackout.html",
    "title": "A Spatial Analysis of Houston Blackouts and the Communtiies it Affected",
    "section": "",
    "text": "Github project repository"
  },
  {
    "objectID": "blogs/houston_blackout/houston_blackout.html#background",
    "href": "blogs/houston_blackout/houston_blackout.html#background",
    "title": "A Spatial Analysis of Houston Blackouts and the Communtiies it Affected",
    "section": "Background",
    "text": "Background\nNatural disasters oftentimes occur unpredictably and rapidly, and the severity of them can vary widely. Specifically in the Southern United States, major storms (like Hurricane Katrina) can have devastating and long lasting consequences on communities.1 “In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”2. While these storms seem like once in a blue moon events, they are becoming much more of a normal occurrence. More importantly, storms do not impact all communities the same, and therefore it is critical that we incorporate racial justice components to our lines of environmental analysese to understand how we can mitigate marginalized groups from being disproportianately harmed from climate change."
  },
  {
    "objectID": "blogs/houston_blackout/houston_blackout.html#goal",
    "href": "blogs/houston_blackout/houston_blackout.html#goal",
    "title": "A Spatial Analysis of Houston Blackouts and the Communtiies it Affected",
    "section": "Goal",
    "text": "Goal\nThe goal of this study is to try and answer the question: Which communities in Houston experienced blackouts from the February 2021 storms, and did they vary by socioeconomic status? Other studies have shown that not only do lower socioeconomic status communities experience more intense consequences of natural disaster, but they also don’t always receive the same degree of aid following one.3\nBy identifying the total number of houses in Houston that lost power and overlaying socioeconomic data to these homes, we can begin to look at these underlying patterns of potential environmental injustice. These types of studies are becoming critically important as climate change continues to increase the frequency of severe storms including intense precipitation events throughout Texas4 as not all communities are affected the same. We need to look at how communities of varying socioeconomic status are impacted as a result."
  },
  {
    "objectID": "blogs/houston_blackout/houston_blackout.html#approach",
    "href": "blogs/houston_blackout/houston_blackout.html#approach",
    "title": "A Spatial Analysis of Houston Blackouts and the Communtiies it Affected",
    "section": "Approach",
    "text": "Approach\nI used remotely-sensed night lights data from before and after the storms to estimate power outages. Data was acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) from the Suomi satellite. I used the VNP46A1 tolook for variation in night lights before and after the storm as a way of identifying areas that lost electric power as a result of the storms. Furthermore, data containing the physical boundaires and area of buildings and roads is needed to spatially join to areas that lost power. I gathered this data from OpenStreetMap providing high resolution data on the structural layout of the city. Finally, we need to join this data with socioeconomic factor data pertaining to all the communities in Houston affected by the blackouts. I pulled this data from the US Census Bureau (see Readme file) that shows block group county level information on various socioeconomic characterisitcs.\nThese types of analysis require a variety of different tools to carry out from satellite images and census data to finalized maps. Some of the most important tools in carrying out this analysis include: * loading, transforming, and wrangling vector/raster data * raster operations to identify blackout areas from satellite imagery * vector operations to isolate affected buildings and remove interference (road lights) * spatial joins of regions experiencing blackouts and the total number of untis within them\n\nData\n\nNight lights\nUsing NASA’s Worldview to find data pre and post storm, I found satellite images recorded on 2021-02-07 and 2021-02-16 that provided two clear, contrasting images to visualize the extent of the power outage in Texas. VIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection that are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06, and so I downloaded two tiles per date to ensure the whole city was being included. These data are stored in the VNP46A1 folder:\n\nVNP46A1.A2021038.h08v05.001.2021039064328.h5.tif: tile h08v05, collected on 2021-02-07\n\nVNP46A1.A2021038.h08v06.001.2021039064329.h5.tif: tile h08v06, collected on 2021-02-07\n\nVNP46A1.A2021047.h08v05.001.2021048091106.h5.tif: tile h08v05, collected on 2021-02-16\n\nVNP46A1.A2021047.h08v06.001.2021048091105.h5.tif: tile h08v06, collected on 2021-02-16\n\n\n\nRoads\nOne issue with night light data is that light pollution can interfere with detecting patterns of interest. Highways make up much of the night lights seen from space (see Google’s Earth at Night). Therefore, it is important to minimize falsely identifying areas with reduced traffic as areas without power. OpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. I used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area.\n\ngis_osm_roads_free_1.gpkg\n\n\n\nHouses\nI also obtained building data from OpenStreetMap. Again, I downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\n\ngis_osm_buildings_a_free_1.gpkg\n\n\n\nSocioeconomic\nI used data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019 to reoresent socioeconomic data of different regions of Houston. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file. Each layer contains a subset of the fields documents in the ACS metadata. The geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use."
  },
  {
    "objectID": "blogs/houston_blackout/houston_blackout.html#analysis",
    "href": "blogs/houston_blackout/houston_blackout.html#analysis",
    "title": "A Spatial Analysis of Houston Blackouts and the Communtiies it Affected",
    "section": "Analysis",
    "text": "Analysis\n\nFind locations of blackouts\n\ncombine the data\nFirst, I read in the night lights tiles data and combining them into a single stars object for each date (2021-02-07 and 2021-02-16).\n\n\ncode\n#reading in raster data \nt1 = read_stars(file.path(data, \"VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif\"))\nt2 = read_stars(file.path(data,\"VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif\"))\nt3 = read_stars(file.path(data,\"VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif\"))\nt4 = read_stars(file.path(data,\"VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif\"))\n\n#creating a mosaic of houston raster data for both days \nlights_mos1 &lt;- st_mosaic(t1, t2)\nlights_mos2 &lt;- st_mosaic(t3, t4)\n\n# plot(lights_mos1)\n\n\n\n\ncreate a blackout mask\nI then found the change in night lights intensity (presumably) caused by the storm. I first reclassified a difference raster, assuming that any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout. NA values were to all locations that experienced a drop of less than 200 nW cm-2sr-1.\n\n\ncode\n#finding difference in light intensity by subtracting light difference \nlight_diff &lt;- lights_mos1 - lights_mos2\n\n# plot(light_diff)\n\n#reclassify raster to give NA values to locations that did not experience a blackout\nlight_diff[light_diff &lt; 200] = NA\n\n# plot(light_diff)\n\n\n\n\nvectorize the mask\nI vectorized the the blackout mask and fixed any invalid geometries using st_make_valid. I then inspected it to make sure it was an sf object and to look at its characteristics.\n\n\ncode\n#creating a black out mask as a sf object. This creates a mask of geometries that experienced a black out\nbo_mask &lt;- st_as_sf(light_diff)\n\n#making sure geometries are valid\nst_make_valid(bo_mask)\n\n#making sure new object is an sf and inspecting it\nclass(bo_mask)\n# plot(bo_mask)\nsummary(bo_mask)\n\n\n\n\ncrop the vectorized map to Houston\nI defined the Houston metropolitan area with the following coordinates: (-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29) and then turned them into a polygon using st_polygon. A CRS was then added to the polygon to make a simple features collection using st_sfc(). Lastly, I cropped (spatially subset) the blackout mask to our region of interest and re-project the cropped blackout dataset to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area).\n\n\ncode\n#creating a matrix of coordinates for houston\n#note: last coordinate need to be same as the first\nhstn_cords &lt;- matrix(c(-96.5, 29, -96.5, 30.5, -94.5, 30.5, -94.5, 29, -96.5, 29), ncol = 2, byrow = TRUE)\n\n#creating a polygon for houston \nhstn_poly &lt;- st_polygon(list(hstn_cords))\n\nclass(hstn_poly)\n\n#making a simple feature collection using same crs as bo_mask\nhstn_sf &lt;- st_sfc(hstn_poly, crs = 4326)\n\nclass(hstn_sf)\n\n#cropping black out mask to the houston sf object \nbo_mask_houst &lt;- bo_mask[hstn_sf, ] \n\n#transforming crs to EPSG:3083 \nbo_mask_houst &lt;- st_transform(bo_mask_houst, 3083)\n\n\n\n\nCheck #1: Ensurign that the mask function worked\n\n\ncode\n##### creating a check ######\n\ncat(\"CHECK #1 - see if raster data is masked within Houston:\", \"\\n\")\n\n\nCHECK #1 - see if raster data is masked within Houston: \n\n\ncode\nif(nrow(bo_mask_houst) == nrow(bo_mask)) {\n  cat(\"No: mask did not work.\", \"\\n\")\n} else {\n  cat(\" Yes, all black out raster data lie within the Houston polygon.\", \"\\n\")\n}\n\n\n Yes, all black out raster data lie within the Houston polygon. \n\n\n \n\n\nexclude highways from blackout mask\nThe roads geopackage provoded includes data on roads other than highways. However, we can avoid reading in data we don’t need by taking advantage of st_read’s ability to subset using a SQL query. This allows us to save comutational energy by just pulling in relevant data we want.\nFirst I defined a SQL query:\nquery &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\nI applied to st_read to load just highway data from geopackage. I then reprojected the data to EPSG:3083 and identified areas within 200m of all highways using st_buffer and dissolved them with st_union. Lastly, I find areas that experienced blackouts that are further than 200m from a highway through spatial subsetting using the st_disjoint operation.\n\n\ncode\n#assigning a query that pulls highway roads\nhighway_query &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n#reading in highway data \nhighways &lt;- st_read(file.path(data,\"gis_osm_roads_free_1.gpkg\"), query = highway_query, quiet = TRUE)\n\n#transforming highways crs to 3083\nhighways &lt;- st_transform(highways, 3083)\n\n#creating a 200m buffer around highways \nhigh_buff &lt;- st_buffer(highways, dist = 200)\n\n#dissolveing buffers \nhigh_buff_dis &lt;- st_union(high_buff)\n\n# plot(high_buff_dis)\n\n#finding areas more than 200m from a highway that experienced a blackout \nmask_no_hwy &lt;- bo_mask_houst[high_buff_dis, op = st_disjoint]\n\n\n\n\nCheck #2: see if the highway exclusion mask removed all polygons within the 200m of highways\n\n\ncode\n#### creating a check #######\ncat(\"CHECK #2 - see if the highway exclusion mask removed all polygons within the 200m of highways:\", \"\\n\")\n\n\nCHECK #2 - see if the highway exclusion mask removed all polygons within the 200m of highways: \n\n\ncode\nif(nrow(mask_no_hwy) + nrow(bo_mask_houst[high_buff_dis, ]) == nrow(bo_mask_houst)) {\n  cat(\"Yes, the highway mask removed all polygons within the 200m of highways.\", \"\\n\")\n  } else {\n    cat(\"Error! Observations within highway mask are included.\", \"\\n\")\n  }\n\n\nYes, the highway mask removed all polygons within the 200m of highways. \n\n\n \n\n\n\nFind homes impacted by blackouts\n\nload buildings data\nThe buildings dataset was then loaded in using st_read and the following SQL query:\nSELECT *  FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\n\nAgain, this reduces the amount of data pulled inoto R, saving us time, energy, and storage.\n\n\ncode\n#making a query for pulling specific building data\nbuildings_query &lt;- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\n#reading in building data \nbuildings &lt;- st_read(file.path(data, \"gis_osm_buildings_a_free_1.gpkg\"), query = buildings_query, quiet = TRUE)\n\n#transforming crs to 3083 \nbuildings &lt;- st_transform(buildings, 3083)\n\n# plot(buildings)\n\n\n\n\nfind homes in blackout areas\nI then filtered the mask to homes within blackout areas and counted the number of impacted homes.\n\n\ncode\n#subsettign the number of homes experiencing blackouts \n#I am including areas inside of houston that also extend outside of Houston border using st_intersects\nbo_homes &lt;- buildings[mask_no_hwy, ]\n\n#counting blackout homes\nbo_homes_cnt &lt;- comma(nrow(bo_homes))\n\ncat(\"Number of homes that experienced blackouts during the storm:\", bo_homes_cnt, \"\\n\")\n\n\nNumber of homes that experienced blackouts during the storm: 139,148 \n\n\n\n\n\n\nInvestigate socioeconomic factors\n\nload ACS data\nFirst, I read in the geodatabase layers using st_read() (geometries are stored in the ACS_2019_5YR_TRACT_48_TEXAS layer and income data is stored in the X19_INCOME layer). For this analysis, I used the median income field B19013e1 for income. Data was then reprojected to EPSG:3083.\n\n\ncode\n# reading ACS gdb layers\nst_layers(file.path(data, \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"))\n\n# reading in geometries layer\nacs_geoms &lt;- st_read(file.path(data, \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"), layer = \"ACS_2019_5YR_TRACT_48_TEXAS\") %&gt;% \n  st_transform(\"EPSG:3083\")\n\n# reading in income layer\n#selecting just GEOID and median income \nacs_income &lt;- st_read(file.path(data, \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"), layer = \"X19_INCOME\") %&gt;% \n  select(GEOID_Data = GEOID, med_income = B19013e1)\n\n\n\n\ndetermine which census tracts experienced blackouts\nNext, I joined the income data to the census tract geometries uaing a left_join function by “ID”. This was then spatially joined to the census tract data with buildings determined to be impacted by blackouts using the st_intersects operation. Lastly, census tracts that had blackouts were identified.\n\n\ncode\n#finding out how many and which census tracts experienced a blackout\n\n#joining income data to geometries \nacs_join &lt;- left_join(acs_geoms, acs_income)\n\n# spatial join census tract data with buildings \nspat_join &lt;- acs_join %&gt;% \n   st_filter(bo_homes, .predicate = st_intersects)\n\n# length(spat_join)\n# names(spat_join)\n\n#census tracts that experience a blackout\nbo_tract_list &lt;- unique(spat_join$NAMELSAD)\n\n#count of census tracts that experience a blackout\nbo_tract_count &lt;- length(bo_tract_list)\n\n\n\n\nCHECK #3: making sure only census tracts with homes that experienced a blackout are included:\n\n\ncode\n### creating a check #####\ncat(\"CHECK #3 - making sure only census tracts with homes that experienced a blackout are included:\", \"\\n\")\n\n\nCHECK #3 - making sure only census tracts with homes that experienced a blackout are included: \n\n\ncode\nif(nrow(acs_join) &gt; nrow(spat_join)) {\n  cat(\"Yes, only black out census tracts included.\", \"\\n\")\n} else {\n  cat(\"No, filter didn't work\", \"\\n\")\n}\n\n\nYes, only black out census tracts included. \n\n\n\n\ncompare incomes of impacted tracts to unimpacted tracts\nFinally, I created a map of median income by census tract and designated which tracts experienced blackouts. I then plot the distribution of income in impacted and nonimpacted tracts to visualize what the spread of the community income looked like.\n\n\ncode\n# In this section, I am mapping the median income of affected census tracts from the black out to visually show what the income of affected areas are in Houston\n\n#check census tract crs\nst_crs(acs_join)\n\n#transform Houston polygon crs to match census tract crs\nhstn_3083 &lt;- st_transform(hstn_sf, 3083)\n\n# masking census tracts to houston\ntract_mask &lt;- acs_join[hstn_3083, ]\n\n#making a new column saying whether or not tracts was a black out or not\ntract_mask &lt;- tract_mask %&gt;% \n  mutate(blackout = ifelse(NAMELSAD %in% bo_tract_list, \"TRUE\", \"FALSE\"))\n\n\n\n# mapping median income of census tracts that experienced blackouts\n\n# getting a polygon of texas \ntexas &lt;- us_states %&gt;% \n  filter(NAME == \"Texas\") %&gt;% \n  st_transform(3083)\n\n#making an inset map of texas with Houston filled in red  \ninset_map &lt;- ggplot() +\n  geom_sf(data = texas) + \n  geom_sf(data = hstn_3083, fill = \"red\") + \n  theme_bw() +\n   theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.grid.minor = element_blank())\n\n\n#creating a main map of census tracts that experienced blackouts by color \nmain_map &lt;-\n  ggplot() +\n  geom_sf(data = tract_mask, color = \"lightgrey\") +\n  geom_sf(data = spat_join, aes(fill = med_income)) +\n  scale_fill_viridis_c() +\n  labs(fill = \"Median Income ($)\") +\n  theme_minimal() +\n  annotation_scale(plot_unit = \"m\", location = \"bl\") +\n  theme( legend.position= c(1.25, .75),\n         plot.margin=unit(c(1,6,.5,1),\"cm\")) +\n  annotation_north_arrow(location = \"bl\",\n                         pad_x = unit(0.05, \"in\"),\n                         pad_y = unit(0.3, \"in\"),\n                         style = ggspatial::north_arrow_minimal())\n\n#combining main and inset map \n comb_map &lt;- main_map + \n    inset_element(inset_map, 1, -0.4, 1.4, .9) +\n  plot_annotation(tag_levels = \"A\")\n \n comb_map\n\n\n\n\n\nFigure 1. Median income of census tracts in Houston, TX that experienced blackouts in February 2021. Map A shows all census tracts in Houston area, were tracts that experienced a blackout color coded by the median income of houslholds within the tracts. Brighter colored tracts have higher median incomes where darker colored tracts have lower. Map B shows an inset map of where Houston is located in the state of Texas.\n\n\n\n\n\n\n\ncode\n# here I am creating a histogram and box plot to compare the median incomes of affected and unaffected tracts\n\n#first, calculate median of median incomes for both groups \nmeds &lt;- tract_mask %&gt;% \n  group_by(blackout) %&gt;% \n  summarize(med_med_income = median(med_income, na.rm = TRUE)) %&gt;% \n  st_drop_geometry()\n\n\n#creating a histogram of income distribution \nhist &lt;- ggplot(tract_mask, aes(x = med_income, fill = blackout)) +\n  geom_histogram(bins = 65) +\n  labs(x = \"Median income ($)\", y = \"Frequency\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  geom_segment(data = meds, aes(x = med_med_income, xend = med_med_income, y = 0, yend = 80, color = blackout), linetype = \"dashed\") +\n  # geom_vline(data = meds, aes(xintercept = med_med_income, color = blackout), linetype = \"dashed\") +\n  scale_fill_manual(name = \"\", values = c( \"lightblue\", \"orange\"), labels = c(\"No blackouts\", \"Blackouts\" )) +\n  scale_color_manual(name = \"Median\", values = c( \"blue\", \"orange3\"), labels = c(\"No blackouts\", \"Blackouts\" )) \n  # coord_cartesian(ylim = c(0, 75))\n  \n  \n#creating a boxplot of median income distribution \nbox_p &lt;- ggplot(tract_mask, aes(x = blackout, y = med_income, fill = blackout)) +\n  geom_boxplot(width = 0.5, show.legend = FALSE) +\n  # geom_jitter(width = 0.2, alpha = 0.5, color = \"grey\") +  \n  labs(y = \"Median Income ($)\", x = \"\") +\n  theme_minimal() +\n  scale_fill_manual( name = \"\", values = c( \"lightblue\", \"orange\"), labels = c(\"No blackouts\", \"Blackouts\")) +\n  scale_x_discrete(labels = c(\"FALSE\" = \"No blackouts\", \"TRUE\" = \"Blackouts\")) \n\n\n#adding up plots into one using patchwork\npatched &lt;- hist + box_p + \n  plot_annotation(tag_levels = \"A\") +\n  plot_layout(guides = \"collect\") &\n  theme(legend.position = \"bottom\")\n\npatched\n\n\n\n\n\nFigure 2. Median houshold income distribution of census tracts in blackout vs non-blackout areas in Houston, 2021. The first plot A shows a histogram of the distribution of median income in both groups of tracts. The frequency is concentrated just under $50,000 for both groups. Data is abnormally distributed with a long right tail, indiating most housholds are lower income with some higher income households drawing the tail right. Plot B shows a whisker plot of median distribution. Boxes represent the inter quartile range (IQR, 50% of the data), and the line inside each box represents the median. The whiskers represent 1.5 times the length of the IQR, with some outliers falling above the whiskers.\n\n\n\n\n\n\n\n\nConclusion\nOverall, these results show that census tracts had a similar median income distribution regardless of whether or not they experienced a black out in the February 2021 storm, but income was slightly higher for tracts that experienced a blackout than those that did not. The median of the census tract median income distribution was $57,002 for non-blackout tracts and $60,642 for blackout tracts. The majority of the income distribution is centered at about $35,000 - $45,000, with some outliers in the $200,000 + range. Some of the limitations of the study are that the census tracts don’t represent how many homes experienced a black out or how long the black out was experienced for. These factors could have underlying socioeconomic patterns that we can’t see here which could explain why certain tracts experienced blackouts for the duration that they did. This should be an area for fututre investigation to strengthen the results found in this study."
  },
  {
    "objectID": "blogs/houston_blackout/houston_blackout.html#footnotes",
    "href": "blogs/houston_blackout/houston_blackout.html#footnotes",
    "title": "A Spatial Analysis of Houston Blackouts and the Communtiies it Affected",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLaJoie, A. S., Sprang, G., & McKinney, W. P. (2010). Long‐term effects of Hurricane Katrina on the psychological well‐being of evacuees. Disasters, 34(4), 1031-1044.↩︎\nHess, D. B. (2005). Access to employment for adults in poverty in the Buffalo-Niagara region. Urban Studies, 42(7), 1177-1200.↩︎\nHess, D. B. (2005). Access to employment for adults in poverty in the Buffalo-Niagara region. Urban Studies, 42(7), 1177-1200.↩︎\nLi, Z., Li, X., Wang, Y., & Quiring, S. M. (2019). Impact of climate change on precipitation patterns in Houston, Texas, USA. Anthropocene, 25, 100193.↩︎"
  },
  {
    "objectID": "blogs/landuse_cover_ML/index.html",
    "href": "blogs/landuse_cover_ML/index.html",
    "title": "Applying Supervised Machine Learning to Landuse Cover in Santa Barbara, CA",
    "section": "",
    "text": "Humans have been altering the natural world for centuries through agriculture, farming, development, recreation, etc. The impacts of landuse change have become critical to understand as evidence shows that it contributes significantly to climate change and is responsible for ecological degredation globally.[^1] Monitoring the distribution and change in landuse types can help us understand the impacts of climate change, natural disasters, deforestation, urbanization, and much more.\nDale, V. H. (1997). The relationship between land‐use change and climate change. Ecological applications, 7(3), 753-769.\nDetermining land cover types over large areas is a major application of remote sensing because we are able to distinguish different materials based on their spectral reflectance. In other words, remote sensing has opened up new doors to study land use change by looking at different proportions of light that it reflects up to satellites. By utiliizng remotely sensed imagery, we can classify landcover into classes or groups that allow us to understand the distribution and change in landcover types over large areas. There are many approaches for performing landcover classification – supervised approaches use training data labeled by the user, whereas unsupervised approaches use algorithms to create groups which are identified by the user afterward."
  },
  {
    "objectID": "blogs/landuse_cover_ML/index.html#summary",
    "href": "blogs/landuse_cover_ML/index.html#summary",
    "title": "Applying Supervised Machine Learning to Landuse Cover in Santa Barbara, CA",
    "section": "Summary",
    "text": "Summary\nMy approach to this analysis can be summed up in 5 steps:\n\nStep 1: load and process Landsat scene data\nStep 2: crop and mask Landsat data to study area (Santa Barbara)\nStep 3: extract spectral data at training sites (subset of parcels within Santa Barbara)\nStep 4: train and apply decision tree classifier\nStep 5: plot results"
  },
  {
    "objectID": "blogs/landuse_cover_ML/index.html#data",
    "href": "blogs/landuse_cover_ML/index.html#data",
    "title": "Applying Supervised Machine Learning to Landuse Cover in Santa Barbara, CA",
    "section": "Data",
    "text": "Data\nLandsat 5 Thematic Mapper\nData was obtained from Landsat 5, including 1 scene from September 25, 2007. The specific spectral bands being used are: 1, 2, 3, 4, 5, 7 from the collection 2 surface reflectance product.\nStudy area and training data A polygon representing southern Santa Barbara county was used as the overall study site. I also used polygons of regions within Santa Barbara representing training sites, including character string with land cover type. The decision tree derived from the training data is then applied to the entire Santa Barbara county polygon to create predictions of landuse cover for the whole region."
  },
  {
    "objectID": "blogs/landuse_cover_ML/index.html#process-data",
    "href": "blogs/landuse_cover_ML/index.html#process-data",
    "title": "Applying Supervised Machine Learning to Landuse Cover in Santa Barbara, CA",
    "section": "Process data",
    "text": "Process data\n\nLoad packages and set working directory\nBecause this project required working with both vector and raster data, I used both sf and terra packages in my workflow. To train our classification decision tree and plot the results, I used the rpart and rpart.plot packages.\n\n\ncode\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, results = FALSE, fig.align = \"center\")\n\nlibrary(sf)\nlibrary(terra)\nlibrary(here)\nlibrary(dplyr)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(tmap)\n\n\n\n\nLoad Landsat data\nFirst, I created a raster stack based on the 6 bands I worked with. Each file name ends with the band number (e.g. B1.tif). Band 6 corresponds to thermal data, which we will not be working with for this analysis, so it was not included in the data. To create a raster stack, I made a list of the files that I wanted to work with and read them all in at once using the rast function. The names of the layers were then updated to match the spectral bands and plot a true color image to see what we’re working with.\n\n\ncode\n# list files for each band, including the full file path\nfilelist &lt;- list.files(file.path(data, \"landsat-data/\"), full.names = TRUE)\n\n# read in and store as a raster stack\n\nlandsat &lt;- rast(filelist)\nlandsat\n\n# update layer names to match band\n\nnames(landsat) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nlandsat\n\n# plot true color image\n\nplotRGB(landsat, r = 3, g = 2, blue = 1, stretch = \"lin\")\n\n\n\n\n\n\n\n\n\n\n\nLoad study area\nI wanted to constrain the analysis to the southern portion of the county where there was training data, so I read in a file that defines the region of interest (ROI). I made sure that both CRS from the shape file and the Landsat data matched.\n\n\ncode\n# read in shapefile for southern portion of SB county\nSB_county_south &lt;- st_read(file.path(data, \"SB_county_south.shp\"))\n\n# project to match the Landsat data\nSB_county_south &lt;- st_transform(SB_county_south, crs = st_crs(landsat))\n\n# plot(SB_county_south)\n\n\n\n\nCrop and mask Landsat data to study area\nNext, I cropped and masked the Landsat data to the study area. This reduced the amount of data that needed to be worked with and therefore saves computational time and energy. Furthermore, I removed objects that I no longer was going to be working with using the rm() function to save space (optional).\n\n\ncode\n# crop Landsat scene to the extent of the SB county shapefile\nlandsat_cropped &lt;- crop(landsat, SB_county_south)\n\n# mask the raster to southern portion of SB county\nlandsat_masked &lt;- mask(landsat_cropped, SB_county_south)\n\n# remove unnecessary object from environment\nrm(landsat, landsat_cropped, SB_county_south)\n\nplotRGB(landsat_masked, r = 3, g = 2, blue = 1, stretch = \"lin\")\n\n\n\n\n\n\n\n\n\n\n\nConvert Landsat values to reflectance\nNext, I converted the values in the raster stack to correspond to reflectance values. To do so, I first removed erroneous values and applied any scaling factors to convert to reflectance. In this case, I worked with Landsat Collection 2. The valid range of pixel values for this collection is 7,273-43,636, with a multiplicative scale factor of 0.0000275 and an additive scale factor of -0.2. So I reclassified any erroneous values as NA and updated the values for each pixel based on the scaling factors. Now the pixel values should range from 0-100%.\n\n\ncode\nsummary(landsat_masked)\n# reclassify erroneous values as NA\nrcl &lt;- matrix(c(-Inf, 7273, NA,\n         43636, Inf, NA), \n        ncol = 3, byrow = TRUE)\n\n\nlandsat &lt;- classify(landsat_masked, rcl = rcl)\n\n# adjust values based on scaling factor\n\nlandsat &lt;- (landsat * 0.0000275 - 0.2)*100\nsummary(landsat)\n\n# plot true color image to check results\nplotRGB(landsat, r = 3, g = 2, b = 1, stretch = \"lin\")\n\n\n\n\n\n\n\n\n\ncode\n# check values are 0 - 100\nsummary(landsat)"
  },
  {
    "objectID": "blogs/landuse_cover_ML/index.html#classify-image",
    "href": "blogs/landuse_cover_ML/index.html#classify-image",
    "title": "Applying Supervised Machine Learning to Landuse Cover in Santa Barbara, CA",
    "section": "Classify image",
    "text": "Classify image\n\nExtract reflectance values for training data\nHere, I loaded in the shapefile identifying different locations within the study area as containing one of the 4 land cover types. I then extracted the spectral values at each site to create a data frame that relates land cover types to their spectral reflectance.\n\n\ncode\n# read in and transform training data\ntraining_data &lt;- st_read(file.path(data, \"trainingdata.shp\")) %&gt;% \n  st_transform(., crs = st_crs(landsat))\n#plot\nplot(training_data)\n\n\n\n\n\n\n\n\n\ncode\n# extract reflectance values at training sites\ntraining_data_values &lt;- extract(landsat, training_data, df = TRUE)\n\n# convert training data to data frame\ntraining_data_attributes &lt;- training_data %&gt;% \n  st_drop_geometry()\n\n# join training data attributes and extracted reflectance values\n\nSB_training_data &lt;- left_join(training_data_values, training_data_attributes,\n          by = c(\"ID\" = \"id\")) %&gt;% \n  mutate(type = as.factor(type))\n\n\n\n\nTrain decision tree classifier\nTo train the decision tree, I first needed to establish a model formula (i.e. what the response and predictor variables are). The rpart function implements the CART algorithm. The rpart function needs to know the model formula and training data you would like to use. Because I was performing a classification, I set method = \"class\". I also set na.action = na.omit to remove any pixels with NAs from the analysis.\n\nTo understand how the decision tree will classify pixels, I first plotted the results. The decision tree is comprised of a hierarchy of binary decisions. Each decision rule has 2 outcomes based on a conditional statement pertaining to values in each spectral band.\n\n\ncode\n# establish model formula\n\nSB_formula &lt;- type ~ red + green + blue + NIR + SWIR1 + SWIR2\n\n# train decision tree\nSB_decision_tree &lt;- rpart(formula = SB_formula,\n      data = SB_training_data, \n      method = \"class\", \n      na.action = na.omit)\n\n# plot decision tree\nprp(SB_decision_tree)\n\n\n\n\n\n\n\n\n\n\n\nApply decision tree\nAfter making the decision tree, I applied it to the entire image. The terra package includes a predict() function that allows you to apply a model to the data. In order for this to work properly, the names of the layers needed to match the column names of the predictors I used to train our decision tree. The predict() function then returns a raster layer with integer values. These integer values correspond to the factor levels in the training data. To figure out what category each integer corresponded to, I then inspected the levels of the training data.\n\n\ncode\n# classify image based on decision tree\nSB_classification &lt;- predict(landsat, SB_decision_tree, type = \"class\", na.rm = TRUE)\n\n# inspect level to understand the order of classes in prediction\nSB_classification\n\n\n\n\nPlot results\nFinally, I plotted the results to check out the land cover map.\n\n\ncode\n# plot results\ntm_shape(SB_classification) +\n  tm_raster()"
  },
  {
    "objectID": "blogs/landuse_cover_ML/index.html#conclusion",
    "href": "blogs/landuse_cover_ML/index.html#conclusion",
    "title": "Applying Supervised Machine Learning to Landuse Cover in Santa Barbara, CA",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis highlights the power of using supervised classification to gain insight of land use over on a large scale. While this technique has some challenges, it is a powerful tool that can be used in the environmental field to understand not just land use cover but a wide variety of environmental topics. It is important that these practices are also ground truthed to ensure the predicted model results are accurate."
  },
  {
    "objectID": "blogs/marine_aquaculture/index.html",
    "href": "blogs/marine_aquaculture/index.html",
    "title": "Potential Marine Aquaculture Habitat Along the US West Coast",
    "section": "",
    "text": "Github Project Repository"
  },
  {
    "objectID": "blogs/marine_aquaculture/index.html#background",
    "href": "blogs/marine_aquaculture/index.html#background",
    "title": "Potential Marine Aquaculture Habitat Along the US West Coast",
    "section": "Background",
    "text": "Background\nLand based meat production poses serious environmental challenges through its contribution to greenhouse gas (GHG) emissions as well as harsh land use impacts. As society progresses further into uncharted waters with climate change, it is critical that we explore alternative options to mitigate GHG emissions. Marine aquaculture is an important practice that has a high potential to transform the way the global food supply operates as a more sustainable protein option than land-based meat production.1 One important study by Gentry et al. mapped the potential for marine aquaculture globally based on multiple constraints, including ship traffic, dissolved oxygen, and bottom depth .2 This highlighted the true potential of the industry, although it is still far from reaching its capacity."
  },
  {
    "objectID": "blogs/marine_aquaculture/index.html#objectives",
    "href": "blogs/marine_aquaculture/index.html#objectives",
    "title": "Potential Marine Aquaculture Habitat Along the US West Coast",
    "section": "Objectives",
    "text": "Objectives\nThe purpose of this work flow is to utilize spatial analysis tools to understand how we can identify commercial potential of natural resources. In this analysis, I am looking at how to identify potential marine species habitat suitable for aquaculture along the West Coast of the United States in each of its Exclusive Economic Zones (EEZ). I specifically look at habitat requirements across temperature and depth requirements for oysters (a widely commercially available food source) and then create a function from this analysis that can be applied to any west coast species with a given temperature and depth range and potential for aquaculture. This analysis utilizes a variety of spatial analysis tools to for both vector and raster data. These include:\n\nspatial subsetting and joins\nraster reclassifiaction\nrasterization and vectorization of spatial data\ncropping and masking layers\nzonal statistics\ntransforming CRS\nutilizing periodic checks"
  },
  {
    "objectID": "blogs/marine_aquaculture/index.html#approach",
    "href": "blogs/marine_aquaculture/index.html#approach",
    "title": "Potential Marine Aquaculture Habitat Along the US West Coast",
    "section": "Approach",
    "text": "Approach\nBased on previous research, we know that oysters need the following conditions in order to reach optimal growth rates: - sea surface temperature: 11-30°C - depth: 0-70 meters below sea level\nI used raster and vector data and spatial analysis tools to identify where these ranges overlapped to isolate suitable habitat. Following these steps, I created maps visualizing priority EEZ’s depending on their total and percent suitable aquaculture habitat. I applied this methodology into a function that reproduces the same workflow to find suitable habita for any species with a temperature and depth range that could be potentially grown along the west coast.\n\nData\n\nSea Surface Temperature\nAverage annual sea surface temperature (SST) from the years 2008 to 2012 was used to characterize the average sea surface temperature within the region. The data was originally generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\n\n\nBathymetry\nTo characterize the depth of the ocean I used the General Bathymetric Chart of the Oceans (GEBCO).3\n\n\nExclusive Economic Zones\nMaritime boundaries were designated using Exclusive Economic Zones off of the west coast of US from Marineregions.org."
  },
  {
    "objectID": "blogs/marine_aquaculture/index.html#analysis",
    "href": "blogs/marine_aquaculture/index.html#analysis",
    "title": "Potential Marine Aquaculture Habitat Along the US West Coast",
    "section": "Analysis",
    "text": "Analysis\n\nPrepare data\nTo start, I loaded in and organized all necessary data and made sure it had the correct coordinate reference system. This involved:\n\nreading in the shapefile for the West Coast EEZ (wc_regions_clean.shp)\n\nreading in SST raster data\n\naverage_annual_sst_2008.tif\n\naverage_annual_sst_2009.tif\n\naverage_annual_sst_2010.tif\n\naverage_annual_sst_2011.tif\n\naverage_annual_sst_2012.tif\n\n\ncombining SST rasters into a raster stack\nreading in bathymetry raster (depth.tif)\n\nchecking that data are in the same coordinate reference system\n\n\n\n\ncode\n#loading in and preparing the data in raster format \n\n# reading in west coast eez shape file \neez &lt;- st_read(file.path(data, \"wc_regions_clean.shp\"))\n\n#read in SST raster data as a stack\nstack &lt;- rast(c(file.path(data, \"average_annual_sst_2008.tif\"), \n                file.path(data, \"average_annual_sst_2009.tif\"), \n                file.path(data, \"average_annual_sst_2010.tif\"), \n                file.path(data, \"average_annual_sst_2011.tif\"), \n                file.path(data, \"average_annual_sst_2012.tif\")))\n\n#changing CRS\nstack &lt;- project(stack, \"epsg:4326\")\n\n#read in bathymetry data \nbath &lt;- rast(file.path(data, \"depth.tif\"))\n\n# plot(stack)\n# plot(bath)\n\n\n\n\nProcess data\nNext, I processed the SST and depth data so that they could be combined. In this case the SST and depth data had slightly different resolutions, extents, and positions. Because I didn’t want to change the underlying depth data, I resampled to match the SST data using the nearest neighbor approach.\nThe steps I took can be summarized as follows:\n\nStep 1: find the mean SST from 2008-2012\n\nStep 2: convert SST data from Kelvin to Celsius\n\nStep 3: crop depth raster to match the extent of the SST raster (match CRS first)\nStep 4: resample the NPP data to match the resolution of the SST datausing the nearest neighbor approach\n\nStep 5: check that the depth and SST match in resolution, extent, and coordinate reference system\n\n\nI then created a check to ensure that the extent, resolution, and CRS of bathymetry and temperature raster layers matched.\n\n\ncode\n# finding mean temperature raster data in celsius and aligning resolutions and extents of rasters\n\n#findig mean of sst rasters\nmean_sst &lt;- mean(stack)\n# plot(mean_sst)\n\n#finding temp in celsius\ncels_sst &lt;- mean_sst - 273.15\n\n#cropping depth raster to sst\ncrop_bath &lt;- crop(bath, cels_sst)\n# plot(crop_bath)\n\n#resampling bath layer to match resolution of celsius\nresamp_bath &lt;- resample(crop_bath, cels_sst, method = \"near\")\n\n# plot(reclass_bath)\n\n                  ####### creating a check #######\ncat(\"Extent, resolution, and CRS of bathymetry and temprature raster layers match:\")\n\n\nExtent, resolution, and CRS of bathymetry and temprature raster layers match:\n\n\ncode\nif(ext(resamp_bath) == ext(cels_sst) & \n   st_crs(resamp_bath)$epsg == st_crs(cels_sst)$epsg &\n   # resolution has more than one responses, so we need to make sure that both are true \n   all(res(resamp_bath) == res(cels_sst))) {\n  cat(\"TRUE\")\n} else{\n  cat(\"FALSE\")\n}\n\n\nTRUE\n\n\n\n\nFinding suitable locations\nIn order to find suitable locations for marine aquaculture, I first needed to find locations that were suitable in terms of both SST and depth. This involved reclassifying the SST and depth data into locations that are suitable for Oysters by assigning a 1 or a NA value to suitable and unsuitable habitats. I then multiplied the two rasters to look for locations that satisfy both SST and depth conditions (raster cells where 1 values overlapped) using the lapp() function. Lastly, I created a check to ensure that the masked ideal layer only showed overlapping ideal depth and temperature conditions.\n\n\ncode\n#reclassifying temperature and bathymetry raster data, and then overlaying the layers to find ideal habitat \n\n\n# reclassify temp raster to NA or 1 if in 11-30C temp range \nrcl_temp &lt;- classify(cels_sst, rcl = \n                      matrix(c(-Inf, 11, NA, \n                      11, 30, 1, \n                      30, Inf, NA), ncol = 3, byrow = TRUE))\n# plot(rcl_temp)\n\n# reclassify bathymetry data (0-70 meters below sea level)\nrcl_bath &lt;- classify(resamp_bath, rcl = \n                      matrix(c(-Inf, -70, NA, \n                      -70, 0, 1, \n                      0, Inf, NA), ncol = 3, byrow = TRUE))\n# plot(rcl_bath)\n\n#creating a raster stack\nideal_stack &lt;- c(rcl_temp, rcl_bath)\n\n#renaming rasters in stack\nnames(ideal_stack) &lt;- c(\"temperature\", \"depth\")\n\n#creating a multiplication function \nmultiply &lt;- function(x,y){\n  return(x*y)\n}\n\n# multiplying layers to get just cells = 1 (ideal depth and temp )\nideal_layer &lt;- lapp(ideal_stack[[c(1,2)]], fun = multiply)\n\n# plot(ideal_layer)\n\n              ################### Check ###############\n#checking to make sure ideal layer is only showing areas where the temp and depth are in ideal range\ncat(\"Masked ideal layer only shows overlapping ideal depth and temperature conditons:\")\n\n\nMasked ideal layer only shows overlapping ideal depth and temperature conditons:\n\n\ncode\nif(all(\n  values(ideal_layer == 1) == values(ideal_stack[[1]] == 1),\n  values(ideal_layer == 1) == values(ideal_stack[[2]] == 1), na.rm = TRUE)) {\n  cat(\"TRUE\")\n} else{\n  cat(\"FALSE\")\n}\n\n\nTRUE\n\n\n\n\nDetermine the most suitable EEZ\nI first needed to determine the total suitable area within each EEZ in order to rank zones by priority. To do so, I found the total area of suitable locations within each EEZ. The steps in which I approached this can be summarized as such:\n\nStep 1: read in, rasterize, and crop the the eez data to the extent and resolution of the ideal habitat layer\nStep 2: calculate the size of each raster cell in km^2\nStep 3: sum the tala area of suitable habitat cells using zonal() function\nStep 4: join this data with the eez data and create new columns with the total eez area, the toal suitable habitat in each eez, and the percent suitable habitat by eez\n\n\n\ncode\n#here we are calculating the total area of suitable habitat and the percent area of suitable habitat for each EEZ\n\n#read in EEZ data \neez &lt;- st_read(file.path(data, \"wc_regions_clean.shp\"))\n\n#rasterizing polygon data with ideal locations by region and masking it to ideal locations\neez_mask &lt;- rasterize(eez, ideal_layer, field = \"rgn\") %&gt;% \n  terra::crop(ideal_layer, mask = TRUE)\n# plot(eez_mask)\n\n#find area of raster cells \ncell_area &lt;- cellSize(eez_mask, unit = \"km\")\n# plot(cell_area)\n\n#calculate the total area of km2 by region\nideal_area &lt;- zonal(cell_area, eez_mask, fun = \"sum\", na.rm = TRUE)\n\n#joining ideal area data w/ eez vector data \njoined_eez &lt;- left_join(eez, \n            ideal_area, \n            by = \"rgn\") %&gt;% \n  mutate(ideal_area_km2 = area, \n         perc_ideal = (ideal_area_km2/area_km2)*100, \n         .before = geometry)\n\n\n\n\nVisualize results\nFinally, I created two maps showing 1) the total suitable oyster aquaculture habitat area by region, and 2) the percent suitable oyster aquaculture habitat area by region:\n\n\ncode\n# pulling in states/base map layer for mapping EEZ's\nstates &lt;- us_states\n\n#choosing just the west coast regions\nwest_coast &lt;- st_sf(us_states) %&gt;% \n  filter(REGION == \"West\") %&gt;% \n  st_transform(crs = st_crs(joined_eez))\n\n\n#plotting the percent suitable area of ideal habitat for oysters for each EEZ\nggplot() +\n  geom_sf(data = west_coast, color = \"black\", fill = \"#E4CA74\") +\n  geom_sf(data = joined_eez, aes(fill = perc_ideal)) +\n  scale_fill_viridis_c() +\n  labs(title = \"% Suitable Oyster Aquaculture Habitat of West \\nCoast Exclusive Economic Zones in the US\", y = \"Longitude\", x = \"Latitude\", fill = expression(\"% Suitable Area\")) +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"lightblue3\"), \n    plot.title = element_text(size = 12, hjust = 0.5)) +\n    # panel.grid.major = element_blank(),\n   # panel.grid.minor = element_blank()) +\n     coord_sf(xlim = c(-132.5, -114.5)) +\n   annotation_scale( location = \"bl\") +\n  # theme( legend.position= \"bottom\") +\n         # plot.margin=unit(c(1,6,.5,1),\"cm\")) +\n  annotation_north_arrow(location = \"bl\",\n                         pad_x = unit(0.05, \"in\"),\n                         pad_y = unit(0.3, \"in\"),\n                         style = ggspatial::north_arrow_minimal()) +\n  annotate(\"text\", x = -121, y = 32.5, label = \"Southern CA\", size = 2, color = \"black\") +\n   annotate(\"text\", x = -124, y = 36, label = \"Central CA\", size = 2, color = \"black\") +\n  annotate(\"text\", x = -126.5, y = 40, label = \"Northern CA\", size = 2, color = \"lightgrey\") +\n    annotate(\"text\", x = -126.5, y = 44, label = \"Oregon\", size = 2, color = \"lightgrey\") +\n      annotate(\"text\", x = -126.5, y = 46.7, label = \"Washington\", size = 2, color = \"black\") \n\n\n\n\n\nFigure 1. % Suitable Oyster Aquaculture Habitat of West Coast Exclusive Economic Zones in the US. Shows the percent of ideal oyster aquaculture habitat by exclusive economic zones along the west coast. Washington shows the highest % suitable habitat, followed by Central and Southern California.\n\n\n\n\n\n\ncode\n#plotting the total suitable area of ideal habitat for oysters in each EEZ\nggplot() +\n  geom_sf(data = west_coast, color = \"black\", fill = \"#E4CA74\") +\n  geom_sf(data = joined_eez, aes(fill = ideal_area_km2)) +\n  scale_fill_viridis_c() +\n  labs(title = \"Suitable Oyster Aquaculture Habitat Area by West \\nCoast Exclusive Economic Zones in the US\", y = \"Longitude\", x = \"Latitude\", fill = expression(\"Total Suitable Area (km\"^2*\")\")) +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"lightblue3\"), \n    plot.title = element_text(size = 12, hjust = 0.5)) +\n    # panel.grid.major = element_blank(),\n   # panel.grid.minor = element_blank()) +\n     coord_sf(xlim = c(-132.5, -114.5)) +\n   annotation_scale( location = \"bl\") +\n  # theme( legend.position= \"bottom\") +\n         # plot.margin=unit(c(1,6,.5,1),\"cm\")) +\n  annotation_north_arrow(location = \"bl\",\n                         pad_x = unit(0.05, \"in\"),\n                         pad_y = unit(0.3, \"in\"),\n                         style = ggspatial::north_arrow_minimal()) +\n  annotate(\"text\", x = -121, y = 32.5, label = \"Southern CA\", size = 2, color = \"black\") +\n   annotate(\"text\", x = -124, y = 36, label = \"Central CA\", size = 2, color = \"black\") +\n  annotate(\"text\", x = -126.5, y = 40, label = \"Northern CA\", size = 2, color = \"lightgrey\") +\n    annotate(\"text\", x = -126.5, y = 44, label = \"Oregon\", size = 2, color = \"lightgrey\") +\n      annotate(\"text\", x = -126.5, y = 46.7, label = \"Washington\", size = 2, color = \"lightgrey\") \n\n\n\n\n\nFigure 2. Suitable Oyster Aquaculture Habitat Area by West Coast Exclusive Economic Zones in the US. Shows the total area (km2) of ideal oyster farming habitat by exclusive economic zones along the West Coast. Central California shows the highest total suitable area, followed by Souther California and Washington.\n\n\n\n\n\n\nExpanding to other species\nAfter having identified the potential suitable habitat for oyster aquaculture, I wanted to expand this workflow to easily make maps for other species requirements. I created a function that can accept any species name, temperature, and depth ranges, and it will create two replicated maps for the given species.\n\n\ncode\n#creating a function that shows the ideal species habitat using species name,  temperature, and depth as inputs \nideal_crib_for_a_species &lt;- function(species = \"NAME\", depth_min, depth_max, temp_min, temp_max) {\n# depth reclassification\n  rcl_depth &lt;- classify(resamp_bath, rcl = matrix(c(-Inf, depth_min, NA, \n                                  depth_min, depth_max, 1, \n                                  depth_max, Inf, NA), ncol = 3, byrow = TRUE))\n#temp reclassification\n  rcl_temp &lt;- classify(cels_sst, rcl = matrix(c(-Inf, temp_min, NA, \n                                temp_min, temp_max, 1, \n                                temp_max, Inf, NA), ncol = 3, byrow = TRUE))\n  #ideal layers stack\n  ideal_stack &lt;- c(rcl_temp, rcl_bath)\n  #ideal layer\n  ideal_layer &lt;- lapp(ideal_stack[[c(1,2)]], fun = multiply)\n  #rasterize eez polygons\n  eez_mask &lt;- rasterize(eez, ideal_layer, field = \"rgn\") %&gt;% \n    terra::crop(ideal_layer, mask = TRUE)\n  #find area of raster cells \n  cell_area &lt;- cellSize(eez_mask, unit = \"km\")\n  #calculate the total area of km2 by region\n  ideal_area &lt;- zonal(cell_area, eez_mask, fun = \"sum\", na.rm = TRUE)\n  #joining ideal area data w/ eez vector data \n  joined_eez &lt;- left_join(eez, ideal_area, by = \"rgn\") %&gt;% \n    mutate(ideal_area_km2 = area, perc_ideal = (ideal_area_km2/area_km2)*100, .before = geometry)\n\n\n############################# mapping #####################################\n  \n  #percent area\n m1 &lt;- ggplot() +\n    geom_sf(data = west_coast, color = \"black\", fill = \"#E4CA74\") +\n    geom_sf(data = joined_eez, aes(fill = perc_ideal)) +\n    scale_fill_viridis_c() +\n    labs(title = paste(\"% Suitable\", species, \"Aquaculture Habitat of West \\nCoast Exclusive Economic Zones in the US\"), y = \"Longitude\", x = \"Latitude\", fill = expression(\"% Suitable Area\")) +\n    theme_minimal() +\n    theme(\n      panel.background = element_rect(fill = \"lightblue3\"), \n      plot.title = element_text(size = 12, hjust = 0.5)) +\n    coord_sf(xlim = c(-132.5, -114.5)) +\n    annotation_scale( location = \"bl\") +\n    annotation_north_arrow(location = \"bl\",\n                           pad_x = unit(0.05, \"in\"),\n                           pad_y = unit(0.3, \"in\"),\n                           style = ggspatial::north_arrow_minimal()) +\n    annotate(\"text\", x = -121, y = 32.5, label = \"Southern CA\", size = 2, color = \"lightgrey\") +\n    annotate(\"text\", x = -124, y = 36, label = \"Central CA\", size = 2, color = \"lightgrey\") +\n    annotate(\"text\", x = -126.5, y = 40, label = \"Northern CA\", size = 2, color = \"lightgrey\") +\n    annotate(\"text\", x = -126.5, y = 44, label = \"Oregon\", size = 2, color = \"lightgrey\") +\n    annotate(\"text\", x = -126.5, y = 46.7, label = \"Washington\", size = 2, color = \"black\") \n###################################################################################\n\n  #plotting total area\n  m2 &lt;- ggplot() +\n  geom_sf(data = west_coast, color = \"black\", fill = \"#E4CA74\") +\n  geom_sf(data = joined_eez, aes(fill = ideal_area_km2)) +\n  scale_fill_viridis_c() +\n  labs(title = paste(\"Suitable\", species, \"Aquaculture Habitat Area by West \\nCoast Exclusive Economic Zones in the US\"), y = \"Longitude\", x = \"Latitude\", fill = expression(\"Total Suitable Area (km\"^2*\")\")) +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"lightblue3\"), \n    plot.title = element_text(size = 12, hjust = 0.5)) +\n    # panel.grid.major = element_blank(),\n   # panel.grid.minor = element_blank()) +\n     coord_sf(xlim = c(-132.5, -114.5)) +\n   annotation_scale( location = \"bl\") +\n  # theme( legend.position= \"bottom\") +\n         # plot.margin=unit(c(1,6,.5,1),\"cm\")) +\n  annotation_north_arrow(location = \"bl\",\n                         pad_x = unit(0.05, \"in\"),\n                         pad_y = unit(0.3, \"in\"),\n                         style = ggspatial::north_arrow_minimal()) +\n  annotate(\"text\", x = -121, y = 32.5, label = \"Southern CA\", size = 2, color = \"lightgrey\") +\n   annotate(\"text\", x = -124, y = 36, label = \"Central CA\", size = 2, color = \"lightgrey\") +\n  annotate(\"text\", x = -126.5, y = 40, label = \"Northern CA\", size = 2, color = \"lightgrey\") +\n    annotate(\"text\", x = -126.5, y = 44, label = \"Oregon\", size = 2, color = \"lightgrey\") +\n      annotate(\"text\", x = -126.5, y = 46.7, label = \"Washington\", size = 2, color = \"black\") \n  \n  return(list(m1, m2))\n  \n  }\n\n\nI wanted to test out this function on kelp crabs (Pugettia producta) as they are widely dispersed and could potentially make a good candidate for aquaculture. Data on the Kelp Crab was obtained from SeaLifeBase which presents habitat information on thousands of species. Lets take a look:\n\n\ncode\nideal_crib_for_a_species(\"Kelp Crab\", depth_min = -98, depth_max =  0, temp_min = 7.8, temp_max =  12.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd there you have it! This is how I used spatial analysis tools in R to figure out where potential oyster (and any other species) aquaculture might be by EEZ along the west coast of the United States."
  },
  {
    "objectID": "blogs/marine_aquaculture/index.html#footnotes",
    "href": "blogs/marine_aquaculture/index.html#footnotes",
    "title": "Potential Marine Aquaculture Habitat Along the US West Coast",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHall, S. J., Delaporte, A., Phillips, M. J., Beveridge, M. & O’Keefe, M. Blue Frontiers: Managing the Environmental Costs of Aquaculture (The WorldFish Center, Penang, Malaysia, 2011).↩︎\nGentry, R. R., Froehlich, H. E., Grimm, D., Kareiva, P., Parke, M., Rust, M., Gaines, S. D., & Halpern, B. S. Mapping the global potential for marine aquaculture. Nature Ecology & Evolution, 1, 1317-1324 (2017).↩︎\nGEBCO Compilation Group (2022) GEBCO_2022 Grid (doi:10.5285/e0f0bb80-ab44-2739-e053-6c86abc0289c).↩︎"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "EducationProfessional ExperienceAcademic ExperienceSkills/Certifications/AwardsFull Resume\n\n\nUniversity of California, Santa Barbara | Santa Barbara, CA\n Masters in Environmental Science and Management | Sept 2022 - June 2024\nUniversity of California, Santa Cruz | cum laude 3.84 GP | Santa Cruz, CA\n B.Sc. Ecology and Evolutionary Biology | Highest Honors 4.0 GPA| Sept 2017 - Dec 2020\n B.A. Environmental Studies | Highest Honors 4.0 GPA| Sept 2017 - Dec 2020\n\n\nData Manager – Masters Group Project, UCSB (3/23-pres.)\n\nSpearheaded a multifaceted economic analysis using heterogeneous data sets and cost modeling to inform NOAA management on priority conservation sites and estimated project costs\nPerformed extensive spatial analyses in tandem with interactive nested function models to extract cost estimates from large data sets while overseeing a GitHub repository management\n\nData Analyst – Yoga Soup, Santa Barbara, CA (9/23-pres.)\n\nAnalyzed and relocated 1,000’s of video files in R to reduce company subscription overhead fees\n\nBiosecurity Intern – The Nature Conservancy, Santa Barbara, CA (6/23-9-23)\n\nLed a comprehensive study encompassing fieldwork, report writing, and statistical analysis to address weaknesses in current biosecurity protocols for TNC management while mentoring an undergraduate\nCoded an invasive species occupancy model in R (logit-link function) testing causal effects of treatments on potential invasive species behavior responses using primary, field collected data\nPresented research at the California Islands Symposium and am publishing a first-authored scientific paper\n\nBiologist – Mountain View Biological Consulting, Mammoth Lakes, CA (2/21–6/22)\n\nMapped dozens of field sites in GIS, located access points and project perimeters, and communicated project logistics to senior biologists, project managers, and contractors\nDrafted environmental compliance reports for contractors, consultants, and land owners summarizing project description and biological activity within the region of interest (ROI)\n\nLab Technician – Sierra Nevada Aquatic Research laboratory, Mammoth Lakes, CA (1/21–6/22)\n\nQuantified 20+ years of long term acid mining drainage impacts on alpine stream biodiversity to inform US Forest Service on remediation success\nIdentified and recorded 100,000’s of stream organisms in excel to manage over 20 years of project data\n\nResearch Technician – Palkovacs Lab UCSC, Santa Cruz, CA (9/2019-12-20)\n\nSecured $10,000’s in research grants to study the link between steelhead trout genotypes and prey selectivity on benthic macroinvertebrate (BMI) communities\nDesigned a study measuring the ecological consequences of wildfires across 3 different watersheds in coastal California to inform land management of potential trophic cascades\n\nEnvironmental Consultant Assistant – Laguna Geosciences, Laguna Beach, CA (seasonal 1/18-6/20)\n\nOrganized and documented project data sets for senior management to ensure clients’ needs are met\n\n\n\nTeaching Assistant – UCSB (9/22-present)\n\nApplied Ecology | Environmental Chemistry | Environmental Ethics | Infectious Disease Ecology\n\nCarbon Accounting student (9/22-12/22)\n\nManaged a group project calculating the total scope 1-3 emissions of McGrath power plant (Oxnard, CA) according to the Greenhouse Gas Protocol and authored a final report\nCollected emissions data across multiple data bases, calculated relevant scope emissions in excel, and presented our findings for a panel of professors, researchers, and industry experts\n\nREU Researcher – University of Puerto Rico, Rio Piedras, PR (6/19-9/19)\n\nDesigned a research project measuring disturbance impacts on diversity of 25+ freshwater meiofauna taxa in proximity to highly developed areas in low income communities\nCollected hundreds of field samples, analyzed samples under the microscope, and reported findings to a committee of freshwater ecologists and Puerto Rican citizens in a research paper\n\nField Technician – Lyon Lab UCSC, Santa Cruz, CA (9/19-12-20)\n\nDocumented hundreds of sparrow social interactions to provide data on the link between phenotypic hierarchical traits and social dominance within migrating flocks\n\nIntern - Small Mammal Undergraduate Research in the Forest UCSC, Santa Cruz, CA (9/18-12/18)\n\nTrapped, tagged, and recorded hundreds of coastal California rodents and contributed results to long term metadata on mammal populations\nWorked intensive field days setting up transects, laying traps, and collecting field data in Monterey Bay\n\n\n\nSkills\n\nStatistical Analysis and Data Management (R/R-Studio, Git/GitHub, SQL, Excel)\nSpatial Analysis and Cartography (R, QGIS)\n\nRemote sensing, spatial kriging, mapping\n\nData Visualization (Tableau, R/shiny app)\nMachine Learning, Multiple Linear/Logistic Regression Modeling\nCarbon Accounting Scope emissions calculations, GHG protocol, ISO standards\nCost Benefit Analysis (CBA)\n\nCertificates:\n\nGoogle Data Analytics (9/2023)\n\nAwards:\n\nBren Academic Excellence Recruitment Fellowship 2022 & 2023\nNRS Field Science Fellowship 2020\nFuture Leaders in Coastal Science Award 2019\nKathryn D. Sullivan Impact Award 2019\nNorris Center Student Natural History Award 2019\nRichard A. Cooley Award 2019\nGolden Key International Honors Society 2018\nThe National Society of Collegiate Scholars 2017\nHonors Program UCSC\nUCSC Deans Honor Roll (2017-2020)\n\n\n\n\nDownload PDF"
  },
  {
    "objectID": "index.html#hello-world",
    "href": "index.html#hello-world",
    "title": "Raymond Hunter",
    "section": "Hello world!",
    "text": "Hello world!\nOut of the infinite amount of content available on the web, you have stumbled upon a little screenshot of my life on a computer screen. Whoohoo! My name is Raymond (I go by Ray), and I created this personal website using Quarto in R Studio to show off some of the things that are important to me in my academic and professional career. Here, I am sharing some skills, experiences, and hobbies that have molded me into the nature loving, data enthusiast that I am today.\nWith a diverse background in the “environmental” field, I have developed a passion for investigating environmental problems and applying data science skills to make sense of the chaos. Some of my favorite tools to use are R/RStudio, SQL, Tableau, QGIS, and of course, spreadsheets. I use these tools for everything from creating multi-species occupancy analyses of invasive rodents to visualizing anthropogenic energy production and consumption to simply analyzing my monthly budgeting.\nI am currently completing my Masters in the Bren School of Environmental Science and Management at the University of California Santa Barbara, and I work as a teaching assistant on the side. When not grading tests or squinting my eyes at error messages, I enjoy getting after it outside.\nEnjoy!\n\n“Not all those who wonder are lost” - Gandalf the Grey"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "a snapshot of my life\n\n\n\nBioInterestsHobbies\n\n\n\nBackground\n\nBorn and raised in Laguna Beach, CA, my admiration of the natural world was fostered in the blue waters of the Pacific ocean, where I frequently surfed and swam with the local inhabitants of the kelp forests. I started to venture out to the Sierra Nevada mountains where I learned to sit in in awe of its sharp granite and alpine lakes. Upon graduating high school, my connection to the environment brought me to northern California’s redwood forests where I attended the University of California, Santa Cruz. My education in Ecology and Evolutionary Biology, as well as Environmental Studies, planted initial seed of what would become my career as a scientist.\nDuring my undergrad, I worked as lab and field techs doing whatever research jobs spoke to me at the time. One of my favorite experiences was camping out on a remote island in Baja California where I studied desert lizard and insect communities in cacti forests. I also worked quite a bit in Big Sur, analyzing the coastal California Steelhead communities and diets. And of course, I will never forget living in Puerto Rico for 3 months collecting microscopic protozoans in some harsh conditions. These experiences made me ever more curious, and were crucial in developing my skills in observing the natural world.\nUpon graduating in 3 years, I moved to Mammoth Lakes in 2021 to pursue rock climbing, skiing, backpacking, and working as a freshwater ecologist at the Sierra Nevada Aquatic Research Center (SNARL). I frequented streams in the eastern Sierra doing field and lab work, with the goal of contributing to the scientific knowledge of benthic macroinvertebrate community ecology. I learned many of the skills that shape a successful scientist and make one really look at the world in a critical way. And I might have had a bit too much fun romping around in the mountains when he wasn’t working. My passion for learning drew me back to the coast in 2022 to start a masters program in the Bren School of Environmental Science and Management at UCSB where I remain today.\n\n\n\n\n\n\nField work in Puerto Rico\n\n\n\n\n\n\n\nHolding a desert iguana in Baja California\n\n\n\n\n\n\n\n\n\nSome beautiful microbes from Puerto Rico\n\n\n\n\n\n\n\nField work in Big Sur, CA\n\n\n\n\n\n\n\n\n\nSampling a rock glacier stream in the Sierras\n\n\n\n\n\n\n\nWorking in the lab at UCSC\n\n\n\n\n\n\n\n\nEnvironmental Data Science\n\nWhile I am passionate about a variety of environmental topics, one underlying theme keeps the gears in my mind spinning: Environmental Data Science. Grappling, extracting, analyzing, and visualizing data are ways that one can make sense of the world around them, and that is just what I like to do. I serves as the data manager for his masters group project working with the National Oceanic and Atmospheric Administration (NOAA) where my team and I are modeling the costs of various Puget Sound Chinook habitat restorations methods. I have also been working with The Nature Conservancy where I am strengthening their Channel Islands Biosecurity team.\nAside from my masters work, I am particularly interested in working with big data to extract meaningful patterns and insights on how we can improve the state of the environment. Sometimes this looks like modeling the occupancy of invasive rats in Southern California that serve as threats to endemic species. Other times it looks like predicting the effects of a carbon tax on consumer and supplier demand of goods. Regardless of the task, as long as it involves working with data and contributes to solving environmental problems, you will usually get my attention.\n\n\n\nIn my free time . . .\n\nEqually as strong as my love for nature is my desire to immerse myself in it. Some of my favorite activities include: rock climbing, surfing, paragliding, spearfishing, running, backpacking, and practicing yoga. While I know I can’t do them all every day, I try my best to at least one of them… or 3. Here are some of my favorite memories over the years.\n\n\n\n\n\n\nMountaineering trip up Matterhorn Peak in the Sierra Nevada mountains\n\n\n\n\n\n\n\nPreparing for a multiday ascent up El Capitan, Yosemite\n\n\n\n\n\n\n\n\n\nBig wall climbing on El Capitan, Yosemite\n\n\n\n\n\n\n\nBig wall climbing on El Capitan, Yosemite\n\n\n\n\n\n\n\n\n\nBig wall climbing on El Capitan, Yosemite\n\n\n\n\n\n\n\nSnack during a backcountry ski tour\n\n\n\n\n\n\n\n\n\nBeach day in Kauai\n\n\n\n\n\n\n\nParagliding in the eastern Sierra Nevada mountains\n\n\n\n\n\n\n\n\n\nClimbing Lone Pine Peak, California\n\n\n\n\n\n\n\nCamping in Big Sur"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "My Blog",
    "section": "",
    "text": "Applying Supervised Machine Learning to Landuse Cover in Santa Barbara, CA\n\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nSpatial\n\n\n\n\nbuilding algorithms to understand how we use our land\n\n\n\n\n\n\nDec 19, 2023\n\n\nRaymond Hunter\n\n\n\n\n\n\n  \n\n\n\n\nPotential Marine Aquaculture Habitat Along the US West Coast\n\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nSpatial\n\n\n\n\nmodeling habitat through spatial analysis\n\n\n\n\n\n\nDec 16, 2023\n\n\nRaymond Hunter\n\n\n\n\n\n\n  \n\n\n\n\nA Spatial Analysis of Houston Blackouts and the Communities it Affected\n\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nSpatial\n\n\n\n\nan investigation of environmental injustice\n\n\n\n\n\n\nDec 15, 2023\n\n\nRaymond Hunter\n\n\n\n\n\n\n  \n\n\n\n\nDrivers of Zooplankton Community Dynamics in Sierra Nevada Lakes\n\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\necology\n\n\n\n\nA look into modeling microcosms\n\n\n\n\n\n\nDec 8, 2023\n\n\nRaymond Hunter\n\n\n\n\n\n\nNo matching items"
  }
]